# ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô Pandas
## ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏õ‡∏£‡∏¥‡∏ç‡∏ç‡∏≤‡∏ï‡∏£‡∏µ

---

## üìä ‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç
1. [‡∏ö‡∏ó‡∏ô‡∏≥ Pandas](#‡∏ö‡∏ó‡∏ô‡∏≥-pandas)
2. [‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£ Import](#‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£-import)
3. [Series ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô](#series-‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô)
4. [DataFrame ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô](#dataframe-‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô)
5. [‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•](#‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)
6. [Data Selection ‡πÅ‡∏•‡∏∞ Filtering](#data-selection-‡πÅ‡∏•‡∏∞-filtering)
7. [Data Cleaning](#data-cleaning)
8. [Data Transformation](#data-transformation)
9. [Groupby Operations](#groupby-operations)
10. [Merging ‡πÅ‡∏•‡∏∞ Joining](#merging-‡πÅ‡∏•‡∏∞-joining)
11. [Time Series Analysis](#time-series-analysis)
12. [Visualization](#visualization)
13. [Performance Optimization](#performance-optimization)
14. [‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î](#‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î)
15. [‡πÇ‡∏Ñ‡∏£‡∏á‡∏á‡∏≤‡∏ô‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥](#‡πÇ‡∏Ñ‡∏£‡∏á‡∏á‡∏≤‡∏ô‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥)

---

## ‡∏ö‡∏ó‡∏ô‡∏≥ Pandas

### Pandas ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?
**Pandas** (Python Data Analysis Library) ‡πÄ‡∏õ‡πá‡∏ô library ‡∏ó‡∏µ‡πà‡∏ó‡∏£‡∏á‡∏û‡∏•‡∏±‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô Python ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏ó‡∏µ‡πà‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö structured

### ‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ Pandas?

#### ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

```python
# ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ Pandas - ‡∏¢‡∏∏‡πà‡∏á‡∏¢‡∏≤‡∏Å
data = [
    ['Alice', 25, 50000],
    ['Bob', 30, 60000],
    ['Charlie', 35, 70000]
]

# ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô
total_salary = sum(row[2] for row in data)
avg_salary = total_salary / len(data)

# ‡πÉ‡∏ä‡πâ Pandas - ‡∏á‡πà‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
import pandas as pd
df = pd.DataFrame(data, columns=['Name', 'Age', 'Salary'])
avg_salary = df['Salary'].mean()
```

### ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ç‡∏≠‡∏á Pandas
- **Easy Data Manipulation**: ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢
- **Multiple File Formats**: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö (CSV, Excel, JSON, SQL)
- **Missing Data Handling**: ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏î‡∏µ
- **Data Alignment**: ‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
- **Flexible Grouping**: ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢
- **Integration**: ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö NumPy, Matplotlib ‡πÑ‡∏î‡πâ‡∏î‡∏µ

### ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
- ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£ Data Science ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 95%
- ‡∏°‡∏µ‡∏Å‡∏≤‡∏£ download ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 50 ‡∏•‡πâ‡∏≤‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏î‡∏∑‡∏≠‡∏ô
- Essential tool ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Data Scientists ‡∏ó‡∏±‡πà‡∏ß‡πÇ‡∏•‡∏Å

---

## ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£ Import

### ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á
```bash
# ‡∏ú‡πà‡∏≤‡∏ô pip
pip install pandas

# ‡∏ú‡πà‡∏≤‡∏ô conda
conda install pandas

# ‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ö dependencies ‡∏≠‡∏∑‡πà‡∏ô‡πÜ
pip install pandas numpy matplotlib seaborn

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö version
python -c "import pandas as pd; print(pd.__version__)"
```

### ‡∏Å‡∏≤‡∏£ Import
```python
# Standard import
import pandas as pd
import numpy as np

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö version
print(f"Pandas version: {pd.__version__}")

# Import ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
from pandas import DataFrame, Series, read_csv

# ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ display options
pd.set_option('display.max_rows', 100)
pd.set_option('display.max_columns', 20)
pd.set_option('display.width', 1000)
```

### ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
```python
# ‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ó‡∏®‡∏ô‡∏¥‡∏¢‡∏°
pd.set_option('display.float_format', '{:.2f}'.format)

# ‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà
pd.set_option('display.max_colwidth', 50)

# ‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤
pd.reset_option('all')

# ‡∏î‡∏π‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
print(pd.describe_option())
```

---

## Series ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô

### Series ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?
**Series** ‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 1 ‡∏°‡∏¥‡∏ï‡∏¥‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏ô‡∏¥‡∏î‡πÉ‡∏î‡∏Å‡πá‡πÑ‡∏î‡πâ (integers, strings, floats, Python objects) ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ö labels (index)

### ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Series

```python
import pandas as pd
import numpy as np

# ‡∏à‡∏≤‡∏Å Python list
s1 = pd.Series([1, 3, 5, np.nan, 6, 8])
print(s1)

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î index ‡πÄ‡∏≠‡∏á
s2 = pd.Series([1, 3, 5, 6], index=['a', 'b', 'c', 'd'])
print(s2)

# ‡∏à‡∏≤‡∏Å dictionary
data_dict = {'A': 1, 'B': 2, 'C': 3, 'D': 4}
s3 = pd.Series(data_dict)
print(s3)

# ‡∏à‡∏≤‡∏Å NumPy array
arr = np.array([1, 2, 3, 4, 5])
s4 = pd.Series(arr, index=['x', 'y', 'z', 'w', 'v'])
print(s4)

# Series ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
s5 = pd.Series(5, index=[0, 1, 2, 3])
print(s5)
```

### ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á Series
```python
s = pd.Series([1, 3, 5, np.nan, 6, 8], index=['a', 'b', 'c', 'd', 'e', 'f'])

# ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
print(f"Values: {s.values}")          # NumPy array
print(f"Index: {s.index}")            # Index object
print(f"Data type: {s.dtype}")        # ‡∏ä‡∏ô‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
print(f"Shape: {s.shape}")            # ‡∏£‡∏π‡∏õ‡∏£‡πà‡∏≤‡∏á
print(f"Size: {s.size}")              # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô elements
print(f"Name: {s.name}")              # ‡∏ä‡∏∑‡πà‡∏≠ Series

# ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Series ‡πÅ‡∏•‡∏∞ index
s.name = 'My Series'
s.index.name = 'Labels'
print(s)
```

### ‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô Series
```python
s = pd.Series([10, 20, 30, 40, 50], index=['a', 'b', 'c', 'd', 'e'])

# ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏î‡πâ‡∏ß‡∏¢ label
print(s['b'])                    # 20
print(s[['a', 'c', 'e']])        # Multiple labels

# ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏î‡πâ‡∏ß‡∏¢ position
print(s[1])                      # 20
print(s[0:3])                    # Slicing

# Boolean indexing
print(s[s > 25])                 # ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 25

# ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
print('b' in s)                  # True
print(s.get('f', 'Not found'))   # Get with default value
```

### ‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö Series
```python
s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
s2 = pd.Series([10, 20, 30, 40], index=['a', 'b', 'c', 'd'])

# Arithmetic operations
print(s1 + s2)                   # Element-wise addition
print(s1 * 2)                    # Scalar multiplication
print(s1 ** 2)                   # Power

# Statistical operations
print(s1.sum())                  # ‡∏ú‡∏•‡∏£‡∏ß‡∏°
print(s1.mean())                 # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
print(s1.std())                  # ‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏ö‡∏µ‡πà‡∏¢‡∏á‡πÄ‡∏ö‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô
print(s1.describe())             # ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°

# String operations (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Series ‡∏ó‡∏µ‡πà‡∏°‡∏µ string)
s_str = pd.Series(['apple', 'banana', 'cherry'])
print(s_str.str.upper())         # ‡∏ï‡∏±‡∏ß‡∏û‡∏¥‡∏°‡∏û‡πå‡πÉ‡∏´‡∏ç‡πà
print(s_str.str.len())           # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß string
print(s_str.str.contains('a'))   # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ 'a'
```

---

## DataFrame ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô

### DataFrame ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?
**DataFrame** ‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 2 ‡∏°‡∏¥‡∏ï‡∏¥‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô table ‡πÉ‡∏ô SQL ‡∏´‡∏£‡∏∑‡∏≠ spreadsheet ‡πÉ‡∏ô Excel ‡πÇ‡∏î‡∏¢‡∏°‡∏µ labeled axes (rows ‡πÅ‡∏•‡∏∞ columns)

### ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame

```python
# ‡∏à‡∏≤‡∏Å dictionary
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'Diana'],
    'Age': [25, 30, 35, 28],
    'City': ['New York', 'London', 'Tokyo', 'Paris'],
    'Salary': [50000, 60000, 70000, 55000]
}
df = pd.DataFrame(data)
print(df)

# ‡∏à‡∏≤‡∏Å list of dictionaries
data_list = [
    {'Name': 'Alice', 'Age': 25, 'City': 'New York'},
    {'Name': 'Bob', 'Age': 30, 'City': 'London'},
    {'Name': 'Charlie', 'Age': 35, 'City': 'Tokyo'}
]
df2 = pd.DataFrame(data_list)
print(df2)

# ‡∏à‡∏≤‡∏Å NumPy array
arr = np.random.randn(4, 3)
df3 = pd.DataFrame(arr, 
                   columns=['A', 'B', 'C'],
                   index=['Row1', 'Row2', 'Row3', 'Row4'])
print(df3)

# ‡∏à‡∏≤‡∏Å Series
s1 = pd.Series([1, 2, 3], name='Col1')
s2 = pd.Series([4, 5, 6], name='Col2')
df4 = pd.DataFrame({'Col1': s1, 'Col2': s2})
print(df4)
```

### ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á DataFrame
```python
df = pd.DataFrame({
    'A': [1, 2, 3, 4],
    'B': [5.0, 6.0, 7.0, 8.0],
    'C': ['foo', 'bar', 'baz', 'qux'],
    'D': [True, False, True, False]
})

# ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
print(f"Shape: {df.shape}")              # (rows, columns)
print(f"Size: {df.size}")                # total elements
print(f"Columns: {df.columns}")          # column names
print(f"Index: {df.index}")              # row indices
print(f"Data types:\n{df.dtypes}")       # data types
print(f"Memory usage:\n{df.memory_usage()}")

# ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô
print(df.head())                         # 5 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å
print(df.tail(3))                        # 3 ‡πÅ‡∏ñ‡∏ß‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
print(df.info())                         # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°
print(df.describe())                     # ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö numeric columns
```

### ‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô DataFrame

#### ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Columns
```python
df = pd.DataFrame({
    'Name': ['Alice', 'Bob', 'Charlie'],
    'Age': [25, 30, 35],
    'Salary': [50000, 60000, 70000]
})

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å 1 column (‡πÑ‡∏î‡πâ Series)
print(df['Name'])

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏•‡∏≤‡∏¢ columns (‡πÑ‡∏î‡πâ DataFrame)
print(df[['Name', 'Salary']])

# ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏î‡πâ‡∏ß‡∏¢ attribute (‡∏ñ‡πâ‡∏≤‡∏ä‡∏∑‡πà‡∏≠ column ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á)
print(df.Name)
```

#### ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Rows
```python
# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏î‡πâ‡∏ß‡∏¢ position
print(df.iloc[0])              # ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å (Series)
print(df.iloc[0:2])            # ‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà 0-1 (DataFrame)

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏î‡πâ‡∏ß‡∏¢ label
df_indexed = df.set_index('Name')
print(df_indexed.loc['Alice'])  # ‡πÅ‡∏ñ‡∏ß‡∏Ç‡∏≠‡∏á Alice

# Boolean indexing
print(df[df['Age'] > 30])       # ‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏¢‡∏∏‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 30
print(df[df['Name'].isin(['Alice', 'Charlie'])])  # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏â‡∏û‡∏≤‡∏∞
```

#### ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Cells ‡πÄ‡∏â‡∏û‡∏≤‡∏∞
```python
# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å cell ‡πÄ‡∏â‡∏û‡∏≤‡∏∞
print(df.iloc[0, 1])           # ‡πÅ‡∏ñ‡∏ß 0, column 1
print(df.loc[0, 'Age'])        # ‡πÅ‡∏ñ‡∏ß 0, column 'Age'

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ä‡πà‡∏ß‡∏á
print(df.iloc[0:2, 1:3])       # ‡πÅ‡∏ñ‡∏ß 0-1, column 1-2
print(df.loc[0:1, 'Age':'Salary'])  # ‡πÅ‡∏ñ‡∏ß 0-1, column Age-Salary
```

---

## ‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

### ‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ï‡πà‡∏≤‡∏á‡πÜ

#### CSV Files
```python
# ‡∏≠‡πà‡∏≤‡∏ô CSV ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
df = pd.read_csv('data.csv')

# ‡∏û‡∏£‡πâ‡∏≠‡∏° parameters ‡∏ï‡πà‡∏≤‡∏á‡πÜ
df = pd.read_csv(
    'data.csv',
    sep=',',                    # ‡∏ï‡∏±‡∏ß‡πÅ‡∏ö‡πà‡∏á
    header=0,                   # ‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô header
    index_col=0,                # column ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô index
    usecols=['Name', 'Age'],    # column ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
    dtype={'Age': int},         # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏ô‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    parse_dates=['Date'],       # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô datetime
    na_values=['N/A', 'NULL']   # ‡∏Ñ‡πà‡∏≤ missing
)

# ‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å URL
url = 'https://example.com/data.csv'
df = pd.read_csv(url)

# ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ö‡∏≤‡∏á‡πÅ‡∏ñ‡∏ß
df = pd.read_csv('data.csv', nrows=100)  # ‡∏≠‡πà‡∏≤‡∏ô 100 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å
df = pd.read_csv('data.csv', skiprows=10)  # ‡∏Ç‡πâ‡∏≤‡∏° 10 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å
```

#### Excel Files
```python
# ‡∏≠‡πà‡∏≤‡∏ô Excel
df = pd.read_excel('data.xlsx')

# ‡∏≠‡πà‡∏≤‡∏ô sheet ‡πÄ‡∏â‡∏û‡∏≤‡∏∞
df = pd.read_excel('data.xlsx', sheet_name='Sheet1')

# ‡∏≠‡πà‡∏≤‡∏ô‡∏´‡∏•‡∏≤‡∏¢ sheets
dfs = pd.read_excel('data.xlsx', sheet_name=None)  # dictionary ‡∏Ç‡∏≠‡∏á DataFrames

# ‡∏û‡∏£‡πâ‡∏≠‡∏° parameters
df = pd.read_excel(
    'data.xlsx',
    sheet_name='Data',
    header=0,
    index_col=0,
    usecols='A:D',             # columns A ‡∏ñ‡∏∂‡∏á D
    skiprows=2                 # ‡∏Ç‡πâ‡∏≤‡∏° 2 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å
)
```

#### JSON Files
```python
# ‡∏≠‡πà‡∏≤‡∏ô JSON
df = pd.read_json('data.json')

# JSON ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ï‡πà‡∏≤‡∏á‡πÜ
df = pd.read_json('data.json', orient='records')  # list of objects
df = pd.read_json('data.json', orient='index')    # object of objects
df = pd.read_json('data.json', lines=True)        # JSON Lines format
```

#### SQL Database
```python
import sqlite3

# ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
conn = sqlite3.connect('database.db')

# ‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å SQL query
df = pd.read_sql_query('SELECT * FROM table_name', conn)

# ‡∏≠‡πà‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á table
df = pd.read_sql_table('table_name', conn)

# ‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠
conn.close()
```

### ‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

```python
df = pd.DataFrame({
    'Name': ['Alice', 'Bob', 'Charlie'],
    'Age': [25, 30, 35],
    'Salary': [50000, 60000, 70000]
})

# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô CSV
df.to_csv('output.csv', index=False)

# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô Excel
df.to_excel('output.xlsx', sheet_name='Data', index=False)

# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô JSON
df.to_json('output.json', orient='records', indent=2)

# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
conn = sqlite3.connect('database.db')
df.to_sql('table_name', conn, if_exists='replace', index=False)
conn.close()

# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô
df[['Name', 'Age']].to_csv('partial_data.csv', index=False)
```

---

## Data Selection ‡πÅ‡∏•‡∏∞ Filtering

### ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

#### Basic Selection
```python
df = pd.DataFrame({
    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],
    'Age': [25, 30, 35, 28, 32],
    'City': ['NY', 'London', 'Tokyo', 'Paris', 'Berlin'],
    'Salary': [50000, 60000, 70000, 55000, 65000],
    'Department': ['IT', 'HR', 'IT', 'Finance', 'IT']
})

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å columns
single_col = df['Name']                    # Series
multi_cols = df[['Name', 'Age', 'Salary']] # DataFrame

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å rows ‡∏î‡πâ‡∏ß‡∏¢ conditions
high_salary = df[df['Salary'] > 60000]
it_employees = df[df['Department'] == 'IT']
young_high_earners = df[(df['Age'] < 30) & (df['Salary'] > 50000)]
```

#### iloc ‡πÅ‡∏•‡∏∞ loc
```python
# iloc - position-based selection
print(df.iloc[0])              # ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å
print(df.iloc[0:3])            # ‡πÅ‡∏ñ‡∏ß 0-2
print(df.iloc[:, 1:3])         # columns 1-2 ‡∏ó‡∏∏‡∏Å‡πÅ‡∏ñ‡∏ß
print(df.iloc[0:2, 1:3])       # ‡πÅ‡∏ñ‡∏ß 0-1, columns 1-2

# loc - label-based selection
df_with_index = df.set_index('Name')
print(df_with_index.loc['Alice'])           # ‡πÅ‡∏ñ‡∏ß‡∏Ç‡∏≠‡∏á Alice
print(df_with_index.loc['Alice':'Charlie']) # ‡πÅ‡∏ñ‡∏ß Alice ‡∏ñ‡∏∂‡∏á Charlie
print(df_with_index.loc[:, 'Age':'Salary']) # columns Age ‡∏ñ‡∏∂‡∏á Salary
```

#### Query Method
```python
# ‡πÉ‡∏ä‡πâ query ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö filtering ‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô
result1 = df.query('Age > 30')
result2 = df.query('Age > 30 and Salary < 70000')
result3 = df.query('Department == "IT"')
result4 = df.query('City in ["NY", "Tokyo"]')

# ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ external
min_age = 30
result5 = df.query('Age > @min_age')
```

### Advanced Filtering

#### ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ isin()
```python
# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡πà‡∏≤‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô
cities_of_interest = ['NY', 'Tokyo', 'Paris']
filtered = df[df['City'].isin(cities_of_interest)]

# ‡∏ï‡∏£‡∏á‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á isin
not_in_cities = df[~df['City'].isin(cities_of_interest)]
```

#### String Filtering
```python
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
df_str = pd.DataFrame({
    'Name': ['Alice Johnson', 'Bob Smith', 'Charlie Brown', 'Diana Wilson'],
    'Email': ['alice@gmail.com', 'bob@yahoo.com', 'charlie@gmail.com', 'diana@outlook.com']
})

# String methods
gmail_users = df_str[df_str['Email'].str.contains('gmail')]
starts_with_a = df_str[df_str['Name'].str.startswith('A')]
ends_with_son = df_str[df_str['Name'].str.endswith('son')]

# Case-insensitive search
contains_alice = df_str[df_str['Name'].str.contains('alice', case=False)]

# Regular expressions
pattern_match = df_str[df_str['Email'].str.match(r'\w+@gmail\.com')]
```

#### ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Missing Values ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Filter
```python
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ missing values
df_missing = pd.DataFrame({
    'A': [1, 2, np.nan, 4, 5],
    'B': [np.nan, 2, 3, 4, np.nan],
    'C': [1, 2, 3, 4, 5]
})

# Filter ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ missing values
no_missing = df_missing[df_missing['A'].notna()]

# Filter ‡∏ó‡∏µ‡πà‡∏°‡∏µ missing values
has_missing = df_missing[df_missing['A'].isna()]

# Filter ‡∏´‡∏•‡∏≤‡∏¢ columns
complete_rows = df_missing.dropna()  # ‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ missing values
any_missing = df_missing[df_missing.isna().any(axis=1)]  # ‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ missing values
```

---

## Data Cleaning

### ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Missing Values

#### ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Missing Values
```python
df = pd.DataFrame({
    'A': [1, 2, np.nan, 4, 5],
    'B': [np.nan, 2, 3, np.nan, 5],
    'C': [1, 2, 3, 4, 5],
    'D': ['a', 'b', None, 'd', 'e']
})

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö missing values
print(df.isnull().sum())           # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô missing values ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ column
print(df.isna().sum())             # ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ö isnull()
print(df.info())                   # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á missing values

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ missing values
print(df[df.isnull().any(axis=1)])

# ‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå‡∏Ç‡∏≠‡∏á missing values
missing_percent = (df.isnull().sum() / len(df)) * 100
print(missing_percent)
```

#### ‡∏Å‡∏≤‡∏£‡∏•‡∏ö Missing Values
```python
# ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ missing values
df_drop_rows = df.dropna()                    # ‡∏•‡∏ö‡∏ó‡∏∏‡∏Å‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ missing
df_drop_any = df.dropna(how='any')            # ‡∏•‡∏ö‡∏ñ‡πâ‡∏≤‡∏°‡∏µ missing ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏Ñ‡πà‡∏≤
df_drop_all = df.dropna(how='all')            # ‡∏•‡∏ö‡∏ñ‡πâ‡∏≤‡∏ó‡∏∏‡∏Å‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô missing

# ‡∏•‡∏ö columns ‡∏ó‡∏µ‡πà‡∏°‡∏µ missing values
df_drop_cols = df.dropna(axis=1)              # ‡∏•‡∏ö columns ‡∏ó‡∏µ‡πà‡∏°‡∏µ missing

# ‡∏•‡∏ö‡∏ñ‡πâ‡∏≤‡∏°‡∏µ missing values ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
df_thresh = df.dropna(thresh=3)               # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà missing ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 3 ‡∏Ñ‡πà‡∏≤

# ‡∏•‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ columns ‡πÄ‡∏â‡∏û‡∏≤‡∏∞
df_subset = df.dropna(subset=['A', 'B'])      # ‡∏•‡∏ö‡∏ñ‡πâ‡∏≤ A ‡∏´‡∏£‡∏∑‡∏≠ B ‡πÄ‡∏õ‡πá‡∏ô missing
```

#### ‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏¥‡∏° Missing Values
```python
# ‡πÄ‡∏ï‡∏¥‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏á‡∏ó‡∏µ‡πà
df_fill_zero = df.fillna(0)                   # ‡πÄ‡∏ï‡∏¥‡∏°‡∏î‡πâ‡∏ß‡∏¢ 0
df_fill_unknown = df.fillna('Unknown')        # ‡πÄ‡∏ï‡∏¥‡∏°‡∏î‡πâ‡∏ß‡∏¢ 'Unknown'

# ‡πÄ‡∏ï‡∏¥‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
df_fill_mean = df.fillna(df.mean())           # ‡πÄ‡∏ï‡∏¥‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ (numeric columns)
df_fill_median = df.fillna(df.median())       # ‡πÄ‡∏ï‡∏¥‡∏°‡∏î‡πâ‡∏ß‡∏¢ median

# ‡πÄ‡∏ï‡∏¥‡∏°‡πÅ‡∏ï‡πà‡∏•‡∏∞ column ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô
fill_values = {'A': df['A'].mean(), 'B': df['B'].median(), 'D': 'Unknown'}
df_fill_dict = df.fillna(value=fill_values)

# Forward fill ‡πÅ‡∏•‡∏∞ Backward fill
df_ffill = df.fillna(method='ffill')          # ‡πÄ‡∏ï‡∏¥‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
df_bfill = df.fillna(method='bfill')          # ‡πÄ‡∏ï‡∏¥‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡∏ñ‡∏±‡∏î‡πÑ‡∏õ

# Interpolation
df_interp = df.interpolate()                  # interpolate ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö numeric data
```

### ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Duplicates

```python
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ duplicates
df_dup = pd.DataFrame({
    'Name': ['Alice', 'Bob', 'Alice', 'Charlie', 'Bob'],
    'Age': [25, 30, 25, 35, 30],
    'City': ['NY', 'London', 'NY', 'Tokyo', 'London']
})

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö duplicates
print(df_dup.duplicated())                    # Boolean mask
print(df_dup.duplicated().sum())              # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô duplicate rows

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö duplicates ‡πÉ‡∏ô column ‡πÄ‡∏â‡∏û‡∏≤‡∏∞
print(df_dup.duplicated(subset=['Name']))

# ‡∏•‡∏ö duplicates
df_no_dup = df_dup.drop_duplicates()          # ‡∏•‡∏ö duplicate rows
df_no_dup_name = df_dup.drop_duplicates(subset=['Name'])  # ‡∏•‡∏ö duplicate names

# ‡πÄ‡∏Å‡πá‡∏ö duplicate ‡πÅ‡∏£‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
df_keep_first = df_dup.drop_duplicates(keep='first')
df_keep_last = df_dup.drop_duplicates(keep='last')
df_remove_all = df_dup.drop_duplicates(keep=False)  # ‡∏•‡∏ö‡∏ó‡∏∏‡∏Å duplicates
```

### Data Type Conversion

```python
df = pd.DataFrame({
    'A': ['1', '2', '3', '4'],           # string numbers
    'B': [1.0, 2.0, 3.0, 4.0],          # floats
    'C': ['2023-01-01', '2023-01-02'],   # date strings
    'D': ['True', 'False', 'True']       # boolean strings
})

# ‡πÅ‡∏õ‡∏•‡∏á data types
df['A'] = df['A'].astype(int)             # string to int
df['B'] = df['B'].astype(int)             # float to int
df['C'] = pd.to_datetime(df['C'])         # string to datetime
df['D'] = df['D'].astype(bool)            # string to boolean

# ‡πÅ‡∏õ‡∏•‡∏á‡∏´‡∏•‡∏≤‡∏¢ columns ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô
df = df.astype({
    'A': int,
    'B': float,
    'D': bool
})

# ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ errors ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á
df_mixed = pd.DataFrame({'col': ['1', '2', 'text', '4']})
df_mixed['col_numeric'] = pd.to_numeric(df_mixed['col'], errors='coerce')  # NaN ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö errors
```

### ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î String Data

```python
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î
df_messy = pd.DataFrame({
    'Name': ['  Alice  ', 'BOB', 'charlie brown', 'DIANA'],
    'Email': ['ALICE@GMAIL.COM', 'bob@yahoo.com', 'Charlie@Gmail.com', 'diana@OUTLOOK.COM'],
    'Phone': ['123-456-7890', '(234) 567-8901', '345.678.9012', '456 789 0123']
})

# ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î strings
df_clean = df_messy.copy()

# ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏á
df_clean['Name'] = df_clean['Name'].str.strip()

# ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏•‡πá‡∏Å/‡πÉ‡∏´‡∏ç‡πà
df_clean['Email'] = df_clean['Email'].str.lower()
df_clean['Name'] = df_clean['Name'].str.title()  # Title Case

# ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà characters
df_clean['Phone'] = df_clean['Phone'].str.replace(r'[^\d]', '', regex=True)  # ‡πÄ‡∏Å‡πá‡∏ö‡πÅ‡∏ï‡πà‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç

# Split strings
name_parts = df_clean['Name'].str.split(' ', expand=True)
name_parts.columns = ['First_Name', 'Last_Name']

# Extract patterns ‡∏î‡πâ‡∏ß‡∏¢ regex
email_domain = df_clean['Email'].str.extract(r'@(.+)')
email_domain.columns = ['Domain']

print(df_clean)
```

### ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç Outliers

```python
import matplotlib.pyplot as plt

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ outliers
np.random.seed(42)
normal_data = np.random.normal(50, 10, 95)
outliers = [120, 130, -20, -10, 200]
data_with_outliers = np.concatenate([normal_data, outliers])

df_outliers = pd.DataFrame({'value': data_with_outliers})

# ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 1: Z-Score Method
z_scores = np.abs((df_outliers['value'] - df_outliers['value'].mean()) / df_outliers['value'].std())
outlier_zscore = df_outliers[z_scores > 3]
print(f"Outliers by Z-score: {len(outlier_zscore)}")

# ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 2: IQR Method
Q1 = df_outliers['value'].quantile(0.25)
Q3 = df_outliers['value'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outlier_iqr = df_outliers[(df_outliers['value'] < lower_bound) | 
                          (df_outliers['value'] > upper_bound)]
print(f"Outliers by IQR: {len(outlier_iqr)}")

# ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ outliers
# 1. ‡∏•‡∏ö outliers
df_no_outliers = df_outliers[(df_outliers['value'] >= lower_bound) & 
                             (df_outliers['value'] <= upper_bound)]

# 2. Cap outliers (winsorizing)
df_capped = df_outliers.copy()
df_capped['value'] = df_capped['value'].clip(lower_bound, upper_bound)

# 3. Transform data
df_log = df_outliers.copy()
df_log['value_log'] = np.log1p(df_log['value'] - df_log['value'].min() + 1)
```

---

## Data Transformation

### ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Columns ‡πÉ‡∏´‡∏°‡πà

```python
df = pd.DataFrame({
    'Name': ['Alice', 'Bob', 'Charlie', 'Diana'],
    'Age': [25, 30, 35, 28],
    'Salary': [50000, 60000, 70000, 55000],
    'Years_Experience': [3, 8, 12, 5]
})

# ‡∏™‡∏£‡πâ‡∏≤‡∏á column ‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì
df['Salary_per_year_exp'] = df['Salary'] / df['Years_Experience']
df['Age_Group'] = df['Age'].apply(lambda x: 'Young' if x < 30 else 'Senior')

# ‡πÉ‡∏ä‡πâ np.where ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö conditional logic
df['Performance'] = np.where(df['Salary'] > 60000, 'High', 'Normal')

# ‡πÉ‡∏ä‡πâ pd.cut ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö binning
df['Age_Bin'] = pd.cut(df['Age'], bins=[0, 25, 30, 35, 100], 
                       labels=['Very Young', 'Young', 'Middle', 'Senior'])

# Multiple conditions
conditions = [
    (df['Age'] < 30) & (df['Salary'] > 55000),
    (df['Age'] >= 30) & (df['Salary'] > 65000),
    (df['Age'] >= 30) & (df['Salary'] <= 65000)
]
choices = ['High Potential', 'Senior High Performer', 'Senior Normal']
df['Category'] = np.select(conditions, choices, default='Other')

print(df)
```

### ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Apply Functions

```python
# Apply function ‡∏Å‡∏±‡∏ö Series
def categorize_salary(salary):
    if salary < 55000:
        return 'Low'
    elif salary < 65000:
        return 'Medium'
    else:
        return 'High'

df['Salary_Category'] = df['Salary'].apply(categorize_salary)

# Apply ‡∏Å‡∏±‡∏ö lambda function
df['Name_Length'] = df['Name'].apply(lambda x: len(x))
df['Salary_K'] = df['Salary'].apply(lambda x: f"{x/1000:.0f}K")

# Apply ‡∏Å‡∏±‡∏ö DataFrame (axis=1 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö rows)
def calculate_bonus(row):
    base_bonus = row['Salary'] * 0.1
    experience_bonus = row['Years_Experience'] * 1000
    return base_bonus + experience_bonus

df['Bonus'] = df.apply(calculate_bonus, axis=1)

# Apply ‡∏Å‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢ columns
df['Salary_Age_Ratio'] = df.apply(lambda row: row['Salary'] / row['Age'], axis=1)
```

### Map ‡πÅ‡∏•‡∏∞ Replace

```python
# Map ‡∏î‡πâ‡∏ß‡∏¢ dictionary
age_mapping = {25: 'Young', 30: 'Middle', 35: 'Senior', 28: 'Young'}
df['Age_Category'] = df['Age'].map(age_mapping)

# Map ‡∏î‡πâ‡∏ß‡∏¢ Series
salary_avg = df.groupby('Age_Group')['Salary'].mean()
df['Avg_Salary_for_Group'] = df['Age_Group'].map(salary_avg)

# Replace values
df_replace = df.copy()
df_replace['Name'] = df_replace['Name'].replace('Alice', 'Alicia')
df_replace['Performance'] = df_replace['Performance'].replace({'High': 'Excellent', 'Normal': 'Good'})

# Replace with regex
df_replace['Name'] = df_replace['Name'].str.replace(r'[aeiou]', 'X', regex=True)
```

### Pivot Tables ‡πÅ‡∏•‡∏∞ Reshaping

```python
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö pivot
sales_data = pd.DataFrame({
    'Date': pd.date_range('2023-01-01', periods=12, freq='M'),
    'Product': ['A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B'],
    'Region': ['North', 'North', 'South', 'South', 'North', 'North', 
               'South', 'South', 'North', 'North', 'South', 'South'],
    'Sales': [100, 150, 120, 180, 110, 160, 130, 190, 105, 155, 125, 195]
})

# Pivot table
pivot_table = sales_data.pivot_table(
    values='Sales',
    index='Product',
    columns='Region',
    aggfunc='mean'
)
print(pivot_table)

# Melt (inverse of pivot)
melted = pivot_table.reset_index().melt(
    id_vars='Product',
    var_name='Region',
    value_name='Average_Sales'
)
print(melted)

# Stack ‡πÅ‡∏•‡∏∞ Unstack
stacked = pivot_table.stack()
unstacked = stacked.unstack()
```

---

## Groupby Operations

### ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô Groupby

```python
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
employees = pd.DataFrame({
    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank'],
    'Department': ['IT', 'HR', 'IT', 'Finance', 'IT', 'HR'],
    'Age': [25, 30, 35, 28, 32, 27],
    'Salary': [50000, 60000, 70000, 55000, 65000, 58000],
    'Years_Experience': [3, 8, 12, 5, 9, 6]
})

# Basic groupby operations
dept_groups = employees.groupby('Department')

# Aggregation functions
print(dept_groups.mean())              # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
print(dept_groups.sum())               # ‡∏ú‡∏•‡∏£‡∏ß‡∏°
print(dept_groups.count())             # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô
print(dept_groups.std())               # ‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏ö‡∏µ‡πà‡∏¢‡∏á‡πÄ‡∏ö‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô
print(dept_groups.describe())          # ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å column ‡πÄ‡∏â‡∏û‡∏≤‡∏∞
print(dept_groups['Salary'].mean())
print(dept_groups[['Salary', 'Age']].mean())
```

### Advanced Groupby Operations

```python
# Multiple groupby columns
age_dept_groups = employees.groupby(['Department', 'Age'])
print(age_dept_groups.size())          # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÉ‡∏ô group

# Custom aggregation functions
def salary_range(series):
    return series.max() - series.min()

custom_agg = employees.groupby('Department')['Salary'].agg([
    'mean',
    'median',
    'std',
    ('range', salary_range),
    ('count', 'count')
])
print(custom_agg)

# Different aggregations for different columns
multi_agg = employees.groupby('Department').agg({
    'Salary': ['mean', 'max', 'min'],
    'Age': ['mean', 'std'],
    'Years_Experience': 'mean'
})
print(multi_agg)

# Apply custom functions
def department_analysis(group):
    return pd.Series({
        'avg_salary': group['Salary'].mean(),
        'max_experience': group['Years_Experience'].max(),
        'avg_age': group['Age'].mean(),
        'salary_per_experience': group['Salary'].sum() / group['Years_Experience'].sum()
    })

dept_analysis = employees.groupby('Department').apply(department_analysis)
print(dept_analysis)
```

### Transform ‡πÅ‡∏•‡∏∞ Filter

```python
# Transform - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö original
employees['Dept_Avg_Salary'] = employees.groupby('Department')['Salary'].transform('mean')
employees['Salary_vs_Dept_Avg'] = employees['Salary'] - employees['Dept_Avg_Salary']
employees['Salary_Rank_in_Dept'] = employees.groupby('Department')['Salary'].rank(ascending=False)

# Filter - ‡∏Å‡∏£‡∏≠‡∏á groups ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç
large_departments = employees.groupby('Department').filter(lambda x: len(x) >= 2)
high_avg_salary_depts = employees.groupby('Department').filter(lambda x: x['Salary'].mean() > 60000)

print(employees)
print("\nLarge departments:")
print(large_departments)
```

### Rolling ‡πÅ‡∏•‡∏∞ Expanding Operations

```python
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• time series
dates = pd.date_range('2023-01-01', periods=100, freq='D')
ts_data = pd.DataFrame({
    'Date': dates,
    'Value': np.random.randn(100).cumsum() + 100
})
ts_data.set_index('Date', inplace=True)

# Rolling operations
ts_data['Rolling_Mean_7'] = ts_data['Value'].rolling(window=7).mean()
ts_data['Rolling_Std_7'] = ts_data['Value'].rolling(window=7).std()
ts_data['Rolling_Max_7'] = ts_data['Value'].rolling(window=7).max()

# Expanding operations (‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ñ‡∏∂‡∏á‡∏à‡∏∏‡∏î‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô)
ts_data['Expanding_Mean'] = ts_data['Value'].expanding().mean()
ts_data['Expanding_Std'] = ts_data['Value'].expanding().std()

# Rolling ‡∏Å‡∏±‡∏ö groupby
grouped_employees = employees.copy()
grouped_employees['Salary_Rank_Overall'] = grouped_employees['Salary'].rank(ascending=False)

print(ts_data.head(10))
```

---

## Merging ‡πÅ‡∏•‡∏∞ Joining

### ‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏° DataFrames

```python
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
employees = pd.DataFrame({
    'EmployeeID': [1, 2, 3, 4, 5],
    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],
    'DepartmentID': [101, 102, 101, 103, 102]
})

departments = pd.DataFrame({
    'DepartmentID': [101, 102, 103, 104],
    'Department': ['IT', 'HR', 'Finance', 'Marketing'],
    'Location': ['Building A', 'Building B', 'Building C', 'Building D']
})

salaries = pd.DataFrame({
    'EmployeeID': [1, 2, 3, 4, 6],
    'Salary': [50000, 60000, 70000, 55000, 48000],
    'Bonus': [5000, 6000, 7000, 5500, 4800]
})
```

#### Inner Join
```python
# Inner join - ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ records ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô table ‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á
emp_dept = pd.merge(employees, departments, on='DepartmentID', how='inner')
print("Inner Join:")
print(emp_dept)

emp_salary = pd.merge(employees, salaries, on='EmployeeID', how='inner')
print("\nEmployee-Salary Inner Join:")
print(emp_salary)
```

#### Left, Right, ‡πÅ‡∏•‡∏∞ Outer Joins
```python
# Left join - ‡πÄ‡∏Å‡πá‡∏ö‡∏ó‡∏∏‡∏Å records ‡∏à‡∏≤‡∏Å left table
left_join = pd.merge(employees, salaries, on='EmployeeID', how='left')
print("Left Join:")
print(left_join)

# Right join - ‡πÄ‡∏Å‡πá‡∏ö‡∏ó‡∏∏‡∏Å records ‡∏à‡∏≤‡∏Å right table
right_join = pd.merge(employees, salaries, on='EmployeeID', how='right')
print("Right Join:")
print(right_join)

# Outer join - ‡πÄ‡∏Å‡πá‡∏ö‡∏ó‡∏∏‡∏Å records ‡∏à‡∏≤‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á tables
outer_join = pd.merge(employees, salaries, on='EmployeeID', how='outer')
print("Outer Join:")
print(outer_join)
```

#### Advanced Merging
```python
# Merge ‡∏Å‡∏±‡∏ö different column names
emp_modified = employees.rename(columns={'EmployeeID': 'EmpID'})
merge_diff_names = pd.merge(emp_modified, salaries, 
                           left_on='EmpID', right_on='EmployeeID')

# Merge ‡∏Å‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢ columns
combined_data = pd.merge(
    pd.merge(employees, departments, on='DepartmentID'),
    salaries, on='EmployeeID', how='left'
)
print("Complete Employee Data:")
print(combined_data)

# Merge ‡∏Å‡∏±‡∏ö suffixes ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö column ‡∏ó‡∏µ‡πà‡∏ä‡∏∑‡πà‡∏≠‡∏ã‡πâ‡∏≥
df1 = pd.DataFrame({'key': ['A', 'B', 'C'], 'value': [1, 2, 3]})
df2 = pd.DataFrame({'key': ['A', 'B', 'D'], 'value': [4, 5, 6]})
merged_suffix = pd.merge(df1, df2, on='key', how='outer', suffixes=('_left', '_right'))
print("Merge with suffixes:")
print(merged_suffix)
```

### Concatenation

```python
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
df1 = pd.DataFrame({
    'A': [1, 2, 3],
    'B': [4, 5, 6]
})

df2 = pd.DataFrame({
    'A': [7, 8, 9],
    'B': [10, 11, 12]
})

df3 = pd.DataFrame({
    'C': [13, 14, 15],
    'D': [16, 17, 18]
})

# Vertical concatenation (‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏ñ‡∏ß)
vertical_concat = pd.concat([df1, df2], axis=0, ignore_index=True)
print("Vertical Concatenation:")
print(vertical_concat)

# Horizontal concatenation (‡πÄ‡∏û‡∏¥‡πà‡∏° columns)
horizontal_concat = pd.concat([df1, df3], axis=1)
print("Horizontal Concatenation:")
print(horizontal_concat)

# Concatenation ‡∏Å‡∏±‡∏ö keys
keyed_concat = pd.concat([df1, df2], keys=['Group1', 'Group2'])
print("Concatenation with keys:")
print(keyed_concat)
```

### Join Method

```python
# ‡πÉ‡∏ä‡πâ join method (‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö index-based joining)
df_indexed1 = employees.set_index('EmployeeID')
df_indexed2 = salaries.set_index('EmployeeID')

# Join (default ‡πÄ‡∏õ‡πá‡∏ô left join)
joined = df_indexed1.join(df_indexed2)
print("Join result:")
print(joined)

# Join ‡∏Å‡∏±‡∏ö different join types
inner_joined = df_indexed1.join(df_indexed2, how='inner')
outer_joined = df_indexed1.join(df_indexed2, how='outer')
```

---

## Time Series Analysis

### ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Datetime

```python
# ‡∏™‡∏£‡πâ‡∏≤‡∏á datetime data
dates = pd.date_range('2023-01-01', periods=100, freq='D')
ts_data = pd.DataFrame({
    'Date': dates,
    'Sales': np.random.randint(100, 1000, 100),
    'Temperature': np.random.normal(25, 5, 100)
})

# ‡πÅ‡∏õ‡∏•‡∏á string ‡πÄ‡∏õ‡πá‡∏ô datetime
date_strings = ['2023-01-01', '2023-01-02', '2023-01-03']
converted_dates = pd.to_datetime(date_strings)

# Parse datetime ‡∏à‡∏≤‡∏Å format ‡πÄ‡∏â‡∏û‡∏≤‡∏∞
custom_dates = ['01/15/2023', '02/15/2023', '03/15/2023']
parsed_dates = pd.to_datetime(custom_dates, format='%m/%d/%Y')

# Set datetime ‡πÄ‡∏õ‡πá‡∏ô index
ts_data.set_index('Date', inplace=True)
print(ts_data.head())
```

### Time-based Indexing ‡πÅ‡∏•‡∏∞ Slicing

```python
# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏° date range
jan_data = ts_data['2023-01']                    # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°
first_week = ts_data['2023-01-01':'2023-01-07']  # ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡πÅ‡∏£‡∏Å
recent_data = ts_data.last('30D')                # 30 ‡∏ß‡∏±‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ß‡∏±‡∏ô
mondays = ts_data[ts_data.index.dayofweek == 0]  # ‡∏ß‡∏±‡∏ô‡∏à‡∏±‡∏ô‡∏ó‡∏£‡πå
weekends = ts_data[ts_data.index.dayofweek.isin([5, 6])]  # ‡πÄ‡∏™‡∏≤‡∏£‡πå-‡∏≠‡∏≤‡∏ó‡∏¥‡∏ï‡∏¢‡πå

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏ß‡∏•‡∏≤ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ time component)
ts_hourly = pd.DataFrame({
    'Datetime': pd.date_range('2023-01-01', periods=48, freq='H'),
    'Value': np.random.randn(48)
}).set_index('Datetime')

business_hours = ts_hourly.between_time('09:00', '17:00')
```

### Resampling ‡πÅ‡∏•‡∏∞ Frequency Conversion

```python
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô
daily_data = pd.DataFrame({
    'Date': pd.date_range('2023-01-01', periods=365, freq='D'),
    'Sales': np.random.randint(100, 1000, 365),
    'Visitors': np.random.randint(50, 500, 365)
}).set_index('Date')

# Upsample (‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà)
hourly_upsampled = daily_data.resample('H').interpolate()

# Downsample (‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà)
weekly_data = daily_data.resample('W').agg({
    'Sales': 'sum',
    'Visitors': 'mean'
})

monthly_data = daily_data.resample('M').agg({
    'Sales': ['sum', 'mean', 'std'],
    'Visitors': ['mean', 'max', 'min']
})

quarterly_data = daily_data.resample('Q').sum()

print("Weekly aggregated data:")
print(weekly_data.head())
```

### Time-based Calculations

```python
# Shift operations
daily_data['Sales_Previous_Day'] = daily_data['Sales'].shift(1)
daily_data['Sales_Next_Day'] = daily_data['Sales'].shift(-1)
daily_data['Sales_Weekly_Lag'] = daily_data['Sales'].shift(7)

# Percentage change
daily_data['Sales_Pct_Change'] = daily_data['Sales'].pct_change()
daily_data['Sales_Pct_Change_Weekly'] = daily_data['Sales'].pct_change(periods=7)

# Cumulative operations
daily_data['Sales_Cumsum'] = daily_data['Sales'].cumsum()
daily_data['Sales_Cummax'] = daily_data['Sales'].cummax()

# Rolling statistics
daily_data['Sales_MA_7'] = daily_data['Sales'].rolling(window=7).mean()
daily_data['Sales_MA_30'] = daily_data['Sales'].rolling(window=30).mean()
daily_data['Sales_Std_7'] = daily_data['Sales'].rolling(window=7).std()

# Expanding statistics
daily_data['Sales_Expanding_Mean'] = daily_data['Sales'].expanding().mean()

print(daily_data[['Sales', 'Sales_MA_7', 'Sales_MA_30']].head(10))
```

### Time Zone Handling

```python
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏±‡∏ö timezone
utc_data = pd.DataFrame({
    'Datetime': pd.date_range('2023-01-01', periods=24, freq='H', tz='UTC'),
    'Value': np.random.randn(24)
}).set_index('Datetime')

# ‡πÅ‡∏õ‡∏•‡∏á timezone
bkk_data = utc_data.tz_convert('Asia/Bangkok')
ny_data = utc_data.tz_convert('America/New_York')

# Localize timezone (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ timezone)
naive_data = pd.DataFrame({
    'Datetime': pd.date_range('2023-01-01', periods=24, freq='H'),
    'Value': np.random.randn(24)
}).set_index('Datetime')

localized_data = naive_data.tz_localize('Asia/Bangkok')
```

---

## Visualization

### ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Pandas Plot

```python
import matplotlib.pyplot as plt

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
data = pd.DataFrame({
    'Date': pd.date_range('2023-01-01', periods=100, freq='D'),
    'Sales': np.random.randint(100, 1000, 100),
    'Profit': np.random.randint(10, 100, 100),
    'Category': np.random.choice(['A', 'B', 'C'], 100)
}).set_index('Date')

# Line plot
data['Sales'].plot(kind='line', title='Sales Over Time', figsize=(10, 6))
plt.show()

# Multiple lines
data[['Sales', 'Profit']].plot(kind='line', title='Sales and Profit', figsize=(10, 6))
plt.show()

# Bar plot
monthly_sales = data.resample('M')['Sales'].sum()
monthly_sales.plot(kind='bar', title='Monthly Sales', figsize=(10, 6))
plt.show()

# Histogram
data['Sales'].plot(kind='hist', bins=20, title='Sales Distribution', figsize=(8, 6))
plt.show()

# Box plot
data.boxplot(column=['Sales', 'Profit'], figsize=(8, 6))
plt.show()

# Scatter plot
data.plot(kind='scatter', x='Sales', y='Profit', title='Sales vs Profit', figsize=(8, 6))
plt.show()
```

### Advanced Plotting

```python
# Group by plots
category_data = data.groupby('Category')['Sales'].sum()
category_data.plot(kind='pie', title='Sales by Category', figsize=(8, 8))
plt.show()

# Subplots
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

data['Sales'].plot(ax=axes[0,0], title='Sales')
data['Profit'].plot(ax=axes[0,1], title='Profit')
data['Sales'].hist(ax=axes[1,0], bins=20, title='Sales Distribution')
data.plot(kind='scatter', x='Sales', y='Profit', ax=axes[1,1], title='Sales vs Profit')

plt.tight_layout()
plt.show()

# Rolling average plot
data['Sales_MA'] = data['Sales'].rolling(window=10).mean()
data[['Sales', 'Sales_MA']].plot(title='Sales with Moving Average', figsize=(12, 6))
plt.show()
```

---

## Performance Optimization

### Memory Optimization

```python
# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ memory
def memory_usage(df):
    return df.memory_usage(deep=True).sum() / 1024 / 1024  # MB

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
large_df = pd.DataFrame({
    'int_col': np.random.randint(0, 100, 1000000),
    'float_col': np.random.random(1000000),
    'str_col': np.random.choice(['A', 'B', 'C', 'D'], 1000000)
})

print(f"Original memory usage: {memory_usage(large_df):.2f} MB")

# Optimize data types
optimized_df = large_df.copy()

# Integer optimization
optimized_df['int_col'] = pd.to_numeric(optimized_df['int_col'], downcast='integer')

# Float optimization
optimized_df['float_col'] = pd.to_numeric(optimized_df['float_col'], downcast='float')

# Category optimization ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö string ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏ã‡πâ‡∏≥‡πÜ
optimized_df['str_col'] = optimized_df['str_col'].astype('category')

print(f"Optimized memory usage: {memory_usage(optimized_df):.2f} MB")
print(f"Memory reduction: {(memory_usage(large_df) - memory_usage(optimized_df))/memory_usage(large_df)*100:.1f}%")

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö data types
print("\nOriginal dtypes:")
print(large_df.dtypes)
print("\nOptimized dtypes:")
print(optimized_df.dtypes)
```

### Vectorization vs Loops

```python
import time

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà
df_large = pd.DataFrame({
    'A': np.random.randint(1, 100, 100000),
    'B': np.random.randint(1, 100, 100000)
})

# Method 1: Loop (‡∏ä‡πâ‡∏≤)
start_time = time.time()
result_loop = []
for i in range(len(df_large)):
    result_loop.append(df_large.iloc[i]['A'] * df_large.iloc[i]['B'])
df_large['Result_Loop'] = result_loop
loop_time = time.time() - start_time

# Method 2: Vectorization (‡πÄ‡∏£‡πá‡∏ß)
start_time = time.time()
df_large['Result_Vectorized'] = df_large['A'] * df_large['B']
vectorized_time = time.time() - start_time

# Method 3: Apply (‡∏Å‡∏•‡∏≤‡∏á)
start_time = time.time()
df_large['Result_Apply'] = df_large.apply(lambda row: row['A'] * row['B'], axis=1)
apply_time = time.time() - start_time

print(f"Loop time: {loop_time:.4f} seconds")
print(f"Vectorized time: {vectorized_time:.4f} seconds")
print(f"Apply time: {apply_time:.4f} seconds")
print(f"Vectorization speedup: {loop_time/vectorized_time:.1f}x")
```

### Efficient Data Loading

```python
# ‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û
def efficient_csv_reading():
    # ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞ columns ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
    df = pd.read_csv('large_file.csv', usecols=['col1', 'col2', 'col3'])
    
    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î data types ‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤
    dtypes = {
        'col1': 'int32',
        'col2': 'category',
        'col3': 'float32'
    }
    df = pd.read_csv('large_file.csv', dtype=dtypes)
    
    # ‡∏≠‡πà‡∏≤‡∏ô‡∏ó‡∏µ‡∏•‡∏∞ chunk ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà
    chunk_size = 10000
    chunks = []
    for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):
        # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• chunk
        processed_chunk = chunk.groupby('category').sum()
        chunks.append(processed_chunk)
    
    # ‡∏£‡∏ß‡∏° chunks
    result = pd.concat(chunks, ignore_index=True)
    return result

# ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ compression
df = pd.read_csv('data.csv.gz', compression='gzip')  # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà compress
df.to_csv('output.csv.gz', compression='gzip', index=False)  # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏ö‡∏ö compress
```

### Query Optimization

```python
# ‡πÉ‡∏ä‡πâ query ‡πÅ‡∏ó‡∏ô boolean indexing ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö performance ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤
large_df = pd.DataFrame({
    'A': np.random.randint(1, 1000, 1000000),
    'B': np.random.randint(1, 1000, 1000000),
    'C': np.random.choice(['X', 'Y', 'Z'], 1000000)
})

# Slow: boolean indexing
start_time = time.time()
result1 = large_df[(large_df['A'] > 500) & (large_df['B'] < 300) & (large_df['C'] == 'X')]
boolean_time = time.time() - start_time

# Fast: query method
start_time = time.time()
result2 = large_df.query('A > 500 and B < 300 and C == "X"')
query_time = time.time() - start_time

print(f"Boolean indexing: {boolean_time:.4f} seconds")
print(f"Query method: {query_time:.4f} seconds")
```

---

## ‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î

### ‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î‡∏ó‡∏µ‡πà 1: Series ‡πÅ‡∏•‡∏∞ DataFrame ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô (15 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô)

#### ‡∏Ç‡πâ‡∏≠ 1.1 (5 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô)
‡∏™‡∏£‡πâ‡∏≤‡∏á Series ‡πÅ‡∏•‡∏∞ DataFrame ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î:

```python
# TODO: ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡∏£‡πâ‡∏≤‡∏á
# a) Series ‡∏ó‡∏µ‡πà‡∏°‡∏µ index ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏î‡∏∑‡∏≠‡∏ô ‡πÅ‡∏•‡∏∞ values ‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏±‡∏ô‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏î‡∏∑‡∏≠‡∏ô
# b) DataFrame ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô 5 ‡∏Ñ‡∏ô ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢:
#    - ‡∏ä‡∏∑‡πà‡∏≠ (Name)
#    - ‡∏≠‡∏≤‡∏¢‡∏∏ (Age) 
#    - ‡πÄ‡∏Å‡∏£‡∏î (Grade)
#    - ‡πÄ‡∏°‡∏∑‡∏≠‡∏á (City)
# c) ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á DataFrame (shape, dtypes, info)
```

**‡πÄ‡∏â‡∏•‡∏¢:**
```python
# a) Series ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô
months_days = pd.Series(
    [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31],
    index=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',
           'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'],
    name='Days_in_Month'
)

# b) DataFrame ‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô
students = pd.DataFrame({
    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],
    'Age': [20, 21, 19, 22, 20],
    'Grade': ['A', 'B', 'A', 'C', 'B'],
    'City': ['Bangkok', 'Chiang Mai', 'Phuket', 'Bangkok', 'Pattaya']
})

# c) ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô
print(f"Shape: {students.shape}")
print(f"Data types:\n{students.dtypes}")
print(f"Info:\n{students.info()}")
```

#### ‡∏Ç‡πâ‡∏≠ 1.2 (5 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô)
‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ ‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ selection ‡πÅ‡∏•‡∏∞ filtering:

```python
sales_data = pd.DataFrame({
    'Date': pd.date_range('2023-01-01', periods=50, freq='D'),
    'Product': np.random.choice(['A', 'B', 'C'], 50),
    'Sales': np.random.randint(100, 1000, 50),
    'Region': np.random.choice(['North', 'South', 'East', 'West'], 50)
})

# TODO: ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î
# a) ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ 'A' ‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 500
# b) ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏†‡∏π‡∏°‡∏¥‡∏†‡∏≤‡∏Ñ
# c) ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 10 ‡∏ß‡∏±‡∏ô‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°
# d) ‡∏™‡∏£‡πâ‡∏≤‡∏á column ‡πÉ‡∏´‡∏°‡πà‡∏ä‡∏∑‡πà‡∏≠ 'Sales_Category' ‡∏ó‡∏µ‡πà‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô 'High' (>700), 'Medium' (400-700), 'Low' (<400)
```

#### ‡∏Ç‡πâ‡∏≠ 1.3 (5 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô)
‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå DataFrame:

```python
def analyze_dataframe(df, numeric_cols=None):
    """
    ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå DataFrame ‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡∏Å‡∏•‡∏±‡∏ö‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•
    
    Parameters:
    df: DataFrame ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
    numeric_cols: list ‡∏Ç‡∏≠‡∏á numeric columns (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏ó‡∏∏‡∏Å numeric columns)
    
    Returns:
    dict ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:
    - shape: ‡∏£‡∏π‡∏õ‡∏£‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á DataFrame
    - missing_values: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô missing values ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ column
    - numeric_summary: ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á numeric columns
    - categorical_summary: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô unique values ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ categorical column
    """
    # TODO: implement function
    pass

# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô
test_data = pd.DataFrame({
    'A': [1, 2, np.nan, 4, 5],
    'B': ['x', 'y', 'x', 'z', 'y'],
    'C': [10.5, 20.3, 30.1, np.nan, 50.2]
})

result = analyze_dataframe(test_data)
```

### ‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î‡∏ó‡∏µ‡πà 2: Data Cleaning (20 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô)

#### ‡∏Ç‡πâ‡∏≠ 2.1 (10 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô)
‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ ‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î:

```python
messy_data = pd.DataFrame({
    'Name': ['  Alice  ', 'BOB', 'charlie brown', '  DIANA', 'Eve'],
    'Age': [25, 'thirty', 35, np.nan, '28'],
    'Email': ['alice@gmail.com', 'BOB@YAHOO.COM', 'charlie@gmail.com', 'diana@', 'eve@outlook.com'],
    'Salary': ['50,000', '60000', 'sixty thousand', '55,000', np.nan],
    'Phone': ['123-456-7890', '(234) 567-8901', '345.678.9012', '456 789 0123', '567-890-1234'],
    'Date_Joined': ['2023-01-01', '2023/02/15', '01-03-2023', '2023.04.10', '2023-05-20']
})

# TODO: ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
# 1. ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î Name column (trim, proper case)
# 2. ‡πÅ‡∏õ‡∏•‡∏á Age ‡πÄ‡∏õ‡πá‡∏ô numeric (handle text values)
# 3. ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î Email (lowercase, validate format)
# 4. ‡πÅ‡∏õ‡∏•‡∏á Salary ‡πÄ‡∏õ‡πá‡∏ô numeric (remove commas, handle text)
# 5. ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Phone ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô xxx-xxx-xxxx
# 6. ‡πÅ‡∏õ‡∏•‡∏á Date_Joined ‡πÄ‡∏õ‡πá‡∏ô datetime
# 7. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ missing values ‡∏î‡πâ‡∏ß‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
```

#### ‡∏Ç‡πâ‡∏≠ 2.2 (10 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô)
‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô data cleaning pipeline:

```python
def clean_employee_data(df):
    """
    Data cleaning pipeline ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô
    
    Parameters:
    df: DataFrame ‡∏ó‡∏µ‡πà‡∏°‡∏µ columns: Name, Age, Email, Salary, Phone, Date_Joined
    
    Returns:
    DataFrame ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡πâ‡∏ß
    
    Cleaning steps:
    1. Remove leading/trailing spaces from string columns
    2. Convert text ages to numeric
    3. Standardize email format (lowercase)
    4. Convert salary strings to numeric
    5. Standardize phone format
    6. Convert date strings to datetime
    7. Handle missing values appropriately
    8. Remove duplicates
    9. Add validation flags for invalid data
    """
    # TODO: implement cleaning pipeline
    pass

# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö pipeline
cleaned_data = clean_employee_data(messy_data)
print("Cleaned data:")
print(cleaned_data)
print(f"\nData types after cleaning:\n{cleaned_data.dtypes}")
```

### ‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î‡∏ó‡∏µ‡πà 3: Groupby ‡πÅ‡∏•‡∏∞ Aggregation (20 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô)

#### ‡∏Ç‡πâ‡∏≠ 3.1 (10 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô)
‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏Ç‡∏≤‡∏¢‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ:

```python
sales_analysis = pd.DataFrame({
    'Date': pd.date_range('2023-01-01', periods=365, freq='D'),
    'Product': np.random.choice(['Laptop', 'Phone', 'Tablet', 'Watch'], 365),
    'Category': np.random.choice(['Electronics', 'Accessories'], 365),
    'Region': np.random.choice(['North', 'South', 'East', 'West'], 365),
    'Salesperson': np.random.choice(['Alice', 'Bob', 'Charlie', 'Diana'], 365),
    'Units_Sold': np.random.randint(1, 20, 365),
    'Unit_Price': np.random.randint(100, 2000, 365)
})
sales_analysis['Total_Sales'] = sales_analysis['Units_Sold'] * sales_analysis['Unit_Price']

# TODO: ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
# 1. ‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏£‡∏ß‡∏°‡πÅ‡∏•‡∏∞‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤
# 2. Top 3 salesperson ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
# 3. ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏†‡∏π‡∏°‡∏¥‡∏†‡∏≤‡∏Ñ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏î‡∏∑‡∏≠‡∏ô
# 4. ‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏¢‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™
# 5. Seasonal analysis: ‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏î‡∏∑‡∏≠‡∏ô
```

#### ‡∏Ç‡πâ‡∏≠ 3.2 (10 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô)
‡∏™‡∏£‡πâ‡∏≤‡∏á advanced aggregation functions:

```python
def sales_metrics(group):
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢
    
    Returns:
    Series ‡∏ó‡∏µ‡πà‡∏°‡∏µ:
    - total_sales: ‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏£‡∏ß‡∏°
    - avg_sales: ‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
    - sales_std: ‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏ö‡∏µ‡πà‡∏¢‡∏á‡πÄ‡∏ö‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô
    - max_sales: ‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
    - min_sales: ‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î
    - sales_range: ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÅ‡∏•‡∏∞‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î
    - coefficient_of_variation: CV
    """
    # TODO: implement metrics calculation
    pass

def product_performance_analysis(df, date_col, product_col, sales_col):
    """
    ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå performance ‡∏Ç‡∏≠‡∏á‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤
    
    Parameters:
    df: DataFrame
    date_col: ‡∏ä‡∏∑‡πà‡∏≠ column ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô date
    product_col: ‡∏ä‡∏∑‡πà‡∏≠ column ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô product
    sales_col: ‡∏ä‡∏∑‡πà‡∏≠ column ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô sales
    
    Returns:
    DataFrame ‡∏ó‡∏µ‡πà‡∏°‡∏µ analysis results ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤
    """
    # TODO: implement product analysis
    pass

# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô
metrics_result = sales_analysis.groupby('Product').apply(sales_metrics)
performance_result = product_performance_analysis(sales_analysis, 'Date', 'Product', 'Total_Sales')
```

### ‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î‡∏ó‡∏µ‡πà 4: Time Series ‡πÅ‡∏•‡∏∞ Merging (25 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô)

#### ‡∏Ç‡πâ‡∏≠ 4.1 (15 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô)
‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå time series data:

```python
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• stock prices
stock_data = pd.DataFrame({
    'Date': pd.date_range('2023-01-01', periods=252, freq='B'),  # Business days
    'AAPL': np.random.randn(252).cumsum() + 150,
    'GOOGL': np.random.randn(252).cumsum() + 2500,
    'MSFT': np.random.randn(252).cumsum() + 250,
    'TSLA': np.random.randn(252).cumsum() + 200
}).set_index('Date')

# TODO: Time series analysis
# 1. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì daily returns ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏∏‡πâ‡∏ô
# 2. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì 20-day ‡πÅ‡∏•‡∏∞ 50-day moving averages
# 3. ‡∏´‡∏≤ correlation matrix ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏∏‡πâ‡∏ô‡∏ï‡πà‡∏≤‡∏á‡πÜ
# 4. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì monthly returns ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ resampling
# 5. ‡∏´‡∏≤‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ volatility ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (‡πÉ‡∏ä‡πâ rolling standard deviation)
# 6. ‡∏™‡∏£‡πâ‡∏≤‡∏á simple trading signal: ‡∏ã‡∏∑‡πâ‡∏≠‡πÄ‡∏°‡∏∑‡πà‡∏≠ 20-day MA > 50-day MA
```

#### ‡∏Ç‡πâ‡∏≠ 4.2 (10 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô)
Merging ‡πÅ‡∏•‡∏∞ joining exercises:

```python
# ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏£‡∏≤‡∏á
customers = pd.DataFrame({
    'CustomerID': [1, 2, 3, 4, 5],
    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],
    'City': ['Bangkok', 'Chiang Mai', 'Phuket', 'Bangkok', 'Pattaya'],
    'Age': [25, 30, 35, 28, 32]
})

orders = pd.DataFrame({
    'OrderID': [101, 102, 103, 104, 105, 106],
    'CustomerID': [1, 2, 1, 3, 2, 6],
    'Product': ['Laptop', 'Phone', 'Tablet', 'Watch', 'Laptop', 'Phone'],
    'Amount': [50000, 25000, 15000, 8000, 52000, 26000],
    'Order_Date': pd.date_range('2023-01-01', periods=6, freq='10D')
})

products = pd.DataFrame({
    'Product': ['Laptop', 'Phone', 'Tablet', 'Watch', 'Headphones'],
    'Category': ['Computer', 'Mobile', 'Computer', 'Wearable', 'Audio'],
    'Cost': [40000, 20000, 12000, 6000, 3000]
})

# TODO: Merging exercises
# 1. ‡∏£‡∏ß‡∏° customers ‡πÅ‡∏•‡∏∞ orders (‡πÅ‡∏™‡∏î‡∏á‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠)
# 2. ‡∏£‡∏ß‡∏° orders ‡πÅ‡∏•‡∏∞ products ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ profit ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ order
# 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á comprehensive report ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≤‡∏°‡∏ï‡∏≤‡∏£‡∏≤‡∏á
# 4. ‡∏´‡∏≤‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏°‡∏∑‡∏≠‡∏á
# 5. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏ï‡∏≤‡∏° category ‡πÅ‡∏•‡∏∞ customer demographics
```

### ‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î‡∏ó‡∏µ‡πà 5: Advanced Operations (20 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô)

#### ‡∏Ç‡πâ‡∏≠ 5.1 (10 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô)
‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ pivot tables ‡πÅ‡∏•‡∏∞ reshaping:

```python
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• survey
survey_data = pd.DataFrame({
    'RespondentID': range(1, 101),
    'Age_Group': np.random.choice(['18-25', '26-35', '36-45', '46-55', '55+'], 100),
    'Gender': np.random.choice(['Male', 'Female', 'Other'], 100),
    'Region': np.random.choice(['Bangkok', 'North', 'Northeast', 'Central', 'South'], 100),
    'Product_A_Rating': np.random.randint(1, 6, 100),
    'Product_B_Rating': np.random.randint(1, 6, 100),
    'Product_C_Rating': np.random.randint(1, 6, 100),
    'Overall_Satisfaction': np.random.randint(1, 6, 100)
})

# TODO: Pivot ‡πÅ‡∏•‡∏∞ reshape operations
# 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á pivot table ‡πÅ‡∏™‡∏î‡∏á average rating ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ product ‡∏ï‡∏≤‡∏° age group ‡πÅ‡∏•‡∏∞ gender
# 2. Melt data ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏õ‡∏•‡∏á product ratings ‡πÄ‡∏õ‡πá‡∏ô long format
# 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á cross-tabulation ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á region ‡πÅ‡∏•‡∏∞ age group
# 4. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì correlation ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á product ratings ‡πÅ‡∏•‡∏∞ overall satisfaction
# 5. ‡∏™‡∏£‡πâ‡∏≤‡∏á summary report ‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á insights ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
```

#### ‡∏Ç‡πâ‡∏≠ 5.2 (10 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô)
Performance optimization exercise:

```python
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà
large_dataset = pd.DataFrame({
    'ID': range(1000000),
    'Category': np.random.choice(['A', 'B', 'C', 'D', 'E'], 1000000),
    'Value1': np.random.randn(1000000),
    'Value2': np.random.randint(1, 1000, 1000000),
    'Date': pd.date_range('2020-01-01', periods=1000000, freq='min')
})

# TODO: Optimization exercises
# 1. ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö performance ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á iterrows(), apply(), ‡πÅ‡∏•‡∏∞ vectorization
# 2. Optimize memory usage ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô data types
# 3. ‡πÉ‡∏ä‡πâ categorical data type ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Category column
# 4. ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö performance ‡∏Ç‡∏≠‡∏á groupby operations ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà
# 5. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà process ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û

def optimize_dataframe(df):
    """
    Optimize DataFrame ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö memory ‡πÅ‡∏•‡∏∞ performance
    
    Returns:
    optimized DataFrame ‡πÅ‡∏•‡∏∞ report ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î memory
    """
    # TODO: implement optimization
    pass
```

