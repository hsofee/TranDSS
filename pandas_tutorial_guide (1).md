# à¹€à¸­à¸à¸ªà¸²à¸£à¸›à¸£à¸°à¸à¸­à¸šà¸à¸²à¸£à¸ªà¸­à¸™ Pandas
## à¸ªà¸³à¸«à¸£à¸±à¸šà¸™à¸±à¸à¸¨à¸¶à¸à¸©à¸²à¸›à¸£à¸´à¸à¸à¸²à¸•à¸£à¸µ

---

## ðŸ“Š à¸ªà¸²à¸£à¸šà¸±à¸
1. [à¸šà¸—à¸™à¸³ Pandas](#à¸šà¸—à¸™à¸³-pandas)
2. [à¸à¸²à¸£à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡à¹à¸¥à¸°à¸à¸²à¸£ Import](#à¸à¸²à¸£à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡à¹à¸¥à¸°à¸à¸²à¸£-import)
3. [Series à¸žà¸·à¹‰à¸™à¸à¸²à¸™](#series-à¸žà¸·à¹‰à¸™à¸à¸²à¸™)
4. [DataFrame à¸žà¸·à¹‰à¸™à¸à¸²à¸™](#dataframe-à¸žà¸·à¹‰à¸™à¸à¸²à¸™)
5. [à¸à¸²à¸£à¹‚à¸«à¸¥à¸”à¹à¸¥à¸°à¸šà¸±à¸™à¸—à¸¶à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥](#à¸à¸²à¸£à¹‚à¸«à¸¥à¸”à¹à¸¥à¸°à¸šà¸±à¸™à¸—à¸¶à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥)
6. [Data Selection à¹à¸¥à¸° Filtering](#data-selection-à¹à¸¥à¸°-filtering)
7. [Data Cleaning](#data-cleaning)
8. [Data Transformation](#data-transformation)
9. [Groupby Operations](#groupby-operations)
10. [Merging à¹à¸¥à¸° Joining](#merging-à¹à¸¥à¸°-joining)
11. [Time Series Analysis](#time-series-analysis)
12. [Visualization](#visualization)
13. [Performance Optimization](#performance-optimization)
14. [à¹à¸šà¸šà¸à¸¶à¸à¸«à¸±à¸”](#à¹à¸šà¸šà¸à¸¶à¸à¸«à¸±à¸”)
15. [à¹‚à¸„à¸£à¸‡à¸‡à¸²à¸™à¸›à¸à¸´à¸šà¸±à¸•à¸´](#à¹‚à¸„à¸£à¸‡à¸‡à¸²à¸™à¸›à¸à¸´à¸šà¸±à¸•à¸´)

---

## à¸šà¸—à¸™à¸³ Pandas

### Pandas à¸„à¸·à¸­à¸­à¸°à¹„à¸£?
**Pandas** (Python Data Analysis Library) à¹€à¸›à¹‡à¸™ library à¸—à¸µà¹ˆà¸—à¸£à¸‡à¸žà¸¥à¸±à¸‡à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£à¸ˆà¸±à¸”à¸à¸²à¸£à¹à¸¥à¸°à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹ƒà¸™ Python à¹‚à¸”à¸¢à¹ƒà¸«à¹‰à¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡à¸¡à¸·à¸­à¸—à¸µà¹ˆà¹€à¸£à¹‡à¸§à¹à¸¥à¸°à¸¢à¸·à¸”à¸«à¸¢à¸¸à¹ˆà¸™à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£à¸—à¸³à¸‡à¸²à¸™à¸à¸±à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹à¸šà¸š structured

### à¸—à¸³à¹„à¸¡à¸•à¹‰à¸­à¸‡à¹ƒà¸Šà¹‰ Pandas?

#### à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸šà¸à¸²à¸£à¸—à¸³à¸‡à¸²à¸™à¸à¸±à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥

```python
# à¹„à¸¡à¹ˆà¹ƒà¸Šà¹‰ Pandas - à¸¢à¸¸à¹ˆà¸‡à¸¢à¸²à¸
data = [
    ['Alice', 25, 50000],
    ['Bob', 30, 60000],
    ['Charlie', 35, 70000]
]

# à¸«à¸²à¸„à¹ˆà¸²à¹€à¸‰à¸¥à¸µà¹ˆà¸¢à¹€à¸‡à¸´à¸™à¹€à¸”à¸·à¸­à¸™
total_salary = sum(row[2] for row in data)
avg_salary = total_salary / len(data)

# à¹ƒà¸Šà¹‰ Pandas - à¸‡à¹ˆà¸²à¸¢à¹à¸¥à¸°à¸Šà¸±à¸”à¹€à¸ˆà¸™
import pandas as pd
df = pd.DataFrame(data, columns=['Name', 'Age', 'Salary'])
avg_salary = df['Salary'].mean()
```

### à¸‚à¹‰à¸­à¸”à¸µà¸‚à¸­à¸‡ Pandas
- **Easy Data Manipulation**: à¸ˆà¸±à¸”à¸à¸²à¸£à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹„à¸”à¹‰à¸‡à¹ˆà¸²à¸¢
- **Multiple File Formats**: à¸£à¸­à¸‡à¸£à¸±à¸šà¹„à¸Ÿà¸¥à¹Œà¸«à¸¥à¸²à¸¢à¸£à¸¹à¸›à¹à¸šà¸š (CSV, Excel, JSON, SQL)
- **Missing Data Handling**: à¸ˆà¸±à¸”à¸à¸²à¸£à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸«à¸²à¸¢à¹„à¸”à¹‰à¸”à¸µ
- **Data Alignment**: à¸ˆà¸±à¸”à¹€à¸£à¸µà¸¢à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´
- **Flexible Grouping**: à¸ˆà¸±à¸”à¸à¸¥à¸¸à¹ˆà¸¡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹„à¸”à¹‰à¸«à¸¥à¸²à¸à¸«à¸¥à¸²à¸¢
- **Integration**: à¸—à¸³à¸‡à¸²à¸™à¸£à¹ˆà¸§à¸¡à¸à¸±à¸š NumPy, Matplotlib à¹„à¸”à¹‰à¸”à¸µ

### à¸ªà¸–à¸´à¸•à¸´à¸à¸²à¸£à¹ƒà¸Šà¹‰à¸‡à¸²à¸™
- à¹ƒà¸Šà¹‰à¹ƒà¸™à¹‚à¸„à¸£à¸‡à¸à¸²à¸£ Data Science à¸¡à¸²à¸à¸à¸§à¹ˆà¸² 95%
- à¸¡à¸µà¸à¸²à¸£ download à¸¡à¸²à¸à¸à¸§à¹ˆà¸² 50 à¸¥à¹‰à¸²à¸™à¸„à¸£à¸±à¹‰à¸‡à¸•à¹ˆà¸­à¹€à¸”à¸·à¸­à¸™
- Essential tool à¸ªà¸³à¸«à¸£à¸±à¸š Data Scientists à¸—à¸±à¹ˆà¸§à¹‚à¸¥à¸

---

## à¸à¸²à¸£à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡à¹à¸¥à¸°à¸à¸²à¸£ Import

### à¸à¸²à¸£à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡
```bash
# à¸œà¹ˆà¸²à¸™ pip
pip install pandas

# à¸œà¹ˆà¸²à¸™ conda
conda install pandas

# à¸£à¸§à¸¡à¸à¸±à¸š dependencies à¸­à¸·à¹ˆà¸™à¹†
pip install pandas numpy matplotlib seaborn

# à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š version
python -c "import pandas as pd; print(pd.__version__)"
```

### à¸à¸²à¸£ Import
```python
# Standard import
import pandas as pd
import numpy as np

# à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š version
print(f"Pandas version: {pd.__version__}")

# Import à¹€à¸‰à¸žà¸²à¸°à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£
from pandas import DataFrame, Series, read_csv

# à¸à¸²à¸£à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² display options
pd.set_option('display.max_rows', 100)
pd.set_option('display.max_columns', 20)
pd.set_option('display.width', 1000)
```

### à¸à¸²à¸£à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸²à¹€à¸žà¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡
```python
# à¸à¸²à¸£à¹à¸ªà¸”à¸‡à¸œà¸¥à¸—à¸¨à¸™à¸´à¸¢à¸¡
pd.set_option('display.float_format', '{:.2f}'.format)

# à¸à¸²à¸£à¹à¸ªà¸”à¸‡à¸œà¸¥à¸‚à¸™à¸²à¸”à¹ƒà¸«à¸à¹ˆ
pd.set_option('display.max_colwidth', 50)

# à¸£à¸µà¹€à¸‹à¹‡à¸•à¸à¸²à¸£à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸²
pd.reset_option('all')

# à¸”à¸¹à¸à¸²à¸£à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸²à¸›à¸±à¸ˆà¸ˆà¸¸à¸šà¸±à¸™
print(pd.describe_option())
```

---

## Series à¸žà¸·à¹‰à¸™à¸à¸²à¸™

### Series à¸„à¸·à¸­à¸­à¸°à¹„à¸£?
**Series** à¹€à¸›à¹‡à¸™à¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ 1 à¸¡à¸´à¸•à¸´à¸—à¸µà¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¹€à¸à¹‡à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸Šà¸™à¸´à¸”à¹ƒà¸”à¸à¹‡à¹„à¸”à¹‰ (integers, strings, floats, Python objects) à¸žà¸£à¹‰à¸­à¸¡à¸à¸±à¸š labels (index)

### à¸à¸²à¸£à¸ªà¸£à¹‰à¸²à¸‡ Series

```python
import pandas as pd
import numpy as np

# à¸ˆà¸²à¸ Python list
s1 = pd.Series([1, 3, 5, np.nan, 6, 8])
print(s1)

# à¸à¸³à¸«à¸™à¸” index à¹€à¸­à¸‡
s2 = pd.Series([1, 3, 5, 6], index=['a', 'b', 'c', 'd'])
print(s2)

# à¸ˆà¸²à¸ dictionary
data_dict = {'A': 1, 'B': 2, 'C': 3, 'D': 4}
s3 = pd.Series(data_dict)
print(s3)

# à¸ˆà¸²à¸ NumPy array
arr = np.array([1, 2, 3, 4, 5])
s4 = pd.Series(arr, index=['x', 'y', 'z', 'w', 'v'])
print(s4)

# Series à¸—à¸µà¹ˆà¸¡à¸µà¸„à¹ˆà¸²à¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸™à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”
s5 = pd.Series(5, index=[0, 1, 2, 3])
print(s5)
```

### à¸„à¸¸à¸“à¸ªà¸¡à¸šà¸±à¸•à¸´à¸‚à¸­à¸‡ Series
```python
s = pd.Series([1, 3, 5, np.nan, 6, 8], index=['a', 'b', 'c', 'd', 'e', 'f'])

# à¸„à¸¸à¸“à¸ªà¸¡à¸šà¸±à¸•à¸´à¸žà¸·à¹‰à¸™à¸à¸²à¸™
print(f"Values: {s.values}")          # NumPy array
print(f"Index: {s.index}")            # Index object
print(f"Data type: {s.dtype}")        # à¸Šà¸™à¸´à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥
print(f"Shape: {s.shape}")            # à¸£à¸¹à¸›à¸£à¹ˆà¸²à¸‡
print(f"Size: {s.size}")              # à¸ˆà¸³à¸™à¸§à¸™ elements
print(f"Name: {s.name}")              # à¸Šà¸·à¹ˆà¸­ Series

# à¸•à¸±à¹‰à¸‡à¸Šà¸·à¹ˆà¸­à¹ƒà¸«à¹‰ Series à¹à¸¥à¸° index
s.name = 'My Series'
s.index.name = 'Labels'
print(s)
```

### à¸à¸²à¸£à¹€à¸‚à¹‰à¸²à¸–à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹ƒà¸™ Series
```python
s = pd.Series([10, 20, 30, 40, 50], index=['a', 'b', 'c', 'd', 'e'])

# à¹€à¸‚à¹‰à¸²à¸–à¸¶à¸‡à¸”à¹‰à¸§à¸¢ label
print(s['b'])                    # 20
print(s[['a', 'c', 'e']])        # Multiple labels

# à¹€à¸‚à¹‰à¸²à¸–à¸¶à¸‡à¸”à¹‰à¸§à¸¢ position
print(s[1])                      # 20
print(s[0:3])                    # Slicing

# Boolean indexing
print(s[s > 25])                 # à¸„à¹ˆà¸²à¸—à¸µà¹ˆà¸¡à¸²à¸à¸à¸§à¹ˆà¸² 25

# à¸à¸²à¸£à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š
print('b' in s)                  # True
print(s.get('f', 'Not found'))   # Get with default value
```

### à¸à¸²à¸£à¸”à¸³à¹€à¸™à¸´à¸™à¸à¸²à¸£à¸à¸±à¸š Series
```python
s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
s2 = pd.Series([10, 20, 30, 40], index=['a', 'b', 'c', 'd'])

# Arithmetic operations
print(s1 + s2)                   # Element-wise addition
print(s1 * 2)                    # Scalar multiplication
print(s1 ** 2)                   # Power

# Statistical operations
print(s1.sum())                  # à¸œà¸¥à¸£à¸§à¸¡
print(s1.mean())                 # à¸„à¹ˆà¸²à¹€à¸‰à¸¥à¸µà¹ˆà¸¢
print(s1.std())                  # à¸ªà¹ˆà¸§à¸™à¹€à¸šà¸µà¹ˆà¸¢à¸‡à¹€à¸šà¸™à¸¡à¸²à¸•à¸£à¸à¸²à¸™
print(s1.describe())             # à¸ªà¸–à¸´à¸•à¸´à¹‚à¸”à¸¢à¸£à¸§à¸¡

# String operations (à¸ªà¸³à¸«à¸£à¸±à¸š Series à¸—à¸µà¹ˆà¸¡à¸µ string)
s_str = pd.Series(['apple', 'banana', 'cherry'])
print(s_str.str.upper())         # à¸•à¸±à¸§à¸žà¸´à¸¡à¸žà¹Œà¹ƒà¸«à¸à¹ˆ
print(s_str.str.len())           # à¸„à¸§à¸²à¸¡à¸¢à¸²à¸§ string
print(s_str.str.contains('a'))   # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¸¡à¸µ 'a'
```

---

## DataFrame à¸žà¸·à¹‰à¸™à¸à¸²à¸™

### DataFrame à¸„à¸·à¸­à¸­à¸°à¹„à¸£?
**DataFrame** à¹€à¸›à¹‡à¸™à¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ 2 à¸¡à¸´à¸•à¸´à¸—à¸µà¹ˆà¹€à¸«à¸¡à¸·à¸­à¸™ table à¹ƒà¸™ SQL à¸«à¸£à¸·à¸­ spreadsheet à¹ƒà¸™ Excel à¹‚à¸”à¸¢à¸¡à¸µ labeled axes (rows à¹à¸¥à¸° columns)

### à¸à¸²à¸£à¸ªà¸£à¹‰à¸²à¸‡ DataFrame

```python
# à¸ˆà¸²à¸ dictionary
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'Diana'],
    'Age': [25, 30, 35, 28],
    'City': ['New York', 'London', 'Tokyo', 'Paris'],
    'Salary': [50000, 60000, 70000, 55000]
}
df = pd.DataFrame(data)
print(df)

# à¸ˆà¸²à¸ list of dictionaries
data_list = [
    {'Name': 'Alice', 'Age': 25, 'City': 'New York'},
    {'Name': 'Bob', 'Age': 30, 'City': 'London'},
    {'Name': 'Charlie', 'Age': 35, 'City': 'Tokyo'}
]
df2 = pd.DataFrame(data_list)
print(df2)

# à¸ˆà¸²à¸ NumPy array
arr = np.random.randn(4, 3)
df3 = pd.DataFrame(arr, 
                   columns=['A', 'B', 'C'],
                   index=['Row1', 'Row2', 'Row3', 'Row4'])
print(df3)

# à¸ˆà¸²à¸ Series
s1 = pd.Series([1, 2, 3], name='Col1')
s2 = pd.Series([4, 5, 6], name='Col2')
df4 = pd.DataFrame({'Col1': s1, 'Col2': s2})
print(df4)
```

### à¸„à¸¸à¸“à¸ªà¸¡à¸šà¸±à¸•à¸´à¸‚à¸­à¸‡ DataFrame
```python
df = pd.DataFrame({
    'A': [1, 2, 3, 4],
    'B': [5.0, 6.0, 7.0, 8.0],
    'C': ['foo', 'bar', 'baz', 'qux'],
    'D': [True, False, True, False]
})

# à¸„à¸¸à¸“à¸ªà¸¡à¸šà¸±à¸•à¸´à¸žà¸·à¹‰à¸™à¸à¸²à¸™
print(f"Shape: {df.shape}")              # (rows, columns)
print(f"Size: {df.size}")                # total elements
print(f"Columns: {df.columns}")          # column names
print(f"Index: {df.index}")              # row indices
print(f"Data types:\n{df.dtypes}")       # data types
print(f"Memory usage:\n{df.memory_usage()}")

# à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸šà¸·à¹‰à¸­à¸‡à¸•à¹‰à¸™
print(df.head())                         # 5 à¹à¸–à¸§à¹à¸£à¸
print(df.tail(3))                        # 3 à¹à¸–à¸§à¸ªà¸¸à¸”à¸—à¹‰à¸²à¸¢
print(df.info())                         # à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹‚à¸”à¸¢à¸£à¸§à¸¡
print(df.describe())                     # à¸ªà¸–à¸´à¸•à¸´à¸ªà¸³à¸«à¸£à¸±à¸š numeric columns
```

### à¸à¸²à¸£à¹€à¸‚à¹‰à¸²à¸–à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹ƒà¸™ DataFrame

#### à¸à¸²à¸£à¹€à¸¥à¸·à¸­à¸ Columns
```python
df = pd.DataFrame({
    'Name': ['Alice', 'Bob', 'Charlie'],
    'Age': [25, 30, 35],
    'Salary': [50000, 60000, 70000]
})

# à¹€à¸¥à¸·à¸­à¸ 1 column (à¹„à¸”à¹‰ Series)
print(df['Name'])

# à¹€à¸¥à¸·à¸­à¸à¸«à¸¥à¸²à¸¢ columns (à¹„à¸”à¹‰ DataFrame)
print(df[['Name', 'Salary']])

# à¹€à¸‚à¹‰à¸²à¸–à¸¶à¸‡à¸”à¹‰à¸§à¸¢ attribute (à¸–à¹‰à¸²à¸Šà¸·à¹ˆà¸­ column à¹„à¸¡à¹ˆà¸¡à¸µà¸Šà¹ˆà¸­à¸‡à¸§à¹ˆà¸²à¸‡)
print(df.Name)
```

#### à¸à¸²à¸£à¹€à¸¥à¸·à¸­à¸ Rows
```python
# à¹€à¸¥à¸·à¸­à¸à¸”à¹‰à¸§à¸¢ position
print(df.iloc[0])              # à¹à¸–à¸§à¹à¸£à¸ (Series)
print(df.iloc[0:2])            # à¹à¸–à¸§à¸—à¸µà¹ˆ 0-1 (DataFrame)

# à¹€à¸¥à¸·à¸­à¸à¸”à¹‰à¸§à¸¢ label
df_indexed = df.set_index('Name')
print(df_indexed.loc['Alice'])  # à¹à¸–à¸§à¸‚à¸­à¸‡ Alice

# Boolean indexing
print(df[df['Age'] > 30])       # à¸„à¸™à¸—à¸µà¹ˆà¸­à¸²à¸¢à¸¸à¸¡à¸²à¸à¸à¸§à¹ˆà¸² 30
print(df[df['Name'].isin(['Alice', 'Charlie'])])  # à¹€à¸¥à¸·à¸­à¸à¸Šà¸·à¹ˆà¸­à¹€à¸‰à¸žà¸²à¸°
```

#### à¸à¸²à¸£à¹€à¸¥à¸·à¸­à¸ Cells à¹€à¸‰à¸žà¸²à¸°
```python
# à¹€à¸¥à¸·à¸­à¸ cell à¹€à¸‰à¸žà¸²à¸°
print(df.iloc[0, 1])           # à¹à¸–à¸§ 0, column 1
print(df.loc[0, 'Age'])        # à¹à¸–à¸§ 0, column 'Age'

# à¹€à¸¥à¸·à¸­à¸à¸Šà¹ˆà¸§à¸‡
print(df.iloc[0:2, 1:3])       # à¹à¸–à¸§ 0-1, column 1-2
print(df.loc[0:1, 'Age':'Salary'])  # à¹à¸–à¸§ 0-1, column Age-Salary
```

---

## à¸à¸²à¸£à¹‚à¸«à¸¥à¸”à¹à¸¥à¸°à¸šà¸±à¸™à¸—à¸¶à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥

### à¸à¸²à¸£à¸­à¹ˆà¸²à¸™à¹„à¸Ÿà¸¥à¹Œà¸•à¹ˆà¸²à¸‡à¹†

#### CSV Files
```python
# à¸­à¹ˆà¸²à¸™ CSV à¸žà¸·à¹‰à¸™à¸à¸²à¸™
df = pd.read_csv('data.csv')

# à¸žà¸£à¹‰à¸­à¸¡ parameters à¸•à¹ˆà¸²à¸‡à¹†
df = pd.read_csv(
    'data.csv',
    sep=',',                    # à¸•à¸±à¸§à¹à¸šà¹ˆà¸‡
    header=0,                   # à¹à¸–à¸§à¸—à¸µà¹ˆà¹€à¸›à¹‡à¸™ header
    index_col=0,                # column à¸—à¸µà¹ˆà¹€à¸›à¹‡à¸™ index
    usecols=['Name', 'Age'],    # column à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£
    dtype={'Age': int},         # à¸à¸³à¸«à¸™à¸”à¸Šà¸™à¸´à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥
    parse_dates=['Date'],       # à¹à¸›à¸¥à¸‡à¹€à¸›à¹‡à¸™ datetime
    na_values=['N/A', 'NULL']   # à¸„à¹ˆà¸² missing
)

# à¸­à¹ˆà¸²à¸™à¸ˆà¸²à¸ URL
url = 'https://example.com/data.csv'
df = pd.read_csv(url)

# à¸­à¹ˆà¸²à¸™à¹€à¸‰à¸žà¸²à¸°à¸šà¸²à¸‡à¹à¸–à¸§
df = pd.read_csv('data.csv', nrows=100)  # à¸­à¹ˆà¸²à¸™ 100 à¹à¸–à¸§à¹à¸£à¸
df = pd.read_csv('data.csv', skiprows=10)  # à¸‚à¹‰à¸²à¸¡ 10 à¹à¸–à¸§à¹à¸£à¸
```

#### Excel Files
```python
# à¸­à¹ˆà¸²à¸™ Excel
df = pd.read_excel('data.xlsx')

# à¸­à¹ˆà¸²à¸™ sheet à¹€à¸‰à¸žà¸²à¸°
df = pd.read_excel('data.xlsx', sheet_name='Sheet1')

# à¸­à¹ˆà¸²à¸™à¸«à¸¥à¸²à¸¢ sheets
dfs = pd.read_excel('data.xlsx', sheet_name=None)  # dictionary à¸‚à¸­à¸‡ DataFrames

# à¸žà¸£à¹‰à¸­à¸¡ parameters
df = pd.read_excel(
    'data.xlsx',
    sheet_name='Data',
    header=0,
    index_col=0,
    usecols='A:D',             # columns A à¸–à¸¶à¸‡ D
    skiprows=2                 # à¸‚à¹‰à¸²à¸¡ 2 à¹à¸–à¸§à¹à¸£à¸
)
```

#### JSON Files
```python
# à¸­à¹ˆà¸²à¸™ JSON
df = pd.read_json('data.json')

# JSON à¸£à¸¹à¸›à¹à¸šà¸šà¸•à¹ˆà¸²à¸‡à¹†
df = pd.read_json('data.json', orient='records')  # list of objects
df = pd.read_json('data.json', orient='index')    # object of objects
df = pd.read_json('data.json', lines=True)        # JSON Lines format
```

#### SQL Database
```python
import sqlite3

# à¹€à¸Šà¸·à¹ˆà¸­à¸¡à¸•à¹ˆà¸­à¸à¸²à¸™à¸‚à¹‰à¸­à¸¡à¸¹à¸¥
conn = sqlite3.connect('database.db')

# à¸­à¹ˆà¸²à¸™à¸ˆà¸²à¸ SQL query
df = pd.read_sql_query('SELECT * FROM table_name', conn)

# à¸­à¹ˆà¸²à¸™à¸—à¸±à¹‰à¸‡ table
df = pd.read_sql_table('table_name', conn)

# à¸›à¸´à¸”à¸à¸²à¸£à¹€à¸Šà¸·à¹ˆà¸­à¸¡à¸•à¹ˆà¸­
conn.close()
```

### à¸à¸²à¸£à¸šà¸±à¸™à¸—à¸¶à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥

```python
df = pd.DataFrame({
    'Name': ['Alice', 'Bob', 'Charlie'],
    'Age': [25, 30, 35],
    'Salary': [50000, 60000, 70000]
})

# à¸šà¸±à¸™à¸—à¸¶à¸à¹€à¸›à¹‡à¸™ CSV
df.to_csv('output.csv', index=False)

# à¸šà¸±à¸™à¸—à¸¶à¸à¹€à¸›à¹‡à¸™ Excel
df.to_excel('output.xlsx', sheet_name='Data', index=False)

# à¸šà¸±à¸™à¸—à¸¶à¸à¹€à¸›à¹‡à¸™ JSON
df.to_json('output.json', orient='records', indent=2)

# à¸šà¸±à¸™à¸—à¸¶à¸à¸¥à¸‡à¸à¸²à¸™à¸‚à¹‰à¸­à¸¡à¸¹à¸¥
conn = sqlite3.connect('database.db')
df.to_sql('table_name', conn, if_exists='replace', index=False)
conn.close()

# à¸šà¸±à¸™à¸—à¸¶à¸à¹€à¸‰à¸žà¸²à¸°à¸šà¸²à¸‡à¸ªà¹ˆà¸§à¸™
df[['Name', 'Age']].to_csv('partial_data.csv', index=False)
```

---

## Data Selection à¹à¸¥à¸° Filtering

### à¹€à¸—à¸„à¸™à¸´à¸„à¸à¸²à¸£à¹€à¸¥à¸·à¸­à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥

#### Basic Selection
```python
df = pd.DataFrame({
    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],
    'Age': [25, 30, 35, 28, 32],
    'City': ['NY', 'London', 'Tokyo', 'Paris', 'Berlin'],
    'Salary': [50000, 60000, 70000, 55000, 65000],
    'Department': ['IT', 'HR', 'IT', 'Finance', 'IT']
})

# à¹€à¸¥à¸·à¸­à¸ columns
single_col = df['Name']                    # Series
multi_cols = df[['Name', 'Age', 'Salary']] # DataFrame

# à¹€à¸¥à¸·à¸­à¸ rows à¸”à¹‰à¸§à¸¢ conditions
high_salary = df[df['Salary'] > 60000]
it_employees = df[df['Department'] == 'IT']
young_high_earners = df[(df['Age'] < 30) & (df['Salary'] > 50000)]
```

#### iloc à¹à¸¥à¸° loc
```python
# iloc - position-based selection
print(df.iloc[0])              # à¹à¸–à¸§à¹à¸£à¸
print(df.iloc[0:3])            # à¹à¸–à¸§ 0-2
print(df.iloc[:, 1:3])         # columns 1-2 à¸—à¸¸à¸à¹à¸–à¸§
print(df.iloc[0:2, 1:3])       # à¹à¸–à¸§ 0-1, columns 1-2

# loc - label-based selection
df_with_index = df.set_index('Name')
print(df_with_index.loc['Alice'])           # à¹à¸–à¸§à¸‚à¸­à¸‡ Alice
print(df_with_index.loc['Alice':'Charlie']) # à¹à¸–à¸§ Alice à¸–à¸¶à¸‡ Charlie
print(df_with_index.loc[:, 'Age':'Salary']) # columns Age à¸–à¸¶à¸‡ Salary
```

#### Query Method
```python
# à¹ƒà¸Šà¹‰ query à¸ªà¸³à¸«à¸£à¸±à¸š filtering à¸—à¸µà¹ˆà¸‹à¸±à¸šà¸‹à¹‰à¸­à¸™
result1 = df.query('Age > 30')
result2 = df.query('Age > 30 and Salary < 70000')
result3 = df.query('Department == "IT"')
result4 = df.query('City in ["NY", "Tokyo"]')

# à¹ƒà¸Šà¹‰à¸•à¸±à¸§à¹à¸›à¸£ external
min_age = 30
result5 = df.query('Age > @min_age')
```

### Advanced Filtering

#### à¸à¸²à¸£à¹ƒà¸Šà¹‰ isin()
```python
# à¹€à¸¥à¸·à¸­à¸à¸«à¸¥à¸²à¸¢à¸„à¹ˆà¸²à¸žà¸£à¹‰à¸­à¸¡à¸à¸±à¸™
cities_of_interest = ['NY', 'Tokyo', 'Paris']
filtered = df[df['City'].isin(cities_of_interest)]

# à¸•à¸£à¸‡à¸‚à¹‰à¸²à¸¡à¸‚à¸­à¸‡ isin
not_in_cities = df[~df['City'].isin(cities_of_interest)]
```

#### String Filtering
```python
# à¸ªà¸£à¹‰à¸²à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡
df_str = pd.DataFrame({
    'Name': ['Alice Johnson', 'Bob Smith', 'Charlie Brown', 'Diana Wilson'],
    'Email': ['alice@gmail.com', 'bob@yahoo.com', 'charlie@gmail.com', 'diana@outlook.com']
})

# String methods
gmail_users = df_str[df_str['Email'].str.contains('gmail')]
starts_with_a = df_str[df_str['Name'].str.startswith('A')]
ends_with_son = df_str[df_str['Name'].str.endswith('son')]

# Case-insensitive search
contains_alice = df_str[df_str['Name'].str.contains('alice', case=False)]

# Regular expressions
pattern_match = df_str[df_str['Email'].str.match(r'\w+@gmail\.com')]
```

#### à¸à¸²à¸£à¸ˆà¸±à¸”à¸à¸²à¸£ Missing Values à¹ƒà¸™à¸à¸²à¸£ Filter
```python
# à¸ªà¸£à¹‰à¸²à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸¡à¸µ missing values
df_missing = pd.DataFrame({
    'A': [1, 2, np.nan, 4, 5],
    'B': [np.nan, 2, 3, 4, np.nan],
    'C': [1, 2, 3, 4, 5]
})

# Filter à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¸¡à¸µ missing values
no_missing = df_missing[df_missing['A'].notna()]

# Filter à¸—à¸µà¹ˆà¸¡à¸µ missing values
has_missing = df_missing[df_missing['A'].isna()]

# Filter à¸«à¸¥à¸²à¸¢ columns
complete_rows = df_missing.dropna()  # à¹à¸–à¸§à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¸¡à¸µ missing values
any_missing = df_missing[df_missing.isna().any(axis=1)]  # à¹à¸–à¸§à¸—à¸µà¹ˆà¸¡à¸µ missing values
```

---

## Data Cleaning

### à¸à¸²à¸£à¸ˆà¸±à¸”à¸à¸²à¸£ Missing Values

#### à¸à¸²à¸£à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š Missing Values
```python
df = pd.DataFrame({
    'A': [1, 2, np.nan, 4, 5],
    'B': [np.nan, 2, 3, np.nan, 5],
    'C': [1, 2, 3, 4, 5],
    'D': ['a', 'b', None, 'd', 'e']
})

# à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š missing values
print(df.isnull().sum())           # à¸ˆà¸³à¸™à¸§à¸™ missing values à¹ƒà¸™à¹à¸•à¹ˆà¸¥à¸° column
print(df.isna().sum())             # à¹€à¸«à¸¡à¸·à¸­à¸™à¸à¸±à¸š isnull()
print(df.info())                   # à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹‚à¸”à¸¢à¸£à¸§à¸¡à¸£à¸§à¸¡à¸–à¸¶à¸‡ missing values

# à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¹à¸–à¸§à¸—à¸µà¹ˆà¸¡à¸µ missing values
print(df[df.isnull().any(axis=1)])

# à¹€à¸›à¸­à¸£à¹Œà¹€à¸‹à¹‡à¸™à¸•à¹Œà¸‚à¸­à¸‡ missing values
missing_percent = (df.isnull().sum() / len(df)) * 100
print(missing_percent)
```

#### à¸à¸²à¸£à¸¥à¸š Missing Values
```python
# à¸¥à¸šà¹à¸–à¸§à¸—à¸µà¹ˆà¸¡à¸µ missing values
df_drop_rows = df.dropna()                    # à¸¥à¸šà¸—à¸¸à¸à¹à¸–à¸§à¸—à¸µà¹ˆà¸¡à¸µ missing
df_drop_any = df.dropna(how='any')            # à¸¥à¸šà¸–à¹‰à¸²à¸¡à¸µ missing à¸­à¸¢à¹ˆà¸²à¸‡à¸™à¹‰à¸­à¸¢ 1 à¸„à¹ˆà¸²
df_drop_all = df.dropna(how='all')            # à¸¥à¸šà¸–à¹‰à¸²à¸—à¸¸à¸à¸„à¹ˆà¸²à¹€à¸›à¹‡à¸™ missing

# à¸¥à¸š columns à¸—à¸µà¹ˆà¸¡à¸µ missing values
df_drop_cols = df.dropna(axis=1)              # à¸¥à¸š columns à¸—à¸µà¹ˆà¸¡à¸µ missing

# à¸¥à¸šà¸–à¹‰à¸²à¸¡à¸µ missing values à¸¡à¸²à¸à¸à¸§à¹ˆà¸²à¸—à¸µà¹ˆà¸à¸³à¸«à¸™à¸”
df_thresh = df.dropna(thresh=3)               # à¸•à¹‰à¸­à¸‡à¸¡à¸µà¸„à¹ˆà¸²à¸—à¸µà¹ˆà¹„à¸¡à¹ˆ missing à¸­à¸¢à¹ˆà¸²à¸‡à¸™à¹‰à¸­à¸¢ 3 à¸„à¹ˆà¸²

# à¸¥à¸šà¹€à¸‰à¸žà¸²à¸° columns à¹€à¸‰à¸žà¸²à¸°
df_subset = df.dropna(subset=['A', 'B'])      # à¸¥à¸šà¸–à¹‰à¸² A à¸«à¸£à¸·à¸­ B à¹€à¸›à¹‡à¸™ missing
```

#### à¸à¸²à¸£à¹€à¸•à¸´à¸¡ Missing Values
```python
# à¹€à¸•à¸´à¸¡à¸”à¹‰à¸§à¸¢à¸„à¹ˆà¸²à¸„à¸‡à¸—à¸µà¹ˆ
df_fill_zero = df.fillna(0)                   # à¹€à¸•à¸´à¸¡à¸”à¹‰à¸§à¸¢ 0
df_fill_unknown = df.fillna('Unknown')        # à¹€à¸•à¸´à¸¡à¸”à¹‰à¸§à¸¢ 'Unknown'

# à¹€à¸•à¸´à¸¡à¸”à¹‰à¸§à¸¢à¸„à¹ˆà¸²à¸ªà¸–à¸´à¸•à¸´
df_fill_mean = df.fillna(df.mean())           # à¹€à¸•à¸´à¸¡à¸”à¹‰à¸§à¸¢à¸„à¹ˆà¸²à¹€à¸‰à¸¥à¸µà¹ˆà¸¢ (numeric columns)
df_fill_median = df.fillna(df.median())       # à¹€à¸•à¸´à¸¡à¸”à¹‰à¸§à¸¢ median

# à¹€à¸•à¸´à¸¡à¹à¸•à¹ˆà¸¥à¸° column à¸•à¹ˆà¸²à¸‡à¸à¸±à¸™
fill_values = {'A': df['A'].mean(), 'B': df['B'].median(), 'D': 'Unknown'}
df_fill_dict = df.fillna(value=fill_values)

# Forward fill à¹à¸¥à¸° Backward fill
df_ffill = df.fillna(method='ffill')          # à¹€à¸•à¸´à¸¡à¸”à¹‰à¸§à¸¢à¸„à¹ˆà¸²à¸à¹ˆà¸­à¸™à¸«à¸™à¹‰à¸²
df_bfill = df.fillna(method='bfill')          # à¹€à¸•à¸´à¸¡à¸”à¹‰à¸§à¸¢à¸„à¹ˆà¸²à¸–à¸±à¸”à¹„à¸›

# Interpolation
df_interp = df.interpolate()                  # interpolate à¸ªà¸³à¸«à¸£à¸±à¸š numeric data
```

### à¸à¸²à¸£à¸ˆà¸±à¸”à¸à¸²à¸£ Duplicates

```python
# à¸ªà¸£à¹‰à¸²à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸¡à¸µ duplicates
df_dup = pd.DataFrame({
    'Name': ['Alice', 'Bob', 'Alice', 'Charlie', 'Bob'],
    'Age': [25, 30, 25, 35, 30],
    'City': ['NY', 'London', 'NY', 'Tokyo', 'London']
})

# à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š duplicates
print(df_dup.duplicated())                    # Boolean mask
print(df_dup.duplicated().sum())              # à¸ˆà¸³à¸™à¸§à¸™ duplicate rows

# à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š duplicates à¹ƒà¸™ column à¹€à¸‰à¸žà¸²à¸°
print(df_dup.duplicated(subset=['Name']))

# à¸¥à¸š duplicates
df_no_dup = df_dup.drop_duplicates()          # à¸¥à¸š duplicate rows
df_no_dup_name = df_dup.drop_duplicates(subset=['Name'])  # à¸¥à¸š duplicate names

# à¹€à¸à¹‡à¸š duplicate à¹à¸£à¸à¸«à¸£à¸·à¸­à¸ªà¸¸à¸”à¸—à¹‰à¸²à¸¢
df_keep_first = df_dup.drop_duplicates(keep='first')
df_keep_last = df_dup.drop_duplicates(keep='last')
df_remove_all = df_dup.drop_duplicates(keep=False)  # à¸¥à¸šà¸—à¸¸à¸ duplicates
```

### Data Type Conversion

```python
df = pd.DataFrame({
    'A': ['1', '2', '3', '4'],           # string numbers
    'B': [1.0, 2.0, 3.0, 4.0],          # floats
    'C': ['2023-01-01', '2023-01-02'],   # date strings
    'D': ['True', 'False', 'True']       # boolean strings
})

# à¹à¸›à¸¥à¸‡ data types
df['A'] = df['A'].astype(int)             # string to int
df['B'] = df['B'].astype(int)             # float to int
df['C'] = pd.to_datetime(df['C'])         # string to datetime
df['D'] = df['D'].astype(bool)            # string to boolean

# à¹à¸›à¸¥à¸‡à¸«à¸¥à¸²à¸¢ columns à¸žà¸£à¹‰à¸­à¸¡à¸à¸±à¸™
df = df.astype({
    'A': int,
    'B': float,
    'D': bool
})

# à¸à¸²à¸£à¸ˆà¸±à¸”à¸à¸²à¸£ errors à¹ƒà¸™à¸à¸²à¸£à¹à¸›à¸¥à¸‡
df_mixed = pd.DataFrame({'col': ['1', '2', 'text', '4']})
df_mixed['col_numeric'] = pd.to_numeric(df_mixed['col'], errors='coerce')  # NaN à¸ªà¸³à¸«à¸£à¸±à¸š errors
```

### à¸à¸²à¸£à¸—à¸³à¸„à¸§à¸²à¸¡à¸ªà¸°à¸­à¸²à¸” String Data

```python
# à¸ªà¸£à¹‰à¸²à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸—à¸³à¸„à¸§à¸²à¸¡à¸ªà¸°à¸­à¸²à¸”
df_messy = pd.DataFrame({
    'Name': ['  Alice  ', 'BOB', 'charlie brown', 'DIANA'],
    'Email': ['ALICE@GMAIL.COM', 'bob@yahoo.com', 'Charlie@Gmail.com', 'diana@OUTLOOK.COM'],
    'Phone': ['123-456-7890', '(234) 567-8901', '345.678.9012', '456 789 0123']
})

# à¸—à¸³à¸„à¸§à¸²à¸¡à¸ªà¸°à¸­à¸²à¸” strings
df_clean = df_messy.copy()

# à¸¥à¸šà¸Šà¹ˆà¸­à¸‡à¸§à¹ˆà¸²à¸‡à¸‚à¹‰à¸²à¸‡à¸«à¸™à¹‰à¸²à¹à¸¥à¸°à¸‚à¹‰à¸²à¸‡à¸«à¸¥à¸±à¸‡
df_clean['Name'] = df_clean['Name'].str.strip()

# à¹à¸›à¸¥à¸‡à¹€à¸›à¹‡à¸™à¸•à¸±à¸§à¸žà¸´à¸¡à¸žà¹Œà¹€à¸¥à¹‡à¸/à¹ƒà¸«à¸à¹ˆ
df_clean['Email'] = df_clean['Email'].str.lower()
df_clean['Name'] = df_clean['Name'].str.title()  # Title Case

# à¹à¸—à¸™à¸—à¸µà¹ˆ characters
df_clean['Phone'] = df_clean['Phone'].str.replace(r'[^\d]', '', regex=True)  # à¹€à¸à¹‡à¸šà¹à¸•à¹ˆà¸•à¸±à¸§à¹€à¸¥à¸‚

# Split strings
name_parts = df_clean['Name'].str.split(' ', expand=True)
name_parts.columns = ['First_Name', 'Last_Name']

# Extract patterns à¸”à¹‰à¸§à¸¢ regex
email_domain = df_clean['Email'].str.extract(r'@(.+)')
email_domain.columns = ['Domain']

print(df_clean)
```

### à¸à¸²à¸£à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¹à¸¥à¸°à¹à¸à¹‰à¹„à¸‚ Outliers

```python
import matplotlib.pyplot as plt

# à¸ªà¸£à¹‰à¸²à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸¡à¸µ outliers
np.random.seed(42)
normal_data = np.random.normal(50, 10, 95)
outliers = [120, 130, -20, -10, 200]
data_with_outliers = np.concatenate([normal_data, outliers])

df_outliers = pd.DataFrame({'value': data_with_outliers})

# à¸§à¸´à¸˜à¸µà¸—à¸µà¹ˆ 1: Z-Score Method
z_scores = np.abs((df_outliers['value'] - df_outliers['value'].mean()) / df_outliers['value'].std())
outlier_zscore = df_outliers[z_scores > 3]
print(f"Outliers by Z-score: {len(outlier_zscore)}")

# à¸§à¸´à¸˜à¸µà¸—à¸µà¹ˆ 2: IQR Method
Q1 = df_outliers['value'].quantile(0.25)
Q3 = df_outliers['value'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outlier_iqr = df_outliers[(df_outliers['value'] < lower_bound) | 
                          (df_outliers['value'] > upper_bound)]
print(f"Outliers by IQR: {len(outlier_iqr)}")

# à¸à¸²à¸£à¸ˆà¸±à¸”à¸à¸²à¸£ outliers
# 1. à¸¥à¸š outliers
df_no_outliers = df_outliers[(df_outliers['value'] >= lower_bound) & 
                             (df_outliers['value'] <= upper_bound)]

# 2. Cap outliers (winsorizing)
df_capped = df_outliers.copy()
df_capped['value'] = df_capped['value'].clip(lower_bound, upper_bound)

# 3. Transform data
df_log = df_outliers.copy()
df_log['value_log'] = np.log1p(df_log['value'] - df_log['value'].min() + 1)
```

---

## Data Transformation

### à¸à¸²à¸£à¸ªà¸£à¹‰à¸²à¸‡ Columns à¹ƒà¸«à¸¡à¹ˆ

```python
df = pd.DataFrame({
    'Name': ['Alice', 'Bob', 'Charlie', 'Diana'],
    'Age': [25, 30, 35, 28],
    'Salary': [50000, 60000, 70000, 55000],
    'Years_Experience': [3, 8, 12, 5]
})

# à¸ªà¸£à¹‰à¸²à¸‡ column à¹ƒà¸«à¸¡à¹ˆà¸ˆà¸²à¸à¸à¸²à¸£à¸„à¸³à¸™à¸§à¸“
df['Salary_per_year_exp'] = df['Salary'] / df['Years_Experience']
df['Age_Group'] = df['Age'].apply(lambda x: 'Young' if x < 30 else 'Senior')

# à¹ƒà¸Šà¹‰ np.where à¸ªà¸³à¸«à¸£à¸±à¸š conditional logic
df['Performance'] = np.where(df['Salary'] > 60000, 'High', 'Normal')

# à¹ƒà¸Šà¹‰ pd.cut à¸ªà¸³à¸«à¸£à¸±à¸š binning
df['Age_Bin'] = pd.cut(df['Age'], bins=[0, 25, 30, 35, 100], 
                       labels=['Very Young', 'Young', 'Middle', 'Senior'])

# Multiple conditions
conditions = [
    (df['Age'] < 30) & (df['Salary'] > 55000),
    (df['Age'] >= 30) & (df['Salary'] > 65000),
    (df['Age'] >= 30) & (df['Salary'] <= 65000)
]
choices = ['High Potential', 'Senior High Performer', 'Senior Normal']
df['Category'] = np.select(conditions, choices, default='Other')

print(df)
```

### à¸à¸²à¸£à¹ƒà¸Šà¹‰ Apply Functions

```python
# Apply function à¸à¸±à¸š Series
def categorize_salary(salary):
    if salary < 55000:
        return 'Low'
    elif salary < 65000:
        return 'Medium'
    else:
        return 'High'

df['Salary_Category'] = df['Salary'].apply(categorize_salary)

# Apply à¸à¸±à¸š lambda function
df['Name_Length'] = df['Name'].apply(lambda x: len(x))
df['Salary_K'] = df['Salary'].apply(lambda x: f"{x/1000:.0f}K")

# Apply à¸à¸±à¸š DataFrame (axis=1 à¸ªà¸³à¸«à¸£à¸±à¸š rows)
def calculate_bonus(row):
    base_bonus = row['Salary'] * 0.1
    experience_bonus = row['Years_Experience'] * 1000
    return base_bonus + experience_bonus

df['Bonus'] = df.apply(calculate_bonus, axis=1)

# Apply à¸à¸±à¸šà¸«à¸¥à¸²à¸¢ columns
df['Salary_Age_Ratio'] = df.apply(lambda row: row['Salary'] / row['Age'], axis=1)
```

### Map à¹à¸¥à¸° Replace

```python
# Map à¸”à¹‰à¸§à¸¢ dictionary
age_mapping = {25: 'Young', 30: 'Middle', 35: 'Senior', 28: 'Young'}
df['Age_Category'] = df['Age'].map(age_mapping)

# Map à¸”à¹‰à¸§à¸¢ Series
salary_avg = df.groupby('Age_Group')['Salary'].mean()
df['Avg_Salary_for_Group'] = df['Age_Group'].map(salary_avg)

# Replace values
df_replace = df.copy()
df_replace['Name'] = df_replace['Name'].replace('Alice', 'Alicia')
df_replace['Performance'] = df_replace['Performance'].replace({'High': 'Excellent', 'Normal': 'Good'})

# Replace with regex
df_replace['Name'] = df_replace['Name'].str.replace(r'[aeiou]', 'X', regex=True)
```

### Pivot Tables à¹à¸¥à¸° Reshaping

```python
# à¸ªà¸£à¹‰à¸²à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸ªà¸³à¸«à¸£à¸±à¸š pivot
sales_data = pd.DataFrame({
    'Date': pd.date_range('2023-01-01', periods=12, freq='M'),
    'Product': ['A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B'],
    'Region': ['North', 'North', 'South', 'South', 'North', 'North', 
               'South', 'South', 'North', 'North', 'South', 'South'],
    'Sales': [100, 150, 120, 180, 110, 160, 130, 190, 105, 155, 125, 195]
})

# Pivot table
pivot_table = sales_data.pivot_table(
    values='Sales',
    index='Product',
    columns='Region',
    aggfunc='mean'
)
print(pivot_table)

# Melt (inverse of pivot)
melted = pivot_table.reset_index().melt(
    id_vars='Product',
    var_name='Region',
    value_name='Average_Sales'
)
print(melted)

# Stack à¹à¸¥à¸° Unstack
stacked = pivot_table.stack()
unstacked = stacked.unstack()
```

---

## Groupby Operations

### à¸žà¸·à¹‰à¸™à¸à¸²à¸™ Groupby

```python
# à¸ªà¸£à¹‰à¸²à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡
employees = pd.DataFrame({
    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank'],
    'Department': ['IT', 'HR', 'IT', 'Finance', 'IT', 'HR'],
    'Age': [25, 30, 35, 28, 32, 27],
    'Salary': [50000, 60000, 70000, 55000, 65000, 58000],
    'Years_Experience': [3, 8, 12, 5, 9, 6]
})

# Basic groupby operations
dept_groups = employees.groupby('Department')

# Aggregation functions
print(dept_groups.mean())              # à¸„à¹ˆà¸²à¹€à¸‰à¸¥à¸µà¹ˆà¸¢
print(dept_groups.sum())               # à¸œà¸¥à¸£à¸§à¸¡
print(dept_groups.count())             # à¸ˆà¸³à¸™à¸§à¸™
print(dept_groups.std())               # à¸ªà¹ˆà¸§à¸™à¹€à¸šà¸µà¹ˆà¸¢à¸‡à¹€à¸šà¸™à¸¡à¸²à¸•à¸£à¸à¸²à¸™
print(dept_groups.describe())          # à¸ªà¸–à¸´à¸•à¸´à¹‚à¸”à¸¢à¸£à¸§à¸¡

# à¹€à¸¥à¸·à¸­à¸ column à¹€à¸‰à¸žà¸²à¸°
print(dept_groups['Salary'].mean())
print(dept_groups[['Salary', 'Age']].mean())
```

### Advanced Groupby Operations

```python
# Multiple groupby columns
age_dept_groups = employees.groupby(['Department', 'Age'])
print(age_dept_groups.size())          # à¸ˆà¸³à¸™à¸§à¸™à¹ƒà¸™ group

# Custom aggregation functions
def salary_range(series):
    return series.max() - series.min()

custom_agg = employees.groupby('Department')['Salary'].agg([
    'mean',
    'median',
    'std',
    ('range', salary_range),
    ('count', 'count')
])
print(custom_agg)

# Different aggregations for different columns
multi_agg = employees.groupby('Department').agg({
    'Salary': ['mean', 'max', 'min'],
    'Age': ['mean', 'std'],
    'Years_Experience': 'mean'
})
print(multi_agg)

# Apply custom functions
def department_analysis(group):
    return pd.Series({
        'avg_salary': group['Salary'].mean(),
        'max_experience': group['Years_Experience'].max(),
        'avg_age': group['Age'].mean(),
        'salary_per_experience': group['Salary'].sum() / group['Years_Experience'].sum()
    })

dept_analysis = employees.groupby('Department').apply(department_analysis)
print(dept_analysis)
```

### Transform à¹à¸¥à¸° Filter

```python
# Transform - à¸ªà¸£à¹‰à¸²à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸‚à¸™à¸²à¸”à¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸š original
employees['Dept_Avg_Salary'] = employees.groupby('Department')['Salary'].transform('mean')
employees['Salary_vs_Dept_Avg'] = employees['Salary'] - employees['Dept_Avg_Salary']
employees['Salary_Rank_in_Dept'] = employees.groupby('Department')['Salary'].rank(ascending=False)

# Filter - à¸à¸£à¸­à¸‡ groups à¸—à¸µà¹ˆà¸•à¸£à¸‡à¹€à¸‡à¸·à¹ˆà¸­à¸™à¹„à¸‚
large_departments = employees.groupby('Department').filter(lambda x: len(x) >= 2)
high_avg_salary_depts = employees.groupby('Department').filter(lambda x: x['Salary'].mean() > 60000)

print(employees)
print("\nLarge departments:")
print(large_departments)
```

### Rolling à¹à¸¥à¸° Expanding Operations

```python
# à¸ªà¸£à¹‰à¸²à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ time series
dates = pd.date_range('2023-01-01', periods=100, freq='D')
ts_data = pd.DataFrame({
    'Date': dates,
    'Value': np.random.randn(100).cumsum() + 100
})
ts_data.set_index('Date', inplace=True)

# Rolling operations
ts_data['Rolling_Mean_7'] = ts_data['Value'].rolling(window=7).mean()
ts_data['Rolling_Std_7'] = ts_data['Value'].rolling(window=7).std()
ts_data['Rolling_Max_7'] = ts_data['Value'].rolling(window=7).max()

# Expanding operations (à¸•à¸±à¹‰à¸‡à¹à¸•à¹ˆà¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™à¸–à¸¶à¸‡à¸ˆà¸¸à¸”à¸›à¸±à¸ˆà¸ˆà¸¸à¸šà¸±à¸™)
ts_data['Expanding_Mean'] = ts_data['Value'].expanding().mean()
ts_data['Expanding_Std'] = ts_data['Value'].expanding().std()

# Rolling à¸à¸±à¸š groupby
grouped_employees = employees.copy()
grouped_employees['Salary_Rank_Overall'] = grouped_employees['Salary'].rank(ascending=False)

print(ts_data.head(10))
```

---

## Merging à¹à¸¥à¸° Joining

### à¸à¸²à¸£à¸£à¸§à¸¡ DataFrames

```python
# à¸ªà¸£à¹‰à¸²à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡
employees = pd.DataFrame({
    'EmployeeID': [1, 2, 3, 4, 5],
    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],
    'DepartmentID': [101, 102, 101, 103, 102]
})

departments = pd.DataFrame({
    'DepartmentID': [101, 102, 103, 104],
    'Department': ['IT', 'HR', 'Finance', 'Marketing'],
    'Location': ['Building A', 'Building B', 'Building C', 'Building D']
})

salaries = pd.DataFrame({
    'EmployeeID': [1, 2, 3, 4, 6],
    'Salary': [50000, 60000, 70000, 55000, 48000],
    'Bonus': [5000, 6000, 7000, 5500, 4800]
})
```

#### Inner Join
```python
# Inner join - à¹€à¸à¹‡à¸šà¹€à¸‰à¸žà¸²à¸° records à¸—à¸µà¹ˆà¸¡à¸µà¹ƒà¸™ table à¸—à¸±à¹‰à¸‡à¸ªà¸­à¸‡
emp_dept = pd.merge(employees, departments, on='DepartmentID', how='inner')
print("Inner Join:")
print(emp_dept)

emp_salary = pd.merge(employees, salaries, on='EmployeeID', how='inner')
print("\nEmployee-Salary Inner Join:")
print(emp_salary)
```

#### Left, Right, à¹à¸¥à¸° Outer Joins
```python
# Left join - à¹€à¸à¹‡à¸šà¸—à¸¸à¸ records à¸ˆà¸²à¸ left table
left_join = pd.merge(employees, salaries, on='EmployeeID', how='left')
print("Left Join:")
print(left_join)

# Right join - à¹€à¸à¹‡à¸šà¸—à¸¸à¸ records à¸ˆà¸²à¸ right table
right_join = pd.merge(employees, salaries, on='EmployeeID', how='right')
print("Right Join:")
print(right_join)

# Outer join - à¹€à¸à¹‡à¸šà¸—à¸¸à¸ records à¸ˆà¸²à¸à¸—à¸±à¹‰à¸‡à¸ªà¸­à¸‡ tables
outer_join = pd.merge(employees, salaries, on='EmployeeID', how='outer')
print("Outer Join:")
print(outer_join)
```

#### Advanced Merging
```python
# Merge à¸à¸±à¸š different column names
emp_modified = employees.rename(columns={'EmployeeID': 'EmpID'})
merge_diff_names = pd.merge(emp_modified, salaries, 
                           left_on='EmpID', right_on='EmployeeID')

# Merge à¸à¸±à¸šà¸«à¸¥à¸²à¸¢ columns
combined_data = pd.merge(
    pd.merge(employees, departments, on='DepartmentID'),
    salaries, on='EmployeeID', how='left'
)
print("Complete Employee Data:")
print(combined_data)

# Merge à¸à¸±à¸š suffixes à¸ªà¸³à¸«à¸£à¸±à¸š column à¸—à¸µà¹ˆà¸Šà¸·à¹ˆà¸­à¸‹à¹‰à¸³
df1 = pd.DataFrame({'key': ['A', 'B', 'C'], 'value': [1, 2, 3]})
df2 = pd.DataFrame({'key': ['A', 'B', 'D'], 'value': [4, 5, 6]})
merged_suffix = pd.merge(df1, df2, on='key', how='outer', suffixes=('_left', '_right'))
print("Merge with suffixes:")
print(merged_suffix)
```

### Concatenation

```python
# à¸ªà¸£à¹‰à¸²à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡
df1 = pd.DataFrame({
    'A': [1, 2, 3],
    'B': [4, 5, 6]
})

df2 = pd.DataFrame({
    'A': [7, 8, 9],
    'B': [10, 11, 12]
})

df3 = pd.DataFrame({
    'C': [13, 14, 15],
    'D': [16, 17, 18]
})

# Vertical concatenation (à¹€à¸žà¸´à¹ˆà¸¡à¹à¸–à¸§)
vertical_concat = pd.concat([df1, df2], axis=0, ignore_index=True)
print("Vertical Concatenation:")
print(vertical_concat)

# Horizontal concatenation (à¹€à¸žà¸´à¹ˆà¸¡ columns)
horizontal_concat = pd.concat([df1, df3], axis=1)
print("Horizontal Concatenation:")
print(horizontal_concat)

# Concatenation à¸à¸±à¸š keys
keyed_concat = pd.concat([df1, df2], keys=['Group1', 'Group2'])
print("Concatenation with keys:")
print(keyed_concat)
```

### Join Method

```python
# à¹ƒà¸Šà¹‰ join method (à¹€à¸«à¸¡à¸²à¸°à¸à¸±à¸š index-based joining)
df_indexed1 = employees.set_index('EmployeeID')
df_indexed2 = salaries.set_index('EmployeeID')

# Join (default à¹€à¸›à¹‡à¸™ left join)
joined = df_indexed1.join(df_indexed2)
print("Join result:")
print(joined)

# Join à¸à¸±à¸š different join types
inner_joined = df_indexed1.join(df_indexed2, how='inner')
outer_joined = df_indexed1.join(df_indexed2, how='outer')
```

---

## Time Series Analysis

### à¸à¸²à¸£à¸ˆà¸±à¸”à¸à¸²à¸£ Datetime

```python
# à¸ªà¸£à¹‰à¸²à¸‡ datetime data
dates = pd.date_range('2023-01-01', periods=100, freq='D')
ts_data = pd.DataFrame({
    'Date': dates,
    'Sales': np.random.randint(100, 1000, 100),
    'Temperature': np.random.normal(25, 5, 100)
})

# à¹à¸›à¸¥à¸‡ string à¹€à¸›à¹‡à¸™ datetime
date_strings = ['2023-01-01', '2023-01-02', '2023-01-03']
converted_dates = pd.to_datetime(date_strings)

# Parse datetime à¸ˆà¸²à¸ format à¹€à¸‰à¸žà¸²à¸°
custom_dates = ['01/15/2023', '02/15/2023', '03/15/2023']
parsed_dates = pd.to_datetime(custom_dates, format='%m/%d/%Y')

# Set datetime à¹€à¸›à¹‡à¸™ index
ts_data.set_index('Date', inplace=True)
print(ts_data.head())
```

### Time-based Indexing à¹à¸¥à¸° Slicing

```python
# à¹€à¸¥à¸·à¸­à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸•à¸²à¸¡ date range
jan_data = ts_data['2023-01']                    # à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸”à¸·à¸­à¸™à¸¡à¸à¸£à¸²à¸„à¸¡
first_week = ts_data['2023-01-01':'2023-01-07']  # à¸ªà¸±à¸›à¸”à¸²à¸«à¹Œà¹à¸£à¸
recent_data = ts_data.last('30D')                # 30 à¸§à¸±à¸™à¸¥à¹ˆà¸²à¸ªà¸¸à¸”

# à¹€à¸¥à¸·à¸­à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸‰à¸žà¸²à¸°à¸§à¸±à¸™
mondays = ts_data[ts_data.index.dayofweek == 0]  # à¸§à¸±à¸™à¸ˆà¸±à¸™à¸—à¸£à¹Œ
weekends = ts_data[ts_data.index.dayofweek.isin([5, 6])]  # à¹€à¸ªà¸²à¸£à¹Œ-à¸­à¸²à¸—à¸´à¸•à¸¢à¹Œ

# à¹€à¸¥à¸·à¸­à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸‰à¸žà¸²à¸°à¹€à¸§à¸¥à¸² (à¸–à¹‰à¸²à¸¡à¸µ time component)
ts_hourly = pd.DataFrame({
    'Datetime': pd.date_range('2023-01-01', periods=48, freq='H'),
    'Value': np.random.randn(48)
}).set_index('Datetime')

business_hours = ts_hourly.between_time('09:00', '17:00')
```

### Resampling à¹à¸¥à¸° Frequency Conversion

```python
# à¸ªà¸£à¹‰à¸²à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸£à¸²à¸¢à¸§à¸±à¸™
daily_data = pd.DataFrame({
    'Date': pd.date_range('2023-01-01', periods=365, freq='D'),
    'Sales': np.random.randint(100, 1000, 365),
    'Visitors': np.random.randint(50, 500, 365)
}).set_index('Date')

# Upsample (à¹€à¸žà¸´à¹ˆà¸¡à¸„à¸§à¸²à¸¡à¸–à¸µà¹ˆ)
hourly_upsampled = daily_data.resample('H').interpolate()

# Downsample (à¸¥à¸”à¸„à¸§à¸²à¸¡à¸–à¸µà¹ˆ)
weekly_data = daily_data.resample('W').agg({
    'Sales': 'sum',
    'Visitors': 'mean'
})

monthly_data = daily_data.resample('M').agg({
    'Sales': ['sum', 'mean', 'std'],
    'Visitors': ['mean', 'max', 'min']
})

quarterly_data = daily_data.resample('Q').sum()

print("Weekly aggregated data:")
print(weekly_data.head())
```

### Time-based Calculations

```python
# Shift operations
daily_data['Sales_Previous_Day'] = daily_data['Sales'].shift(1)
daily_data['Sales_Next_Day'] = daily_data['Sales'].shift(-1)
daily_data['Sales_Weekly_Lag'] = daily_data['Sales'].shift(7)

# Percentage change
daily_data['Sales_Pct_Change'] = daily_data['Sales'].pct_change()
daily_data['Sales_Pct_Change_Weekly'] = daily_data['Sales'].pct_change(periods=7)

# Cumulative operations
daily_data['Sales_Cumsum'] = daily_data['Sales'].cumsum()
daily_data['Sales_Cummax'] = daily_data['Sales'].cummax()

# Rolling statistics
daily_data['Sales_MA_7'] = daily_data['Sales'].rolling(window=7).mean()
daily_data['Sales_MA_30'] = daily_data['Sales'].rolling(window=30).mean()
daily_data['Sales_Std_7'] = daily_data['Sales'].rolling(window=7).std()

# Expanding statistics
daily_data['Sales_Expanding_Mean'] = daily_data['Sales'].expanding().mean()

print(daily_data[['Sales', 'Sales_MA_7', 'Sales_MA_30']].head(10))
```

### Time Zone Handling

```python
# à¸ªà¸£à¹‰à¸²à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸à¸±à¸š timezone
utc_data = pd.DataFrame({
    'Datetime': pd.date_range('2023-01-01', periods=24, freq='H', tz='UTC'),
    'Value': np.random.randn(24)
}).set_index('Datetime')

# à¹à¸›à¸¥à¸‡ timezone
bkk_data = utc_data.tz_convert('Asia/Bangkok')
ny_data = utc_data.tz_convert('America/New_York')

# Localize timezone (à¸ªà¸³à¸«à¸£à¸±à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¸¡à¸µ timezone)
naive_data = pd.DataFrame({
    'Datetime': pd.date_range('2023-01-01', periods=24, freq='H'),
    'Value': np.random.randn(24)
}).set_index('Datetime')

localized_data = naive_data.tz_localize('Asia/Bangkok')
```

---

## Visualization

### à¸à¸²à¸£à¹ƒà¸Šà¹‰ Pandas Plot

```python
import matplotlib.pyplot as plt

# à¸ªà¸£à¹‰à¸²à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡
data = pd.DataFrame({
    'Date': pd.date_range('2023-01-01', periods=100, freq='D'),
    'Sales': np.random.randint(100, 1000, 100),
    'Profit': np.random.randint(10, 100, 100),
    'Category': np.random.choice(['A', 'B', 'C'], 100)
}).set_index('Date')

# Line plot
data['Sales'].plot(kind='line', title='Sales Over Time', figsize=(10, 6))
plt.show()

# Multiple lines
data[['Sales', 'Profit']].plot(kind='line', title='Sales and Profit', figsize=(10, 6))
plt.show()

# Bar plot
monthly_sales = data.resample('M')['Sales'].sum()
monthly_sales.plot(kind='bar', title='Monthly Sales', figsize=(10, 6))
plt.show()

# Histogram
data['Sales'].plot(kind='hist', bins=20, title='Sales Distribution', figsize=(8, 6))
plt.show()

# Box plot
data.boxplot(column=['Sales', 'Profit'], figsize=(8, 6))
plt.show()

# Scatter plot
data.plot(kind='scatter', x='Sales', y='Profit', title='Sales vs Profit', figsize=(8, 6))
plt.show()
```

### Advanced Plotting

```python
# Group by plots
category_data = data.groupby('Category')['Sales'].sum()
category_data.plot(kind='pie', title='Sales by Category', figsize=(8, 8))
plt.show()

# Subplots
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

data['Sales'].plot(ax=axes[0,0], title='Sales')
data['Profit'].plot(ax=axes[0,1], title='Profit')
data['Sales'].hist(ax=axes[1,0], bins=20, title='Sales Distribution')
data.plot(kind='scatter', x='Sales', y='Profit', ax=axes[1,1], title='Sales vs Profit')

plt.tight_layout()
plt.show()

# Rolling average plot
data['Sales_MA'] = data['Sales'].rolling(window=10).mean()
data[['Sales', 'Sales_MA']].plot(title='Sales with Moving Average', figsize=(12, 6))
plt.show()
```

---

## Performance Optimization

### Memory Optimization

```python
# à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸à¸²à¸£à¹ƒà¸Šà¹‰ memory
def memory_usage(df):
    return df.memory_usage(deep=True).sum() / 1024 / 1024  # MB

# à¸ªà¸£à¹‰à¸²à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡
large_df = pd.DataFrame({
    'int_col': np.random.randint(0, 100, 1000000),
    'float_col': np.random.random(1000000),
    'str_col': np.random.choice(['A', 'B', 'C', 'D'], 1000000)
})

print(f"Original memory usage: {memory_usage(large_df):.2f} MB")

# Optimize data types
optimized_df = large_df.copy()

# Integer optimization
optimized_df['int_col'] = pd.to_numeric(optimized_df['int_col'], downcast='integer')

# Float optimization
optimized_df['float_col'] = pd.to_numeric(optimized_df['float_col'], downcast='float')

# Category optimization à¸ªà¸³à¸«à¸£à¸±à¸š string à¸—à¸µà¹ˆà¸¡à¸µà¸„à¹ˆà¸²à¸‹à¹‰à¸³à¹†
optimized_df['str_col'] = optimized_df['str_col'].astype('category')

print(f"Optimized memory usage: {memory_usage(optimized_df):.2f} MB")
print(f"Memory reduction: {(memory_usage(large_df) - memory_usage(optimized_df))/memory_usage(large_df)*100:.1f}%")

# à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š data types
print("\nOriginal dtypes:")
print(large_df.dtypes)
print("\nOptimized dtypes:")
print(optimized_df.dtypes)
```

### Vectorization vs Loops

```python
import time

# à¸ªà¸£à¹‰à¸²à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸‚à¸™à¸²à¸”à¹ƒà¸«à¸à¹ˆ
df_large = pd.DataFrame({
    'A': np.random.randint(1, 100, 100000),
    'B': np.random.randint(1, 100, 100000)
})

# Method 1: Loop (à¸Šà¹‰à¸²)
start_time = time.time()
result_loop = []
for i in range(len(df_large)):
    result_loop.append(df_large.iloc[i]['A'] * df_large.iloc[i]['B'])
df_large['Result_Loop'] = result_loop
loop_time = time.time() - start_time

# Method 2: Vectorization (à¹€à¸£à¹‡à¸§)
start_time = time.time()
df_large['Result_Vectorized'] = df_large['A'] * df_large['B']
vectorized_time = time.time() - start_time

# Method 3: Apply (à¸à¸¥à¸²à¸‡)
start_time = time.time()
df_large['Result_Apply'] = df_large.apply(lambda row: row['A'] * row['B'], axis=1)
apply_time = time.time() - start_time

print(f"Loop time: {loop_time:.4f} seconds")
print(f"Vectorized time: {vectorized_time:.4f} seconds")
print(f"Apply time: {apply_time:.4f} seconds")
print(f"Vectorization speedup: {loop_time/vectorized_time:.1f}x")
```

### Efficient Data Loading

```python
# à¸à¸²à¸£à¸­à¹ˆà¸²à¸™à¹„à¸Ÿà¸¥à¹Œà¸­à¸¢à¹ˆà¸²à¸‡à¸¡à¸µà¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸ž
def efficient_csv_reading():
    # à¸­à¹ˆà¸²à¸™à¹€à¸‰à¸žà¸²à¸° columns à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£
    df = pd.read_csv('large_file.csv', usecols=['col1', 'col2', 'col3'])
    
    # à¸à¸³à¸«à¸™à¸” data types à¸¥à¹ˆà¸§à¸‡à¸«à¸™à¹‰à¸²
    dtypes = {
        'col1': 'int32',
        'col2': 'category',
        'col3': 'float32'
    }
    df = pd.read_csv('large_file.csv', dtype=dtypes)
    
    # à¸­à¹ˆà¸²à¸™à¸—à¸µà¸¥à¸° chunk à¸ªà¸³à¸«à¸£à¸±à¸šà¹„à¸Ÿà¸¥à¹Œà¸‚à¸™à¸²à¸”à¹ƒà¸«à¸à¹ˆ
    chunk_size = 10000
    chunks = []
    for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):
        # à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥ chunk
        processed_chunk = chunk.groupby('category').sum()
        chunks.append(processed_chunk)
    
    # à¸£à¸§à¸¡ chunks
    result = pd.concat(chunks, ignore_index=True)
    return result

# à¸à¸²à¸£à¹ƒà¸Šà¹‰ compression
df = pd.read_csv('data.csv.gz', compression='gzip')  # à¸­à¹ˆà¸²à¸™à¹„à¸Ÿà¸¥à¹Œà¸—à¸µà¹ˆ compress
df.to_csv('output.csv.gz', compression='gzip', index=False)  # à¸šà¸±à¸™à¸—à¸¶à¸à¹à¸šà¸š compress
```

### Query Optimization

```python
# à¹ƒà¸Šà¹‰ query à¹à¸—à¸™ boolean indexing à¸ªà¸³à¸«à¸£à¸±à¸š performance à¸—à¸µà¹ˆà¸”à¸µà¸à¸§à¹ˆà¸²
large_df = pd.DataFrame({
    'A': np.random.randint(1, 1000, 1000000),
    'B': np.random.randint(1, 1000, 1000000),
    'C': np.random.choice(['X', 'Y', 'Z'], 1000000)
})

# Slow: boolean indexing
start_time = time.time()
result1 = large_df[(large_df['A'] > 500) & (large_df['B'] < 300) & (large_df['C'] == 'X')]
boolean_time = time.time() - start_time

# Fast: query method
start_time = time.time()
result2 = large_df.query('A > 500 and B < 300 and C == "X"')
query_time = time.time() - start_time

print(f"Boolean indexing: {boolean_time:.4f} seconds")
print(f"Query method: {query_time:.4f} seconds")
```

---

## à¹à¸šà¸šà¸à¸¶à¸à¸«à¸±à¸”

### à¹à¸šà¸šà¸à¸¶à¸à¸«à¸±à¸”à¸—à¸µà¹ˆ 1: Series à¹à¸¥à¸° DataFrame à¸žà¸·à¹‰à¸™à¸à¸²à¸™ (15 à¸„à¸°à¹à¸™à¸™)

#### à¸‚à¹‰à¸­ 1.1 (5 à¸„à¸°à¹à¸™à¸™)
à¸ªà¸£à¹‰à¸²à¸‡ Series à¹à¸¥à¸° DataFrame à¸•à¸²à¸¡à¸—à¸µà¹ˆà¸à¸³à¸«à¸™à¸”:

```python
# TODO: à¹€à¸‚à¸µà¸¢à¸™à¹‚à¸„à¹‰à¸”à¸ªà¸£à¹‰à¸²à¸‡
# a) Series à¸—à¸µà¹ˆà¸¡à¸µ index à¹€à¸›à¹‡à¸™à¸Šà¸·à¹ˆà¸­à¹€à¸”à¸·à¸­à¸™ à¹à¸¥à¸° values à¹€à¸›à¹‡à¸™à¸ˆà¸³à¸™à¸§à¸™à¸§à¸±à¸™à¹ƒà¸™à¹à¸•à¹ˆà¸¥à¸°à¹€à¸”à¸·à¸­à¸™
# b) DataFrame à¸ªà¸³à¸«à¸£à¸±à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸™à¸±à¸à¹€à¸£à¸µà¸¢à¸™ 5 à¸„à¸™ à¸›à¸£à¸°à¸à¸­à¸šà¸”à¹‰à¸§à¸¢:
#    - à¸Šà¸·à¹ˆà¸­ (Name)
#    - à¸­à¸²à¸¢à¸¸ (Age) 
#    - à¹€à¸à¸£à¸” (Grade)
#    - à¹€à¸¡à¸·à¸­à¸‡ (City)
# c) à¹à¸ªà¸”à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸šà¸·à¹‰à¸­à¸‡à¸•à¹‰à¸™à¸‚à¸­à¸‡ DataFrame (shape, dtypes, info)
```

**à¹€à¸‰à¸¥à¸¢:**
```python
# a) Series à¹€à¸”à¸·à¸­à¸™
months_days = pd.Series(
    [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31],
    index=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',
           'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'],
    name='Days_in_Month'
)

# b) DataFrame à¸™à¸±à¸à¹€à¸£à¸µà¸¢à¸™
students = pd.DataFrame({
    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],
    'Age': [20, 21, 19, 22, 20],
    'Grade': ['A', 'B', 'A', 'C', 'B'],
    'City': ['Bangkok', 'Chiang Mai', 'Phuket', 'Bangkok', 'Pattaya']
})

# c) à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸šà¸·à¹‰à¸­à¸‡à¸•à¹‰à¸™
print(f"Shape: {students.shape}")
print(f"Data types:\n{students.dtypes}")
print(f"Info:\n{students.info()}")
```

#### à¸‚à¹‰à¸­ 1.2 (5 à¸„à¸°à¹à¸™à¸™)
à¸ˆà¸²à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸•à¹ˆà¸­à¹„à¸›à¸™à¸µà¹‰ à¹ƒà¸«à¹‰à¸—à¸³à¸à¸²à¸£ selection à¹à¸¥à¸° filtering:

```python
sales_data = pd.DataFrame({
    'Date': pd.date_range('2023-01-01', periods=50, freq='D'),
    'Product': np.random.choice(['A', 'B', 'C'], 50),
    'Sales': np.random.randint(100, 1000, 50),
    'Region': np.random.choice(['North', 'South', 'East', 'West'], 50)
})

# TODO: à¹€à¸‚à¸µà¸¢à¸™à¹‚à¸„à¹‰à¸”
# a) à¹€à¸¥à¸·à¸­à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ªà¸´à¸™à¸„à¹‰à¸² 'A' à¸—à¸µà¹ˆà¸‚à¸²à¸¢à¹„à¸”à¹‰à¸¡à¸²à¸à¸à¸§à¹ˆà¸² 500
# b) à¸«à¸²à¸„à¹ˆà¸²à¹€à¸‰à¸¥à¸µà¹ˆà¸¢à¸¢à¸­à¸”à¸‚à¸²à¸¢à¸‚à¸­à¸‡à¹à¸•à¹ˆà¸¥à¸°à¸ à¸¹à¸¡à¸´à¸ à¸²à¸„
# c) à¹€à¸¥à¸·à¸­à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ 10 à¸§à¸±à¸™à¹à¸£à¸à¸‚à¸­à¸‡à¹€à¸”à¸·à¸­à¸™à¸¡à¸à¸£à¸²à¸„à¸¡
# d) à¸ªà¸£à¹‰à¸²à¸‡ column à¹ƒà¸«à¸¡à¹ˆà¸Šà¸·à¹ˆà¸­ 'Sales_Category' à¸—à¸µà¹ˆà¹à¸šà¹ˆà¸‡à¹€à¸›à¹‡à¸™ 'High' (>700), 'Medium' (400-700), 'Low' (<400)
```

#### à¸‚à¹‰à¸­ 1.3 (5 à¸„à¸°à¹à¸™à¸™)
à¹€à¸‚à¸µà¸¢à¸™à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ DataFrame:

```python
def analyze_dataframe(df, numeric_cols=None):
    """
    à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ DataFrame à¹à¸¥à¸°à¸ªà¹ˆà¸‡à¸à¸¥à¸±à¸šà¸ªà¸£à¸¸à¸›à¸œà¸¥
    
    Parameters:
    df: DataFrame à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ
    numeric_cols: list à¸‚à¸­à¸‡ numeric columns (à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸£à¸°à¸šà¸¸à¸ˆà¸°à¹ƒà¸Šà¹‰à¸—à¸¸à¸ numeric columns)
    
    Returns:
    dict à¸—à¸µà¹ˆà¸¡à¸µà¸‚à¹‰à¸­à¸¡à¸¹à¸¥:
    - shape: à¸£à¸¹à¸›à¸£à¹ˆà¸²à¸‡à¸‚à¸­à¸‡ DataFrame
    - missing_values: à¸ˆà¸³à¸™à¸§à¸™ missing values à¹ƒà¸™à¹à¸•à¹ˆà¸¥à¸° column
    - numeric_summary: à¸ªà¸–à¸´à¸•à¸´à¸‚à¸­à¸‡ numeric columns
    - categorical_summary: à¸ˆà¸³à¸™à¸§à¸™ unique values à¹ƒà¸™à¹à¸•à¹ˆà¸¥à¸° categorical column
    """
    # TODO: implement function
    pass

# à¸—à¸”à¸ªà¸­à¸šà¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™
test_data = pd.DataFrame({
    'A': [1, 2, np.nan, 4, 5],
    'B': ['x', 'y', 'x', 'z', 'y'],
    'C': [10.5, 20.3, 30.1, np.nan, 50.2]
})

result = analyze_dataframe(test_data)
```

### à¹à¸šà¸šà¸à¸¶à¸à¸«à¸±à¸”à¸—à¸µà¹ˆ 2: Data Cleaning (20 à¸„à¸°à¹à¸™à¸™)

#### à¸‚à¹‰à¸­ 2.1 (10 à¸„à¸°à¹à¸™à¸™)
à¸ˆà¸²à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸¡à¸µà¸›à¸±à¸à¸«à¸²à¸•à¹ˆà¸­à¹„à¸›à¸™à¸µà¹‰ à¹ƒà¸«à¹‰à¸—à¸³à¸„à¸§à¸²à¸¡à¸ªà¸°à¸­à¸²à¸”:

```python
messy_data = pd.DataFrame({
    'Name': ['  Alice  ', 'BOB', 'charlie brown', '  DIANA', 'Eve'],
    'Age': [25, 'thirty', 35, np.nan, '28'],
    'Email': ['alice@gmail.com', 'BOB@YAHOO.COM', 'charlie@gmail.com', 'diana@', 'eve@outlook.com'],
    'Salary': ['50,000', '60000', 'sixty thousand', '55,000', np.nan],
    'Phone': ['123-456-7890', '(234) 567-8901', '345.678.9012', '456 789 0123', '567-890-1234'],
    'Date_Joined': ['2023-01-01', '2023/02/15', '01-03-2023', '2023.04.10', '2023-05-20']
})

# TODO: à¸—à¸³à¸„à¸§à¸²à¸¡à¸ªà¸°à¸­à¸²à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥
# 1. à¸—à¸³à¸„à¸§à¸²à¸¡à¸ªà¸°à¸­à¸²à¸” Name column (trim, proper case)
# 2. à¹à¸›à¸¥à¸‡ Age à¹€à¸›à¹‡à¸™ numeric (handle text values)
# 3. à¸—à¸³à¸„à¸§à¸²à¸¡à¸ªà¸°à¸­à¸²à¸” Email (lowercase, validate format)
# 4. à¹à¸›à¸¥à¸‡ Salary à¹€à¸›à¹‡à¸™ numeric (remove commas, handle text)
# 5. à¸ˆà¸±à¸”à¸£à¸¹à¸›à¹à¸šà¸š Phone à¹ƒà¸«à¹‰à¹€à¸›à¹‡à¸™ xxx-xxx-xxxx
# 6. à¹à¸›à¸¥à¸‡ Date_Joined à¹€à¸›à¹‡à¸™ datetime
# 7. à¸ˆà¸±à¸”à¸à¸²à¸£ missing values à¸”à¹‰à¸§à¸¢à¸§à¸´à¸˜à¸µà¸—à¸µà¹ˆà¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡
```

#### à¸‚à¹‰à¸­ 2.2 (10 à¸„à¸°à¹à¸™à¸™)
à¹€à¸‚à¸µà¸¢à¸™à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™ data cleaning pipeline:

```python
def clean_employee_data(df):
    """
    Data cleaning pipeline à¸ªà¸³à¸«à¸£à¸±à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸žà¸™à¸±à¸à¸‡à¸²à¸™
    
    Parameters:
    df: DataFrame à¸—à¸µà¹ˆà¸¡à¸µ columns: Name, Age, Email, Salary, Phone, Date_Joined
    
    Returns:
    DataFrame à¸—à¸µà¹ˆà¸—à¸³à¸„à¸§à¸²à¸¡à¸ªà¸°à¸­à¸²à¸”à¹à¸¥à¹‰à¸§
    
    Cleaning steps:
    1. Remove leading/trailing spaces from string columns
    2. Convert text ages to numeric
    3. Standardize email format (lowercase)
    4. Convert salary strings to numeric
    5. Standardize phone format
    6. Convert date strings to datetime
    7. Handle missing values appropriately
    8. Remove duplicates
    9. Add validation flags for invalid data
    """
    # TODO: implement cleaning pipeline
    pass

# à¸—à¸”à¸ªà¸­à¸š pipeline
cleaned_data = clean_employee_data(messy_data)
print("Cleaned data:")
print(cleaned_data)
print(f"\nData types after cleaning:\n{cleaned_data.dtypes}")
```

### à¹à¸šà¸šà¸à¸¶à¸à¸«à¸±à¸”à¸—à¸µà¹ˆ 3: Groupby à¹à¸¥à¸° Aggregation (20 à¸„à¸°à¹à¸™à¸™)

#### à¸‚à¹‰à¸­ 3.1 (10 à¸„à¸°à¹à¸™à¸™)
à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸à¸²à¸£à¸‚à¸²à¸¢à¸•à¹ˆà¸­à¹„à¸›à¸™à¸µà¹‰:

```python
sales_analysis = pd.DataFrame({
    'Date': pd.date_range('2023-01-01', periods=365, freq='D'),
    'Product': np.random.choice(['Laptop', 'Phone', 'Tablet', 'Watch'], 365),
    'Category': np.random.choice(['Electronics', 'Accessories'], 365),
    'Region': np.random.choice(['North', 'South', 'East', 'West'], 365),
    'Salesperson': np.random.choice(['Alice', 'Bob', 'Charlie', 'Diana'], 365),
    'Units_Sold': np.random.randint(1, 20, 365),
    'Unit_Price': np.random.randint(100, 2000, 365)
})
sales_analysis['Total_Sales'] = sales_analysis['Units_Sold'] * sales_analysis['Unit_Price']

# TODO: à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸‚à¹‰à¸­à¸¡à¸¹à¸¥
# 1. à¸¢à¸­à¸”à¸‚à¸²à¸¢à¸£à¸§à¸¡à¹à¸¥à¸°à¹€à¸‰à¸¥à¸µà¹ˆà¸¢à¸‚à¸­à¸‡à¹à¸•à¹ˆà¸¥à¸°à¸ªà¸´à¸™à¸„à¹‰à¸²
# 2. Top 3 salesperson à¸—à¸µà¹ˆà¸¡à¸µà¸¢à¸­à¸”à¸‚à¸²à¸¢à¸ªà¸¹à¸‡à¸ªà¸¸à¸”
# 3. à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸šà¸¢à¸­à¸”à¸‚à¸²à¸¢à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡à¸ à¸¹à¸¡à¸´à¸ à¸²à¸„à¹ƒà¸™à¹à¸•à¹ˆà¸¥à¸°à¹€à¸”à¸·à¸­à¸™
# 4. à¸œà¸¥à¸´à¸•à¸ à¸±à¸“à¸‘à¹Œà¸—à¸µà¹ˆà¸‚à¸²à¸¢à¸”à¸µà¸—à¸µà¹ˆà¸ªà¸¸à¸”à¹ƒà¸™à¹à¸•à¹ˆà¸¥à¸°à¹„à¸•à¸£à¸¡à¸²à¸ª
# 5. Seasonal analysis: à¸¢à¸­à¸”à¸‚à¸²à¸¢à¹€à¸‰à¸¥à¸µà¹ˆà¸¢à¹ƒà¸™à¹à¸•à¹ˆà¸¥à¸°à¹€à¸”à¸·à¸­à¸™
```

#### à¸‚à¹‰à¸­ 3.2 (10 à¸„à¸°à¹à¸™à¸™)
à¸ªà¸£à¹‰à¸²à¸‡ advanced aggregation functions:

```python
def sales_metrics(group):
    """
    à¸„à¸³à¸™à¸§à¸“ metrics à¸ªà¸³à¸«à¸£à¸±à¸šà¸¢à¸­à¸”à¸‚à¸²à¸¢
    
    Returns:
    Series à¸—à¸µà¹ˆà¸¡à¸µ:
    - total_sales: à¸¢à¸­à¸”à¸‚à¸²à¸¢à¸£à¸§à¸¡
    - avg_sales: à¸¢à¸­à¸”à¸‚à¸²à¸¢à¹€à¸‰à¸¥à¸µà¹ˆà¸¢
    - sales_std: à¸ªà¹ˆà¸§à¸™à¹€à¸šà¸µà¹ˆà¸¢à¸‡à¹€à¸šà¸™à¸¡à¸²à¸•à¸£à¸à¸²à¸™
    - max_sales: à¸¢à¸­à¸”à¸‚à¸²à¸¢à¸ªà¸¹à¸‡à¸ªà¸¸à¸”
    - min_sales: à¸¢à¸­à¸”à¸‚à¸²à¸¢à¸•à¹ˆà¸³à¸ªà¸¸à¸”
    - sales_range: à¸„à¸§à¸²à¸¡à¹à¸•à¸à¸•à¹ˆà¸²à¸‡à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡à¸ªà¸¹à¸‡à¸ªà¸¸à¸”à¹à¸¥à¸°à¸•à¹ˆà¸³à¸ªà¸¸à¸”
    - coefficient_of_variation: CV
    """
    # TODO: implement metrics calculation
    pass

def product_performance_analysis(df, date_col, product_col, sales_col):
    """
    à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ performance à¸‚à¸­à¸‡à¸ªà¸´à¸™à¸„à¹‰à¸²
    
    Parameters:
    df: DataFrame
    date_col: à¸Šà¸·à¹ˆà¸­ column à¸—à¸µà¹ˆà¹€à¸›à¹‡à¸™ date
    product_col: à¸Šà¸·à¹ˆà¸­ column à¸—à¸µà¹ˆà¹€à¸›à¹‡à¸™ product
    sales_col: à¸Šà¸·à¹ˆà¸­ column à¸—à¸µà¹ˆà¹€à¸›à¹‡à¸™ sales
    
    Returns:
    DataFrame à¸—à¸µà¹ˆà¸¡à¸µ analysis results à¸ªà¸³à¸«à¸£à¸±à¸šà¹à¸•à¹ˆà¸¥à¸°à¸ªà¸´à¸™à¸„à¹‰à¸²
    """
    # TODO: implement product analysis
    pass

# à¸—à¸”à¸ªà¸­à¸šà¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™
metrics_result = sales_analysis.groupby('Product').apply(sales_metrics)
performance_result = product_performance_analysis(sales_analysis, 'Date', 'Product', 'Total_Sales')
```

### à¹à¸šà¸šà¸à¸¶à¸à¸«à¸±à¸”à¸—à¸µà¹ˆ 4: Time Series à¹à¸¥à¸° Merging (25 à¸„à¸°à¹à¸™à¸™)

#### à¸‚à¹‰à¸­ 4.1 (15 à¸„à¸°à¹à¸™à¸™)
à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ time series data:

```python
# à¸ªà¸£à¹‰à¸²à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ stock prices
stock_data = pd.DataFrame({
    'Date': pd.date_range('2023-01-01', periods=252, freq='B'),  # Business days
    'AAPL': np.random.randn(252).cumsum() + 150,
    'GOOGL': np.random.randn(252).cumsum() + 2500,
    'MSFT': np.random.randn(252).cumsum() + 250,
    'TSLA': np.random.randn(252).cumsum() + 200
}).set_index('Date')

# TODO: Time series analysis
# 1. à¸„à¸³à¸™à¸§à¸“ daily returns à¸ªà¸³à¸«à¸£à¸±à¸šà¹à¸•à¹ˆà¸¥à¸°à¸«à¸¸à¹‰à¸™
# 2. à¸„à¸³à¸™à¸§à¸“ 20-day à¹à¸¥à¸° 50-day moving averages
# 3. à¸«à¸² correlation matrix à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡à¸«à¸¸à¹‰à¸™à¸•à¹ˆà¸²à¸‡à¹†
# 4. à¸„à¸³à¸™à¸§à¸“ monthly returns à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰ resampling
# 5. à¸«à¸²à¸§à¸±à¸™à¸—à¸µà¹ˆà¸¡à¸µ volatility à¸ªà¸¹à¸‡à¸ªà¸¸à¸” (à¹ƒà¸Šà¹‰ rolling standard deviation)
# 6. à¸ªà¸£à¹‰à¸²à¸‡ simple trading signal: à¸‹à¸·à¹‰à¸­à¹€à¸¡à¸·à¹ˆà¸­ 20-day MA > 50-day MA
```

#### à¸‚à¹‰à¸­ 4.2 (10 à¸„à¸°à¹à¸™à¸™)
Merging à¹à¸¥à¸° joining exercises:

```python
# à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸•à¸²à¸£à¸²à¸‡
customers = pd.DataFrame({
    'CustomerID': [1, 2, 3, 4, 5],
    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],
    'City': ['Bangkok', 'Chiang Mai', 'Phuket', 'Bangkok', 'Pattaya'],
    'Age': [25, 30, 35, 28, 32]
})

orders = pd.DataFrame({
    'OrderID': [101, 102, 103, 104, 105, 106],
    'CustomerID': [1, 2, 1, 3, 2, 6],
    'Product': ['Laptop', 'Phone', 'Tablet', 'Watch', 'Laptop', 'Phone'],
    'Amount': [50000, 25000, 15000, 8000, 52000, 26000],
    'Order_Date': pd.date_range('2023-01-01', periods=6, freq='10D')
})

products = pd.DataFrame({
    'Product': ['Laptop', 'Phone', 'Tablet', 'Watch', 'Headphones'],
    'Category': ['Computer', 'Mobile', 'Computer', 'Wearable', 'Audio'],
    'Cost': [40000, 20000, 12000, 6000, 3000]
})

# TODO: Merging exercises
# 1. à¸£à¸§à¸¡ customers à¹à¸¥à¸° orders (à¹à¸ªà¸”à¸‡à¸¥à¸¹à¸à¸„à¹‰à¸²à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¸£à¸§à¸¡à¸–à¸¶à¸‡à¸„à¸™à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¹€à¸„à¸¢à¸ªà¸±à¹ˆà¸‡à¸‹à¸·à¹‰à¸­)
# 2. à¸£à¸§à¸¡ orders à¹à¸¥à¸° products à¹€à¸žà¸·à¹ˆà¸­à¸«à¸² profit à¸‚à¸­à¸‡à¹à¸•à¹ˆà¸¥à¸° order
# 3. à¸ªà¸£à¹‰à¸²à¸‡ comprehensive report à¸—à¸µà¹ˆà¸£à¸§à¸¡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸±à¹‰à¸‡à¸ªà¸²à¸¡à¸•à¸²à¸£à¸²à¸‡
# 4. à¸«à¸²à¸¥à¸¹à¸à¸„à¹‰à¸²à¸—à¸µà¹ˆà¹ƒà¸Šà¹‰à¸ˆà¹ˆà¸²à¸¢à¸¡à¸²à¸à¸—à¸µà¹ˆà¸ªà¸¸à¸”à¹ƒà¸™à¹à¸•à¹ˆà¸¥à¸°à¹€à¸¡à¸·à¸­à¸‡
# 5. à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸¢à¸­à¸”à¸‚à¸²à¸¢à¸•à¸²à¸¡ category à¹à¸¥à¸° customer demographics
```

### à¹à¸šà¸šà¸à¸¶à¸à¸«à¸±à¸”à¸—à¸µà¹ˆ 5: Advanced Operations (20 à¸„à¸°à¹à¸™à¸™)

#### à¸‚à¹‰à¸­ 5.1 (10 à¸„à¸°à¹à¸™à¸™)
à¸à¸²à¸£à¹ƒà¸Šà¹‰ pivot tables à¹à¸¥à¸° reshaping:

```python
# à¸ªà¸£à¹‰à¸²à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ survey
survey_data = pd.DataFrame({
    'RespondentID': range(1, 101),
    'Age_Group': np.random.choice(['18-25', '26-35', '36-45', '46-55', '55+'], 100),
    'Gender': np.random.choice(['Male', 'Female', 'Other'], 100),
    'Region': np.random.choice(['Bangkok', 'North', 'Northeast', 'Central', 'South'], 100),
    'Product_A_Rating': np.random.randint(1, 6, 100),
    'Product_B_Rating': np.random.randint(1, 6, 100),
    'Product_C_Rating': np.random.randint(1, 6, 100),
    'Overall_Satisfaction': np.random.randint(1, 6, 100)
})

# TODO: Pivot à¹à¸¥à¸° reshape operations
# 1. à¸ªà¸£à¹‰à¸²à¸‡ pivot table à¹à¸ªà¸”à¸‡ average rating à¸‚à¸­à¸‡à¹à¸•à¹ˆà¸¥à¸° product à¸•à¸²à¸¡ age group à¹à¸¥à¸° gender
# 2. Melt data à¹€à¸žà¸·à¹ˆà¸­à¹à¸›à¸¥à¸‡ product ratings à¹€à¸›à¹‡à¸™ long format
# 3. à¸ªà¸£à¹‰à¸²à¸‡ cross-tabulation à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡ region à¹à¸¥à¸° age group
# 4. à¸„à¸³à¸™à¸§à¸“ correlation à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡ product ratings à¹à¸¥à¸° overall satisfaction
# 5. à¸ªà¸£à¹‰à¸²à¸‡ summary report à¸—à¸µà¹ˆà¹à¸ªà¸”à¸‡à¸–à¸¶à¸‡ insights à¸ªà¸³à¸„à¸±à¸
```

#### à¸‚à¹‰à¸­ 5.2 (10 à¸„à¸°à¹à¸™à¸™)
Performance optimization exercise:

```python
# à¸ªà¸£à¹‰à¸²à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸‚à¸™à¸²à¸”à¹ƒà¸«à¸à¹ˆ
large_dataset = pd.DataFrame({
    'ID': range(1000000),
    'Category': np.random.choice(['A', 'B', 'C', 'D', 'E'], 1000000),
    'Value1': np.random.randn(1000000),
    'Value2': np.random.randint(1, 1000, 1000000),
    'Date': pd.date_range('2020-01-01', periods=1000000, freq='min')
})

# TODO: Optimization exercises
# 1. à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸š performance à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡ iterrows(), apply(), à¹à¸¥à¸° vectorization
# 2. Optimize memory usage à¹‚à¸”à¸¢à¸à¸²à¸£à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™ data types
# 3. à¹ƒà¸Šà¹‰ categorical data type à¸ªà¸³à¸«à¸£à¸±à¸š Category column
# 4. à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸š performance à¸‚à¸­à¸‡ groupby operations à¸à¸±à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸‚à¸™à¸²à¸”à¹ƒà¸«à¸à¹ˆ
# 5. à¸ªà¸£à¹‰à¸²à¸‡à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸—à¸µà¹ˆ process à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸­à¸¢à¹ˆà¸²à¸‡à¸¡à¸µà¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸ž

def optimize_dataframe(df):
    """
    Optimize DataFrame à¸ªà¸³à¸«à¸£à¸±à¸š memory à¹à¸¥à¸° performance
    
    Returns:
    optimized DataFrame à¹à¸¥à¸° report à¸à¸²à¸£à¸›à¸£à¸°à¸«à¸¢à¸±à¸” memory
    """
    # TODO: implement optimization
    pass
```

