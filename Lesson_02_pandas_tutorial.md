# ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤ ‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏≤‡∏£‡∏™‡∏ô‡πÄ‡∏ó‡∏®‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à 
# ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô Pandas
## ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏õ‡∏£‡∏¥‡∏ç‡∏ç‡∏≤‡∏ï‡∏£‡∏µ ‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡πÅ‡∏•‡∏∞‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏î‡∏¥‡∏à‡∏ó‡∏±‡∏• ‡∏°‡∏£‡∏¢
### ‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡πÇ‡∏ã‡∏ü‡∏µ‡∏£‡πå ‡∏´‡∏∞‡∏¢‡∏µ‡∏¢‡∏π‡πÇ‡∏ã‡πä‡∏∞

---

## üìä ‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç
1. [‡∏ö‡∏ó‡∏ô‡∏≥ Pandas](#‡∏ö‡∏ó‡∏ô‡∏≥-pandas)
2. [‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£ Import](#‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£-import)
3. [Series ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô](#series-‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô)
4. [DataFrame ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô](#dataframe-‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô)
5. [‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•](#‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)
6. [Data Selection ‡πÅ‡∏•‡∏∞ Filtering](#data-selection-‡πÅ‡∏•‡∏∞-filtering)
7. [Data Cleaning](#data-cleaning)
8. [Data Transformation](#data-transformation)
9. [Groupby Operations](#groupby-operations)
10. [Merging ‡πÅ‡∏•‡∏∞ Joining](#merging-‡πÅ‡∏•‡∏∞-joining)
11. [Time Series Analysis](#time-series-analysis)
12. [Visualization](#visualization)
13. [Performance Optimization](#performance-optimization)
14. [‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î](#‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î)
15. [‡πÇ‡∏Ñ‡∏£‡∏á‡∏á‡∏≤‡∏ô‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥](#‡πÇ‡∏Ñ‡∏£‡∏á‡∏á‡∏≤‡∏ô‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥)

---

## ‡∏ö‡∏ó‡∏ô‡∏≥ Pandas

### Pandas ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?
**Pandas** (Python Data Analysis Library) ‡πÄ‡∏õ‡πá‡∏ô library ‡∏ó‡∏µ‡πà‡∏ó‡∏£‡∏á‡∏û‡∏•‡∏±‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô Python ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏ó‡∏µ‡πà‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö structured

### ‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ Pandas?

#### ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

```python
# ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ Pandas - ‡∏¢‡∏∏‡πà‡∏á‡∏¢‡∏≤‡∏Å
data = [
    ['Alice', 25, 50000],
    ['Bob', 30, 60000],
    ['Charlie', 35, 70000]
]

# ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô
total_salary = sum(row[2] for row in data)
avg_salary = total_salary / len(data)

# ‡πÉ‡∏ä‡πâ Pandas - ‡∏á‡πà‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
import pandas as pd
df = pd.DataFrame(data, columns=['Name', 'Age', 'Salary'])
avg_salary = df['Salary'].mean()
```

### ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ç‡∏≠‡∏á Pandas
- **Easy Data Manipulation**: ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢
- **Multiple File Formats**: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö (CSV, Excel, JSON, SQL)
- **Missing Data Handling**: ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏î‡∏µ
- **Data Alignment**: ‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
- **Flexible Grouping**: ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢
- **Integration**: ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö NumPy, Matplotlib ‡πÑ‡∏î‡πâ‡∏î‡∏µ

### ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
- ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£ Data Science ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 95%
- ‡∏°‡∏µ‡∏Å‡∏≤‡∏£ download ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 50 ‡∏•‡πâ‡∏≤‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏î‡∏∑‡∏≠‡∏ô
- Essential tool ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Data Scientists ‡∏ó‡∏±‡πà‡∏ß‡πÇ‡∏•‡∏Å

---

## ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£ Import

### ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á
```bash
# ‡∏ú‡πà‡∏≤‡∏ô pip
pip install pandas

# ‡∏ú‡πà‡∏≤‡∏ô conda
conda install pandas

# ‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ö dependencies ‡∏≠‡∏∑‡πà‡∏ô‡πÜ
pip install pandas numpy matplotlib seaborn

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö version
python -c "import pandas as pd; print(pd.__version__)"
```

### ‡∏Å‡∏≤‡∏£ Import
```python
# Standard import
import pandas as pd
import numpy as np

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö version
print(f"Pandas version: {pd.__version__}")

# Import ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
from pandas import DataFrame, Series, read_csv

# ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ display options
pd.set_option('display.max_rows', 100)
pd.set_option('display.max_columns', 20)
pd.set_option('display.width', 1000)
```

### ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
```python
# ‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ó‡∏®‡∏ô‡∏¥‡∏¢‡∏°
pd.set_option('display.float_format', '{:.2f}'.format)

# ‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà
pd.set_option('display.max_colwidth', 50)

# ‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤
pd.reset_option('all')

# ‡∏î‡∏π‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
print(pd.describe_option())
```

---

## Series ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô

### Series ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?
**Series** ‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 1 ‡∏°‡∏¥‡∏ï‡∏¥‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏ô‡∏¥‡∏î‡πÉ‡∏î‡∏Å‡πá‡πÑ‡∏î‡πâ (integers, strings, floats, Python objects) ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ö labels (index)

### ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Series

```python
import pandas as pd
import numpy as np

# ‡∏à‡∏≤‡∏Å Python list
s1 = pd.Series([1, 3, 5, np.nan, 6, 8])
print(s1)

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î index ‡πÄ‡∏≠‡∏á
s2 = pd.Series([1, 3, 5, 6], index=['a', 'b', 'c', 'd'])
print(s2)

# ‡∏à‡∏≤‡∏Å dictionary
data_dict = {'A': 1, 'B': 2, 'C': 3, 'D': 4}
s3 = pd.Series(data_dict)
print(s3)

# ‡∏à‡∏≤‡∏Å NumPy array
arr = np.array([1, 2, 3, 4, 5])
s4 = pd.Series(arr, index=['x', 'y', 'z', 'w', 'v'])
print(s4)

# Series ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
s5 = pd.Series(5, index=[0, 1, 2, 3])
print(s5)
```

### ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á Series
```python
s = pd.Series([1, 3, 5, np.nan, 6, 8], index=['a', 'b', 'c', 'd', 'e', 'f'])

# ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
print(f"Values: {s.values}")          # NumPy array
print(f"Index: {s.index}")            # Index object
print(f"Data type: {s.dtype}")        # ‡∏ä‡∏ô‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
print(f"Shape: {s.shape}")            # ‡∏£‡∏π‡∏õ‡∏£‡πà‡∏≤‡∏á
print(f"Size: {s.size}")              # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô elements
print(f"Name: {s.name}")              # ‡∏ä‡∏∑‡πà‡∏≠ Series

# ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Series ‡πÅ‡∏•‡∏∞ index
s.name = 'My Series'
s.index.name = 'Labels'
print(s)
```

### ‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô Series
```python
s = pd.Series([10, 20, 30, 40, 50], index=['a', 'b', 'c', 'd', 'e'])

# ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏î‡πâ‡∏ß‡∏¢ label
print(s['b'])                    # 20
print(s[['a', 'c', 'e']])        # Multiple labels

# ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏î‡πâ‡∏ß‡∏¢ position
print(s[1])                      # 20
print(s[0:3])                    # Slicing

# Boolean indexing
print(s[s > 25])                 # ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 25

# ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
print('b' in s)                  # True
print(s.get('f', 'Not found'))   # Get with default value
```

### ‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö Series
```python
s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])
s2 = pd.Series([10, 20, 30, 40], index=['a', 'b', 'c', 'd'])

# Arithmetic operations
print(s1 + s2)                   # Element-wise addition
print(s1 * 2)                    # Scalar multiplication
print(s1 ** 2)                   # Power

# Statistical operations
print(s1.sum())                  # ‡∏ú‡∏•‡∏£‡∏ß‡∏°
print(s1.mean())                 # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
print(s1.std())                  # ‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏ö‡∏µ‡πà‡∏¢‡∏á‡πÄ‡∏ö‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô
print(s1.describe())             # ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°

# String operations (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Series ‡∏ó‡∏µ‡πà‡∏°‡∏µ string)
s_str = pd.Series(['apple', 'banana', 'cherry'])
print(s_str.str.upper())         # ‡∏ï‡∏±‡∏ß‡∏û‡∏¥‡∏°‡∏û‡πå‡πÉ‡∏´‡∏ç‡πà
print(s_str.str.len())           # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß string
print(s_str.str.contains('a'))   # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ 'a'
```

---

## DataFrame ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô

### DataFrame ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?
**DataFrame** ‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 2 ‡∏°‡∏¥‡∏ï‡∏¥‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô table ‡πÉ‡∏ô SQL ‡∏´‡∏£‡∏∑‡∏≠ spreadsheet ‡πÉ‡∏ô Excel ‡πÇ‡∏î‡∏¢‡∏°‡∏µ labeled axes (rows ‡πÅ‡∏•‡∏∞ columns)

### ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame

```python
# ‡∏à‡∏≤‡∏Å dictionary
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'Diana'],
    'Age': [25, 30, 35, 28],
    'City': ['New York', 'London', 'Tokyo', 'Paris'],
    'Salary': [50000, 60000, 70000, 55000]
}
df = pd.DataFrame(data)
print(df)

# ‡∏à‡∏≤‡∏Å list of dictionaries
data_list = [
    {'Name': 'Alice', 'Age': 25, 'City': 'New York'},
    {'Name': 'Bob', 'Age': 30, 'City': 'London'},
    {'Name': 'Charlie', 'Age': 35, 'City': 'Tokyo'}
]
df2 = pd.DataFrame(data_list)
print(df2)

# ‡∏à‡∏≤‡∏Å NumPy array
arr = np.random.randn(4, 3)
df3 = pd.DataFrame(arr, 
                   columns=['A', 'B', 'C'],
                   index=['Row1', 'Row2', 'Row3', 'Row4'])
print(df3)

# ‡∏à‡∏≤‡∏Å Series
s1 = pd.Series([1, 2, 3], name='Col1')
s2 = pd.Series([4, 5, 6], name='Col2')
df4 = pd.DataFrame({'Col1': s1, 'Col2': s2})
print(df4)
```

### ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á DataFrame
```python
df = pd.DataFrame({
    'A': [1, 2, 3, 4],
    'B': [5.0, 6.0, 7.0, 8.0],
    'C': ['foo', 'bar', 'baz', 'qux'],
    'D': [True, False, True, False]
})

# ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
print(f"Shape: {df.shape}")              # (rows, columns)
print(f"Size: {df.size}")                # total elements
print(f"Columns: {df.columns}")          # column names
print(f"Index: {df.index}")              # row indices
print(f"Data types:\n{df.dtypes}")       # data types
print(f"Memory usage:\n{df.memory_usage()}")

# ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô
print(df.head())                         # 5 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å
print(df.tail(3))                        # 3 ‡πÅ‡∏ñ‡∏ß‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
print(df.info())                         # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°
print(df.describe())                     # ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö numeric columns
```

### ‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô DataFrame

#### ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Columns
```python
df = pd.DataFrame({
    'Name': ['Alice', 'Bob', 'Charlie'],
    'Age': [25, 30, 35],
    'Salary': [50000, 60000, 70000]
})

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å 1 column (‡πÑ‡∏î‡πâ Series)
print(df['Name'])

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏•‡∏≤‡∏¢ columns (‡πÑ‡∏î‡πâ DataFrame)
print(df[['Name', 'Salary']])

# ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏î‡πâ‡∏ß‡∏¢ attribute (‡∏ñ‡πâ‡∏≤‡∏ä‡∏∑‡πà‡∏≠ column ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á)
print(df.Name)
```

#### ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Rows
```python
# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏î‡πâ‡∏ß‡∏¢ position
print(df.iloc[0])              # ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å (Series)
print(df.iloc[0:2])            # ‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà 0-1 (DataFrame)

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏î‡πâ‡∏ß‡∏¢ label
df_indexed = df.set_index('Name')
print(df_indexed.loc['Alice'])  # ‡πÅ‡∏ñ‡∏ß‡∏Ç‡∏≠‡∏á Alice

# Boolean indexing
print(df[df['Age'] > 30])       # ‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏¢‡∏∏‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 30
print(df[df['Name'].isin(['Alice', 'Charlie'])])  # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏â‡∏û‡∏≤‡∏∞
```

#### ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Cells ‡πÄ‡∏â‡∏û‡∏≤‡∏∞
```python
# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å cell ‡πÄ‡∏â‡∏û‡∏≤‡∏∞
print(df.iloc[0, 1])           # ‡πÅ‡∏ñ‡∏ß 0, column 1
print(df.loc[0, 'Age'])        # ‡πÅ‡∏ñ‡∏ß 0, column 'Age'

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ä‡πà‡∏ß‡∏á
print(df.iloc[0:2, 1:3])       # ‡πÅ‡∏ñ‡∏ß 0-1, column 1-2
print(df.loc[0:1, 'Age':'Salary'])  # ‡πÅ‡∏ñ‡∏ß 0-1, column Age-Salary
```

---

## ‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

### ‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ï‡πà‡∏≤‡∏á‡πÜ

#### CSV Files
```python
# ‡∏≠‡πà‡∏≤‡∏ô CSV ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
df = pd.read_csv('data.csv')

# ‡∏û‡∏£‡πâ‡∏≠‡∏° parameters ‡∏ï‡πà‡∏≤‡∏á‡πÜ
df = pd.read_csv(
    'data.csv',
    sep=',',                    # ‡∏ï‡∏±‡∏ß‡πÅ‡∏ö‡πà‡∏á
    header=0,                   # ‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô header
    index_col=0,                # column ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô index
    usecols=['Name', 'Age'],    # column ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
    dtype={'Age': int},         # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏ô‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    parse_dates=['Date'],       # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô datetime
    na_values=['N/A', 'NULL']   # ‡∏Ñ‡πà‡∏≤ missing
)

# ‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å URL
url = 'https://example.com/data.csv'
df = pd.read_csv(url)

# ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ö‡∏≤‡∏á‡πÅ‡∏ñ‡∏ß
df = pd.read_csv('data.csv', nrows=100)  # ‡∏≠‡πà‡∏≤‡∏ô 100 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å
df = pd.read_csv('data.csv', skiprows=10)  # ‡∏Ç‡πâ‡∏≤‡∏° 10 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å
```

#### Excel Files
```python
# ‡∏≠‡πà‡∏≤‡∏ô Excel
df = pd.read_excel('data.xlsx')

# ‡∏≠‡πà‡∏≤‡∏ô sheet ‡πÄ‡∏â‡∏û‡∏≤‡∏∞
df = pd.read_excel('data.xlsx', sheet_name='Sheet1')

# ‡∏≠‡πà‡∏≤‡∏ô‡∏´‡∏•‡∏≤‡∏¢ sheets
dfs = pd.read_excel('data.xlsx', sheet_name=None)  # dictionary ‡∏Ç‡∏≠‡∏á DataFrames

# ‡∏û‡∏£‡πâ‡∏≠‡∏° parameters
df = pd.read_excel(
    'data.xlsx',
    sheet_name='Data',
    header=0,
    index_col=0,
    usecols='A:D',             # columns A ‡∏ñ‡∏∂‡∏á D
    skiprows=2                 # ‡∏Ç‡πâ‡∏≤‡∏° 2 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å
)
```

#### JSON Files
```python
# ‡∏≠‡πà‡∏≤‡∏ô JSON
df = pd.read_json('data.json')

# JSON ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ï‡πà‡∏≤‡∏á‡πÜ
df = pd.read_json('data.json', orient='records')  # list of objects
df = pd.read_json('data.json', orient='index')    # object of objects
df = pd.read_json('data.json', lines=True)        # JSON Lines format
```

#### SQL Database
```python
import sqlite3

# ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
conn = sqlite3.connect('database.db')

# ‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å SQL query
df = pd.read_sql_query('SELECT * FROM table_name', conn)

# ‡∏≠‡πà‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á table
df = pd.read_sql_table('table_name', conn)

# ‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠
conn.close()
```

### ‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

```python
df = pd.DataFrame({
    'Name': ['Alice', 'Bob', 'Charlie'],
    'Age': [25, 30, 35],
    'Salary': [50000, 60000, 70000]
})

# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô CSV
df.to_csv('output.csv', index=False)

# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô Excel
df.to_excel('output.xlsx', sheet_name='Data', index=False)

# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô JSON
df.to_json('output.json', orient='records', indent=2)

# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
conn = sqlite3.connect('database.db')
df.to_sql('table_name', conn, if_exists='replace', index=False)
conn.close()

# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô
df[['Name', 'Age']].to_csv('partial_data.csv', index=False)
```

---

## Data Selection ‡πÅ‡∏•‡∏∞ Filtering

### ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

#### Basic Selection
```python
df = pd.DataFrame({
    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],
    'Age': [25, 30, 35, 28, 32],
    'City': ['NY', 'London', 'Tokyo', 'Paris', 'Berlin'],
    'Salary': [50000, 60000, 70000, 55000, 65000],
    'Department': ['IT', 'HR', 'IT', 'Finance', 'IT']
})

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å columns
single_col = df['Name']                    # Series
multi_cols = df[['Name', 'Age', 'Salary']] # DataFrame

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å rows ‡∏î‡πâ‡∏ß‡∏¢ conditions
high_salary = df[df['Salary'] > 60000]
it_employees = df[df['Department'] == 'IT']
young_high_earners = df[(df['Age'] < 30) & (df['Salary'] > 50000)]
```

#### iloc ‡πÅ‡∏•‡∏∞ loc
```python
# iloc - position-based selection
print(df.iloc[0])              # ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å
print(df.iloc[0:3])            # ‡πÅ‡∏ñ‡∏ß 0-2
print(df.iloc[:, 1:3])         # columns 1-2 ‡∏ó‡∏∏‡∏Å‡πÅ‡∏ñ‡∏ß
print(df.iloc[0:2, 1:3])       # ‡πÅ‡∏ñ‡∏ß 0-1, columns 1-2

# loc - label-based selection
df_with_index = df.set_index('Name')
print(df_with_index.loc['Alice'])           # ‡πÅ‡∏ñ‡∏ß‡∏Ç‡∏≠‡∏á Alice
print(df_with_index.loc['Alice':'Charlie']) # ‡πÅ‡∏ñ‡∏ß Alice ‡∏ñ‡∏∂‡∏á Charlie
print(df_with_index.loc[:, 'Age':'Salary']) # columns Age ‡∏ñ‡∏∂‡∏á Salary
```

#### Query Method
```python
# ‡πÉ‡∏ä‡πâ query ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö filtering ‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô
result1 = df.query('Age > 30')
result2 = df.query('Age > 30 and Salary < 70000')
result3 = df.query('Department == "IT"')
result4 = df.query('City in ["NY", "Tokyo"]')

# ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ external
min_age = 30
result5 = df.query('Age > @min_age')
```

### Advanced Filtering

#### ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ isin()
```python
# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡πà‡∏≤‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô
cities_of_interest = ['NY', 'Tokyo', 'Paris']
filtered = df[df['City'].isin(cities_of_interest)]

# ‡∏ï‡∏£‡∏á‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á isin
not_in_cities = df[~df['City'].isin(cities_of_interest)]
```

#### String Filtering
```python
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
df_str = pd.DataFrame({
    'Name': ['Alice Johnson', 'Bob Smith', 'Charlie Brown', 'Diana Wilson'],
    'Email': ['alice@gmail.com', 'bob@yahoo.com', 'charlie@gmail.com', 'diana@outlook.com']
})

# String methods
gmail_users = df_str[df_str['Email'].str.contains('gmail')]
starts_with_a = df_str[df_str['Name'].str.startswith('A')]
ends_with_son = df_str[df_str['Name'].str.endswith('son')]

# Case-insensitive search
contains_alice = df_str[df_str['Name'].str.contains('alice', case=False)]

# Regular expressions
pattern_match = df_str[df_str['Email'].str.match(r'\w+@gmail\.com')]
```

#### ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Missing Values ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Filter
```python
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ missing values
df_missing = pd.DataFrame({
    'A': [1, 2, np.nan, 4, 5],
    'B': [np.nan, 2, 3, 4, np.nan],
    'C': [1, 2, 3, 4, 5]
})

# Filter ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ missing values
no_missing = df_missing[df_missing['A'].notna()]

# Filter ‡∏ó‡∏µ‡πà‡∏°‡∏µ missing values
has_missing = df_missing[df_missing['A'].isna()]

# Filter ‡∏´‡∏•‡∏≤‡∏¢ columns
complete_rows = df_missing.dropna()  # ‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ missing values
any_missing = df_missing[df_missing.isna().any(axis=1)]  # ‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ missing values
```

---

## Data Cleaning

### ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Missing Values

#### ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Missing Values
```python
df = pd.DataFrame({
    'A': [1, 2, np.nan, 4, 5],
    'B': [np.nan, 2, 3, np.nan, 5],
    'C': [1, 2, 3, 4, 5],
    'D': ['a', 'b', None, 'd', 'e']
})

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö missing values
print(df.isnull().sum())           # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô missing values ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ column
print(df.isna().sum())             # ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ö isnull()
print(df.info())                   # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á missing values

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ missing values
print(df[df.isnull().any(axis=1)])

# ‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå‡∏Ç‡∏≠‡∏á missing values
missing_percent = (df.isnull().sum() / len(df)) * 100
print(missing_percent)
```

#### ‡∏Å‡∏≤‡∏£‡∏•‡∏ö Missing Values
```python
# ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ missing values
df_drop_rows = df.dropna()                    # ‡∏•‡∏ö‡∏ó‡∏∏‡∏Å‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ missing
df_drop_any = df.dropna(how='any')            # ‡∏•‡∏ö‡∏ñ‡πâ‡∏≤‡∏°‡∏µ missing ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏Ñ‡πà‡∏≤
df_drop_all = df.dropna(how='all')            # ‡∏•‡∏ö‡∏ñ‡πâ‡∏≤‡∏ó‡∏∏‡∏Å‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô missing

# ‡∏•‡∏ö columns ‡∏ó‡∏µ‡πà‡∏°‡∏µ missing values
df_drop_cols = df.dropna(axis=1)              # ‡∏•‡∏ö columns ‡∏ó‡∏µ‡πà‡∏°‡∏µ missing

# ‡∏•‡∏ö‡∏ñ‡πâ‡∏≤‡∏°‡∏µ missing values ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
df_thresh = df.dropna(thresh=3)               # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà missing ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 3 ‡∏Ñ‡πà‡∏≤

# ‡∏•‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ columns ‡πÄ‡∏â‡∏û‡∏≤‡∏∞
df_subset = df.dropna(subset=['A', 'B'])      # ‡∏•‡∏ö‡∏ñ‡πâ‡∏≤ A ‡∏´‡∏£‡∏∑‡∏≠ B ‡πÄ‡∏õ‡πá‡∏ô missing
```

#### ‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏¥‡∏° Missing Values
```python
# ‡πÄ‡∏ï‡∏¥‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏á‡∏ó‡∏µ‡πà
df_fill_zero = df.fillna(0)                   # ‡πÄ‡∏ï‡∏¥‡∏°‡∏î‡πâ‡∏ß‡∏¢ 0
df_fill_unknown = df.fillna('Unknown')        # ‡πÄ‡∏ï‡∏¥‡∏°‡∏î‡πâ‡∏ß‡∏¢ 'Unknown'

# ‡πÄ‡∏ï‡∏¥‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
df_fill_mean = df.fillna(df.mean())           # ‡πÄ‡∏ï‡∏¥‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ (numeric columns)
df_fill_median = df.fillna(df.median())       # ‡πÄ‡∏ï‡∏¥‡∏°‡∏î‡πâ‡∏ß‡∏¢ median

# ‡πÄ‡∏ï‡∏¥‡∏°‡πÅ‡∏ï‡πà‡∏•‡∏∞ column ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô
fill_values = {'A': df['A'].mean(), 'B': df['B'].median(), 'D': 'Unknown'}
df_fill_dict = df.fillna(value=fill_values)

# Forward fill ‡πÅ‡∏•‡∏∞ Backward fill
df_ffill = df.fillna(method='ffill')          # ‡πÄ‡∏ï‡∏¥‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
df_bfill = df.fillna(method='bfill')          # ‡πÄ‡∏ï‡∏¥‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡∏ñ‡∏±‡∏î‡πÑ‡∏õ

# Interpolation
df_interp = df.interpolate()                  # interpolate ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö numeric data
```

### ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Duplicates

```python
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ duplicates
df_dup = pd.DataFrame({
    'Name': ['Alice', 'Bob', 'Alice', 'Charlie', 'Bob'],
    'Age': [25, 30, 25, 35, 30],
    'City': ['NY', 'London', 'NY', 'Tokyo', 'London']
})

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö duplicates
print(df_dup.duplicated())                    # Boolean mask
print(df_dup.duplicated().sum())              # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô duplicate rows

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö duplicates ‡πÉ‡∏ô column ‡πÄ‡∏â‡∏û‡∏≤‡∏∞
print(df_dup.duplicated(subset=['Name']))

# ‡∏•‡∏ö duplicates
df_no_dup = df_dup.drop_duplicates()          # ‡∏•‡∏ö duplicate rows
df_no_dup_name = df_dup.drop_duplicates(subset=['Name'])  # ‡∏•‡∏ö duplicate names

# ‡πÄ‡∏Å‡πá‡∏ö duplicate ‡πÅ‡∏£‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
df_keep_first = df_dup.drop_duplicates(keep='first')
df_keep_last = df_dup.drop_duplicates(keep='last')
df_remove_all = df_dup.drop_duplicates(keep=False)  # ‡∏•‡∏ö‡∏ó‡∏∏‡∏Å duplicates
```

### Data Type Conversion

```python
df = pd.DataFrame({
    'A': ['1', '2', '3', '4'],           # string numbers
    'B': [1.0, 2.0, 3.0, 4.0],          # floats
    'C': ['2023-01-01', '2023-01-02'],   # date strings
    'D': ['True', 'False', 'True']       # boolean strings
})

# ‡πÅ‡∏õ‡∏•‡∏á data types
df['A'] = df['A'].astype(int)             # string to int
df['B'] = df['B'].astype(int)             # float to int
df['C'] = pd.to_datetime(df['C'])         # string to datetime
df['D'] = df['D'].astype(bool)            # string to boolean

# ‡πÅ‡∏õ‡∏•‡∏á‡∏´‡∏•‡∏≤‡∏¢ columns ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô
df = df.astype({
    'A': int,
    'B': float,
    'D': bool
})

# ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ errors ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á
df_mixed = pd.DataFrame({'col': ['1', '2', 'text', '4']})
df_mixed['col_numeric'] = pd.to_numeric(df_mixed['col'], errors='coerce')  # NaN ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö errors
```

### ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î String Data

```python
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î
df_messy = pd.DataFrame({
    'Name': ['  Alice  ', 'BOB', 'charlie brown', 'DIANA'],
    'Email': ['ALICE@GMAIL.COM', 'bob@yahoo.com', 'Charlie@Gmail.com', 'diana@OUTLOOK.COM'],
    'Phone': ['123-456-7890', '(234) 567-8901', '345.678.9012', '456 789 0123']
})

# ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î strings
df_clean = df_messy.copy()

# ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏á
df_clean['Name'] = df_clean['Name'].str.strip()

# ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏•‡πá‡∏Å/‡πÉ‡∏´‡∏ç‡πà
df_clean['Email'] = df_clean['Email'].str.lower()
df_clean['Name'] = df_clean['Name'].str.title()  # Title Case

# ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà characters
df_clean['Phone'] = df_clean['Phone'].str.replace(r'[^\d]', '', regex=True)  # ‡πÄ‡∏Å‡πá‡∏ö‡πÅ‡∏ï‡πà‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç

# Split strings
name_parts = df_clean['Name'].str.split(' ', expand=True)
name_parts.columns = ['First_Name', 'Last_Name']

# Extract patterns ‡∏î‡πâ‡∏ß‡∏¢ regex
email_domain = df_clean['Email'].str.extract(r'@(.+)')
email_domain.columns = ['Domain']

print(df_clean)
```

### ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç Outliers

```python
import matplotlib.pyplot as plt

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ outliers
np.random.seed(42)
normal_data = np.random.normal(50, 10, 95)
outliers = [120, 130, -20, -10, 200]
data_with_outliers = np.concatenate([normal_data, outliers])

df_outliers = pd.DataFrame({'value': data_with_outliers})

# ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 1: Z-Score Method
z_scores = np.abs((df_outliers['value'] - df_outliers['value'].mean()) / df_outliers['value'].std())
outlier_zscore = df_outliers[z_scores > 3]
print(f"Outliers by Z-score: {len(outlier_zscore)}")

# ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 2: IQR Method
Q1 = df_outliers['value'].quantile(0.25)
Q3 = df_outliers['value'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outlier_iqr = df_outliers[(df_outliers['value'] < lower_bound) | 
                          (df_outliers['value'] > upper_bound)]
print(f"Outliers by IQR: {len(outlier_iqr)}")

# ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ outliers
# 1. ‡∏•‡∏ö outliers
df_no_outliers = df_outliers[(df_outliers['value'] >= lower_bound) & 
                             (df_outliers['value'] <= upper_bound)]

# 2. Cap outliers (winsorizing)
df_capped = df_outliers.copy()
df_capped['value'] = df_capped['value'].clip(lower_bound, upper_bound)

# 3. Transform data
df_log = df_outliers.copy()
df_log['value_log'] = np.log1p(df_log['value'] - df_log['value'].min() + 1)
```

---

## Data Transformation

### ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Columns ‡πÉ‡∏´‡∏°‡πà

```python
df = pd.DataFrame({
    'Name': ['Alice', 'Bob', 'Charlie', 'Diana'],
    'Age': [25, 30, 35, 28],
    'Salary': [50000, 60000, 70000, 55000],
    'Years_Experience': [3, 8, 12, 5]
})

# ‡∏™‡∏£‡πâ‡∏≤‡∏á column ‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì
df['Salary_per_year_exp'] = df['Salary'] / df['Years_Experience']
df['Age_Group'] = df['Age'].apply(lambda x: 'Young' if x < 30 else 'Senior')

# ‡πÉ‡∏ä‡πâ np.where ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö conditional logic
df['Performance'] = np.where(df['Salary'] > 60000, 'High', 'Normal')

# ‡πÉ‡∏ä‡πâ pd.cut ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö binning
df['Age_Bin'] = pd.cut(df['Age'], bins=[0, 25, 30, 35, 100], 
                       labels=['Very Young', 'Young', 'Middle', 'Senior'])

# Multiple conditions
conditions = [
    (df['Age'] < 30) & (df['Salary'] > 55000),
    (df['Age'] >= 30) & (df['Salary'] > 65000),
    (df['Age'] >= 30) & (df['Salary'] <= 65000)
]
choices = ['High Potential', 'Senior High Performer', 'Senior Normal']
df['Category'] = np.select(conditions, choices, default='Other')

print(df)
```

### ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Apply Functions

```python
# Apply function ‡∏Å‡∏±‡∏ö Series
def categorize_salary(salary):
    if salary < 55000:
        return 'Low'
    elif salary < 65000:
        return 'Medium'
    else:
        return 'High'

df['Salary_Category'] = df['Salary'].apply(categorize_salary)

# Apply ‡∏Å‡∏±‡∏ö lambda function
df['Name_Length'] = df['Name'].apply(lambda x: len(x))
df['Salary_K'] = df['Salary'].apply(lambda x: f"{x/1000:.0f}K")

# Apply ‡∏Å‡∏±‡∏ö DataFrame (axis=1 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö rows)
def calculate_bonus(row):
    base_bonus = row['Salary'] * 0.1
    experience_bonus = row['Years_Experience'] * 1000
    return base_bonus + experience_bonus

df['Bonus'] = df.apply(calculate_bonus, axis=1)

# Apply ‡∏Å‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢ columns
df['Salary_Age_Ratio'] = df.apply(lambda row: row['Salary'] / row['Age'], axis=1)
```

### Map ‡πÅ‡∏•‡∏∞ Replace

```python
# Map ‡∏î‡πâ‡∏ß‡∏¢ dictionary
age_mapping = {25: 'Young', 30: 'Middle', 35: 'Senior', 28: 'Young'}
df['Age_Category'] = df['Age'].map(age_mapping)

# Map ‡∏î‡πâ‡∏ß‡∏¢ Series
salary_avg = df.groupby('Age_Group')['Salary'].mean()
df['Avg_Salary_for_Group'] = df['Age_Group'].map(salary_avg)

# Replace values
df_replace = df.copy()
df_replace['Name'] = df_replace['Name'].replace('Alice', 'Alicia')
df_replace['Performance'] = df_replace['Performance'].replace({'High': 'Excellent', 'Normal': 'Good'})

# Replace with regex
df_replace['Name'] = df_replace['Name'].str.replace(r'[aeiou]', 'X', regex=True)
```

### Pivot Tables ‡πÅ‡∏•‡∏∞ Reshaping

```python
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö pivot
sales_data = pd.DataFrame({
    'Date': pd.date_range('2023-01-01', periods=12, freq='M'),
    'Product': ['A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B'],
    'Region': ['North', 'North', 'South', 'South', 'North', 'North', 
               'South', 'South', 'North', 'North', 'South', 'South'],
    'Sales': [100, 150, 120, 180, 110, 160, 130, 190, 105, 155, 125, 195]
})

# Pivot table
pivot_table = sales_data.pivot_table(
    values='Sales',
    index='Product',
    columns='Region',
    aggfunc='mean'
)
print(pivot_table)

# Melt (inverse of pivot)
melted = pivot_table.reset_index().melt(
    id_vars='Product',
    var_name='Region',
    value_name='Average_Sales'
)
print(melted)

# Stack ‡πÅ‡∏•‡∏∞ Unstack
stacked = pivot_table.stack()
unstacked = stacked.unstack()
```

---

## Groupby Operations

### ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô Groupby

```python
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
employees = pd.DataFrame({
    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank'],
    'Department': ['IT', 'HR', 'IT', 'Finance', 'IT', 'HR'],
    'Age': [25, 30, 35, 28, 32, 27],
    'Salary': [50000, 60000, 70000, 55000, 65000, 58000],
    'Years_Experience': [3, 8, 12, 5, 9, 6]
})

# Basic groupby operations
dept_groups = employees.groupby('Department')

# Aggregation functions
print(dept_groups.mean())              # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
print(dept_groups.sum())               # ‡∏ú‡∏•‡∏£‡∏ß‡∏°
print(dept_groups.count())             # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô
print(dept_groups.std())               # ‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏ö‡∏µ‡πà‡∏¢‡∏á‡πÄ‡∏ö‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô
print(dept_groups.describe())          # ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å column ‡πÄ‡∏â‡∏û‡∏≤‡∏∞
print(dept_groups['Salary'].mean())
print(dept_groups[['Salary', 'Age']].mean())
```

### Advanced Groupby Operations

```python
# Multiple groupby columns
age_dept_groups = employees.groupby(['Department', 'Age'])
print(age_dept_groups.size())          # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÉ‡∏ô group

# Custom aggregation functions
def salary_range(series):
    return series.max() - series.min()

custom_agg = employees.groupby('Department')['Salary'].agg([
    'mean',
    'median',
    'std',
    ('range', salary_range),
    ('count', 'count')
])
print(custom_agg)

# Different aggregations for different columns
multi_agg = employees.groupby('Department').agg({
    'Salary': ['mean', 'max', 'min'],
    'Age': ['mean', 'std'],
    'Years_Experience': 'mean'
})
print(multi_agg)

# Apply custom functions
def department_analysis(group):
    return pd.Series({
        'avg_salary': group['Salary'].mean(),
        'max_experience': group['Years_Experience'].max(),
        'avg_age': group['Age'].mean(),
        'salary_per_experience': group['Salary'].sum() / group['Years_Experience'].sum()
    })

dept_analysis = employees.groupby('Department').apply(department_analysis)
print(dept_analysis)
```

### Transform ‡πÅ‡∏•‡∏∞ Filter

```python
# Transform - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö original
employees['Dept_Avg_Salary'] = employees.groupby('Department')['Salary'].transform('mean')
employees['Salary_vs_Dept_Avg'] = employees['Salary'] - employees['Dept_Avg_Salary']
employees['Salary_Rank_in_Dept'] = employees.groupby('Department')['Salary'].rank(ascending=False)

# Filter - ‡∏Å‡∏£‡∏≠‡∏á groups ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç
large_departments = employees.groupby('Department').filter(lambda x: len(x) >= 2)
high_avg_salary_depts = employees.groupby('Department').filter(lambda x: x['Salary'].mean() > 60000)

print(employees)
print("\nLarge departments:")
print(large_departments)
```

### Rolling ‡πÅ‡∏•‡∏∞ Expanding Operations

```python
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• time series
dates = pd.date_range('2023-01-01', periods=100, freq='D')
ts_data = pd.DataFrame({
    'Date': dates,
    'Value': np.random.randn(100).cumsum() + 100
})
ts_data.set_index('Date', inplace=True)

# Rolling operations
ts_data['Rolling_Mean_7'] = ts_data['Value'].rolling(window=7).mean()
ts_data['Rolling_Std_7'] = ts_data['Value'].rolling(window=7).std()
ts_data['Rolling_Max_7'] = ts_data['Value'].rolling(window=7).max()

# Expanding operations (‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ñ‡∏∂‡∏á‡∏à‡∏∏‡∏î‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô)
ts_data['Expanding_Mean'] = ts_data['Value'].expanding().mean()
ts_data['Expanding_Std'] = ts_data['Value'].expanding().std()

# Rolling ‡∏Å‡∏±‡∏ö groupby
grouped_employees = employees.copy()
grouped_employees['Salary_Rank_Overall'] = grouped_employees['Salary'].rank(ascending=False)

print(ts_data.head(10))
```

---

## Merging ‡πÅ‡∏•‡∏∞ Joining

### ‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏° DataFrames

```python
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
employees = pd.DataFrame({
    'EmployeeID': [1, 2, 3, 4, 5],
    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],
    'DepartmentID': [101, 102, 101, 103, 102]
})

departments = pd.DataFrame({
    'DepartmentID': [101, 102, 103, 104],
    'Department': ['IT', 'HR', 'Finance', 'Marketing'],
    'Location': ['Building A', 'Building B', 'Building C', 'Building D']
})

salaries = pd.DataFrame({
    'EmployeeID': [1, 2, 3, 4, 6],
    'Salary': [50000, 60000, 70000, 55000, 48000],
    'Bonus': [5000, 6000, 7000, 5500, 4800]
})
```

#### Inner Join
```python
# Inner join - ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ records ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô table ‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á
emp_dept = pd.merge(employees, departments, on='DepartmentID', how='inner')
print("Inner Join:")
print(emp_dept)

emp_salary = pd.merge(employees, salaries, on='EmployeeID', how='inner')
print("\nEmployee-Salary Inner Join:")
print(emp_salary)
```

#### Left, Right, ‡πÅ‡∏•‡∏∞ Outer Joins
```python
# Left join - ‡πÄ‡∏Å‡πá‡∏ö‡∏ó‡∏∏‡∏Å records ‡∏à‡∏≤‡∏Å left table
left_join = pd.merge(employees, salaries, on='EmployeeID', how='left')
print("Left Join:")
print(left_join)

# Right join - ‡πÄ‡∏Å‡πá‡∏ö‡∏ó‡∏∏‡∏Å records ‡∏à‡∏≤‡∏Å right table
right_join = pd.merge(employees, salaries, on='EmployeeID', how='right')
print("Right Join:")
print(right_join)

# Outer join - ‡πÄ‡∏Å‡πá‡∏ö‡∏ó‡∏∏‡∏Å records ‡∏à‡∏≤‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á tables
outer_join = pd.merge(employees, salaries, on='EmployeeID', how='outer')
print("Outer Join:")
print(outer_join)
```

#### Advanced Merging
```python
# Merge ‡∏Å‡∏±‡∏ö different column names
emp_modified = employees.rename(columns={'EmployeeID': 'EmpID'})
merge_diff_names = pd.merge(emp_modified, salaries, 
                           left_on='EmpID', right_on='EmployeeID')

# Merge ‡∏Å‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢ columns
combined_data = pd.merge(
    pd.merge(employees, departments, on='DepartmentID'),
    salaries, on='EmployeeID', how='left'
)
print("Complete Employee Data:")
print(combined_data)

# Merge ‡∏Å‡∏±‡∏ö suffixes ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö column ‡∏ó‡∏µ‡πà‡∏ä‡∏∑‡πà‡∏≠‡∏ã‡πâ‡∏≥
df1 = pd.DataFrame({'key': ['A', 'B', 'C'], 'value': [1, 2, 3]})
df2 = pd.DataFrame({'key': ['A', 'B', 'D'], 'value': [4, 5, 6]})
merged_suffix = pd.merge(df1, df2, on='key', how='outer', suffixes=('_left', '_right'))
print("Merge with suffixes:")
print(merged_suffix)
```

### Concatenation

```python
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
df1 = pd.DataFrame({
    'A': [1, 2, 3],
    'B': [4, 5, 6]
})

df2 = pd.DataFrame({
    'A': [7, 8, 9],
    'B': [10, 11, 12]
})

df3 = pd.DataFrame({
    'C': [13, 14, 15],
    'D': [16, 17, 18]
})

# Vertical concatenation (‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏ñ‡∏ß)
vertical_concat = pd.concat([df1, df2], axis=0, ignore_index=True)
print("Vertical Concatenation:")
print(vertical_concat)

# Horizontal concatenation (‡πÄ‡∏û‡∏¥‡πà‡∏° columns)
horizontal_concat = pd.concat([df1, df3], axis=1)
print("Horizontal Concatenation:")
print(horizontal_concat)

# Concatenation ‡∏Å‡∏±‡∏ö keys
keyed_concat = pd.concat([df1, df2], keys=['Group1', 'Group2'])
print("Concatenation with keys:")
print(keyed_concat)
```

### Join Method

```python
# ‡πÉ‡∏ä‡πâ join method (‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö index-based joining)
df_indexed1 = employees.set_index('EmployeeID')
df_indexed2 = salaries.set_index('EmployeeID')

# Join (default ‡πÄ‡∏õ‡πá‡∏ô left join)
joined = df_indexed1.join(df_indexed2)
print("Join result:")
print(joined)

# Join ‡∏Å‡∏±‡∏ö different join types
inner_joined = df_indexed1.join(df_indexed2, how='inner')
outer_joined = df_indexed1.join(df_indexed2, how='outer')
```

---

## Time Series Analysis

### ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Datetime

```python
# ‡∏™‡∏£‡πâ‡∏≤‡∏á datetime data
dates = pd.date_range('2023-01-01', periods=100, freq='D')
ts_data = pd.DataFrame({
    'Date': dates,
    'Sales': np.random.randint(100, 1000, 100),
    'Temperature': np.random.normal(25, 5, 100)
})

# ‡πÅ‡∏õ‡∏•‡∏á string ‡πÄ‡∏õ‡πá‡∏ô datetime
date_strings = ['2023-01-01', '2023-01-02', '2023-01-03']
converted_dates = pd.to_datetime(date_strings)

# Parse datetime ‡∏à‡∏≤‡∏Å format ‡πÄ‡∏â‡∏û‡∏≤‡∏∞
custom_dates = ['01/15/2023', '02/15/2023', '03/15/2023']
parsed_dates = pd.to_datetime(custom_dates, format='%m/%d/%Y')

# Set datetime ‡πÄ‡∏õ‡πá‡∏ô index
ts_data.set_index('Date', inplace=True)
print(ts_data.head())
```

### Time-based Indexing ‡πÅ‡∏•‡∏∞ Slicing

```python
# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏° date range
jan_data = ts_data['2023-01']                    # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°
first_week = ts_data['2023-01-01':'2023-01-07']  # ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡πÅ‡∏£‡∏Å
recent_data = ts_data.last('30D')                # 30 ‡∏ß‡∏±‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ß‡∏±‡∏ô
mondays = ts_data[ts_data.index.dayofweek == 0]  # ‡∏ß‡∏±‡∏ô‡∏à‡∏±‡∏ô‡∏ó‡∏£‡πå
weekends = ts_data[ts_data.index.dayofweek.isin([5, 6])]  # ‡πÄ‡∏™‡∏≤‡∏£‡πå-‡∏≠‡∏≤‡∏ó‡∏¥‡∏ï‡∏¢‡πå

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏ß‡∏•‡∏≤ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ time component)
ts_hourly = pd.DataFrame({
    'Datetime': pd.date_range('2023-01-01', periods=48, freq='H'),
    'Value': np.random.randn(48)
}).set_index('Datetime')

business_hours = ts_hourly.between_time('09:00', '17:00')
```

### Resampling ‡πÅ‡∏•‡∏∞ Frequency Conversion

```python
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô
daily_data = pd.DataFrame({
    'Date': pd.date_range('2023-01-01', periods=365, freq='D'),
    'Sales': np.random.randint(100, 1000, 365),
    'Visitors': np.random.randint(50, 500, 365)
}).set_index('Date')

# Upsample (‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà)
hourly_upsampled = daily_data.resample('H').interpolate()

# Downsample (‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà)
weekly_data = daily_data.resample('W').agg({
    'Sales': 'sum',
    'Visitors': 'mean'
})

monthly_data = daily_data.resample('M').agg({
    'Sales': ['sum', 'mean', 'std'],
    'Visitors': ['mean', 'max', 'min']
})

quarterly_data = daily_data.resample('Q').sum()

print("Weekly aggregated data:")
print(weekly_data.head())
```






